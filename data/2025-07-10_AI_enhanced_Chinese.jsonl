{"id": "2507.06346", "categories": ["cs.RO", "stat.CO"], "pdf": "https://arxiv.org/pdf/2507.06346", "abs": "https://arxiv.org/abs/2507.06346", "authors": ["Li Zhou", "Elvan Ceyhan"], "title": "Solving the Constrained Random Disambiguation Path Problem via Lagrangian Relaxation and Graph Reduction", "comment": null, "summary": "We study a resource-constrained variant of the Random Disambiguation Path\n(RDP) problem, a generalization of the Stochastic Obstacle Scene (SOS) problem,\nin which a navigating agent must reach a target in a spatial environment\npopulated with uncertain obstacles. Each ambiguous obstacle may be\ndisambiguated at a (possibly) heterogeneous resource cost, subject to a global\ndisambiguation budget. We formulate this constrained planning problem as a\nWeight-Constrained Shortest Path Problem (WCSPP) with risk-adjusted edge costs\nthat incorporate probabilistic blockage and traversal penalties. To solve it,\nwe propose a novel algorithmic framework-COLOGR-combining Lagrangian relaxation\nwith a two-phase vertex elimination (TPVE) procedure. The method prunes\ninfeasible and suboptimal paths while provably preserving the optimal solution,\nand leverages dual bounds to guide efficient search. We establish correctness,\nfeasibility guarantees, and surrogate optimality under mild assumptions. Our\nanalysis also demonstrates that COLOGR frequently achieves zero duality gap and\noffers improved computational complexity over prior constrained path-planning\nmethods. Extensive simulation experiments validate the algorithm's robustness\nacross varying obstacle densities, sensor accuracies, and risk models,\nconsistently outperforming greedy baselines and approaching offline-optimal\nbenchmarks. The proposed framework is broadly applicable to stochastic network\ndesign, mobility planning, and constrained decision-making under uncertainty.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8d44\u6e90\u53d7\u9650\u7684\u968f\u673a\u6d88\u6b67\u8def\u5f84\uff08RDP\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u62c9\u683c\u6717\u65e5\u677e\u5f1b\u548c\u4e24\u9636\u6bb5\u9876\u70b9\u6d88\u9664\uff08TPVE\uff09\u7684\u65b0\u7b97\u6cd5\u6846\u67b6COLOGR\uff0c\u89e3\u51b3\u4e86\u5e26\u6743\u7ea6\u675f\u7684\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff08WCSPP\uff09\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u4e0d\u786e\u5b9a\u969c\u788d\u7269\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u8d44\u6e90\u53d7\u9650\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u4f18\u5316\u6d88\u6b67\u6210\u672c\u4e0e\u8def\u5f84\u98ce\u9669\u3002", "method": "\u63d0\u51faCOLOGR\u6846\u67b6\uff0c\u7ed3\u5408\u62c9\u683c\u6717\u65e5\u677e\u5f1b\u548cTPVE\uff0c\u526a\u679d\u4e0d\u53ef\u884c\u548c\u6b21\u4f18\u8def\u5f84\uff0c\u5229\u7528\u5bf9\u5076\u754c\u6307\u5bfc\u641c\u7d22\u3002", "result": "COLOGR\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u8d2a\u5fc3\u57fa\u7ebf\uff0c\u63a5\u8fd1\u79bb\u7ebf\u6700\u4f18\u57fa\u51c6\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "conclusion": "COLOGR\u9002\u7528\u4e8e\u968f\u673a\u7f51\u7edc\u8bbe\u8ba1\u3001\u79fb\u52a8\u89c4\u5212\u548c\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u53d7\u9650\u51b3\u7b56\u95ee\u9898\u3002"}}
{"id": "2507.06397", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06397", "abs": "https://arxiv.org/abs/2507.06397", "authors": ["Michalis Chatzispyrou", "Luke Horgan", "Hyunkil Hwang", "Harish Sathishchandra", "Monika Roznere", "Alberto Quattrini Li", "Philippos Mordohai", "Ioannis Rekleitis"], "title": "Mapping the Catacombs: An Underwater Cave Segment of the Devil's Eye System", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "This paper presents a framework for mapping underwater caves. Underwater\ncaves are crucial for fresh water resource management, underwater archaeology,\nand hydrogeology. Mapping the cave's outline and dimensions, as well as\ncreating photorealistic 3D maps, is critical for enabling a better\nunderstanding of this underwater domain. In this paper, we present the mapping\nof an underwater cave segment (the catacombs) of the Devil's Eye cave system at\nGinnie Springs, FL. We utilized a set of inexpensive action cameras in\nconjunction with a dive computer to estimate the trajectories of the cameras\ntogether with a sparse point cloud. The resulting reconstructions are utilized\nto produce a one-dimensional retract of the cave passages in the form of the\naverage trajectory together with the boundaries (top, bottom, left, and right).\nThe use of the dive computer enables the observability of the z-dimension in\naddition to the roll and pitch in a visual/inertial framework (SVIn2). In\naddition, the keyframes generated by SVIn2 together with the estimated camera\nposes for select areas are used as input to a global optimization (bundle\nadjustment) framework -- COLMAP -- in order to produce a dense reconstruction\nof those areas. The same cave segment is manually surveyed using the MNemo V2\ninstrument, providing an additional set of measurements validating the proposed\napproach. It is worth noting that with the use of action cameras, the primary\ncomponents of a cave map can be constructed. Furthermore, with the utilization\nof a global optimization framework guided by the results of VI-SLAM package\nSVIn2, photorealistic dense 3D representations of selected areas can be\nreconstructed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4f4e\u6210\u672c\u8fd0\u52a8\u76f8\u673a\u548c\u6f5c\u6c34\u7535\u8111\u7ed8\u5236\u6c34\u4e0b\u6d1e\u7a74\u5730\u56fe\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9/\u60ef\u6027\u6846\u67b6\u548c\u5168\u5c40\u4f18\u5316\u6280\u672f\u751f\u6210\u6d1e\u7a74\u7684\u4e00\u7ef4\u8f6e\u5ed3\u548c\u5bc6\u96c63D\u91cd\u5efa\u3002", "motivation": "\u6c34\u4e0b\u6d1e\u7a74\u5bf9\u6de1\u6c34\u8d44\u6e90\u7ba1\u7406\u3001\u6c34\u4e0b\u8003\u53e4\u548c\u6c34\u6587\u5730\u8d28\u5b66\u81f3\u5173\u91cd\u8981\uff0c\u7ed8\u5236\u5176\u8f6e\u5ed3\u548c\u5c3a\u5bf8\u4ee5\u53ca\u751f\u6210\u903c\u771f\u76843D\u5730\u56fe\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e00\u9886\u57df\u3002", "method": "\u4f7f\u7528\u4f4e\u6210\u672c\u8fd0\u52a8\u76f8\u673a\u548c\u6f5c\u6c34\u7535\u8111\u4f30\u8ba1\u76f8\u673a\u8f68\u8ff9\u548c\u7a00\u758f\u70b9\u4e91\uff0c\u7ed3\u5408SVIn2\u6846\u67b6\u548cCOLMAP\u5168\u5c40\u4f18\u5316\u6280\u672f\u751f\u6210\u4e00\u7ef4\u6d1e\u7a74\u8f6e\u5ed3\u548c\u5bc6\u96c63D\u91cd\u5efa\u3002", "result": "\u6210\u529f\u7ed8\u5236\u4e86Devil's Eye\u6d1e\u7a74\u7cfb\u7edf\u7684\u4e00\u6bb5\uff0c\u5e76\u901a\u8fc7\u624b\u52a8\u6d4b\u91cf\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u751f\u6210\u4e86\u6d1e\u7a74\u7684\u4e00\u7ef4\u8f6e\u5ed3\u548c\u90e8\u5206\u533a\u57df\u7684\u5bc6\u96c63D\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5229\u7528\u4f4e\u6210\u672c\u8bbe\u5907\u5b9e\u73b0\u4e86\u6c34\u4e0b\u6d1e\u7a74\u5730\u56fe\u7684\u4e3b\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5e76\u901a\u8fc7\u5168\u5c40\u4f18\u5316\u6280\u672f\u751f\u6210\u4e86\u903c\u771f\u76843D\u91cd\u5efa\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.06404", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06404", "abs": "https://arxiv.org/abs/2507.06404", "authors": ["Matteo Tiezzi", "Tommaso Apicella", "Carlos Cardenas-Perez", "Giovanni Fregonese", "Stefano Dafarra", "Pietro Morerio", "Daniele Pucci", "Alessio Del Bue"], "title": "Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction", "comment": null, "summary": "Evaluating and comparing the performance of autonomous Humanoid Robots is\nchallenging, as success rate metrics are difficult to reproduce and fail to\ncapture the complexity of robot movement trajectories, critical in Human-Robot\nInteraction and Collaboration (HRIC). To address these challenges, we propose a\ngeneral evaluation framework that measures the quality of Imitation Learning\n(IL) methods by focusing on trajectory performance. We devise the Neural Meta\nEvaluator (NeME), a deep learning model trained to classify actions from robot\njoint trajectories. NeME serves as a meta-evaluator to compare the performance\nof robot control policies, enabling policy evaluation without requiring human\ninvolvement in the loop. We validate our framework on ergoCub, a humanoid\nrobot, using teleoperation data and comparing IL methods tailored to the\navailable platform. The experimental results indicate that our method is more\naligned with the success rate obtained on the robot than baselines, offering a\nreproducible, systematic, and insightful means for comparing the performance of\nmultimodal imitation learning approaches in complex HRI tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u6027\u80fd\u7684\u901a\u7528\u8bc4\u4f30\u6846\u67b6NeME\uff0c\u7528\u4e8e\u6bd4\u8f83\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u4eba\u673a\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u8bc4\u4f30\u4eba\u5f62\u673a\u5668\u4eba\u6027\u80fd\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u6210\u529f\u7387\u6307\u6807\u96be\u4ee5\u590d\u73b0\u4e14\u65e0\u6cd5\u6355\u6349\u8f68\u8ff9\u590d\u6742\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86NeME\uff08\u795e\u7ecf\u5143\u8bc4\u4f30\u5668\uff09\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ece\u5173\u8282\u8f68\u8ff9\u4e2d\u5206\u7c7b\u52a8\u4f5c\uff0c\u4f5c\u4e3a\u65e0\u4eba\u7c7b\u53c2\u4e0e\u7684\u5143\u8bc4\u4f30\u5668\u3002", "result": "\u5728ergoCub\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0cNeME\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u63a5\u8fd1\u5b9e\u9645\u6210\u529f\u7387\uff0c\u63d0\u4f9b\u53ef\u590d\u73b0\u4e14\u7cfb\u7edf\u7684\u8bc4\u4f30\u624b\u6bb5\u3002", "conclusion": "NeME\u4e3a\u590d\u6742\u4eba\u673a\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6027\u80fd\u6bd4\u8f83\u5de5\u5177\u3002"}}
{"id": "2507.06426", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06426", "abs": "https://arxiv.org/abs/2507.06426", "authors": ["Devin Crowley", "Whitney G. Cole", "Christina M. Hospodar", "Ruiting Shen", "Karen E. Adolph", "Alan Fern"], "title": "Evaluating Robots Like Human Infants: A Case Study of Learned Bipedal Locomotion", "comment": "7 pages, 4 figures, accepted into ICDL 2025 as a contributed paper", "summary": "Typically, learned robot controllers are trained via relatively unsystematic\nregimens and evaluated with coarse-grained outcome measures such as average\ncumulative reward. The typical approach is useful to compare learning\nalgorithms but provides limited insight into the effects of different training\nregimens and little understanding about the richness and complexity of learned\nbehaviors. Likewise, human infants and other animals are \"trained\" via\nunsystematic regimens, but in contrast, developmental psychologists evaluate\ntheir performance in highly-controlled experiments with fine-grained measures\nsuch as success, speed of walking, and prospective adjustments. However, the\nstudy of learned behavior in human infants is limited by the practical\nconstraints of training and testing babies. Here, we present a case study that\napplies methods from developmental psychology to study the learned behavior of\nthe simulated bipedal robot Cassie. Following research on infant walking, we\nsystematically designed reinforcement learning training regimens and tested the\nresulting controllers in simulated environments analogous to those used for\nbabies--but without the practical constraints. Results reveal new insights into\nthe behavioral impact of different training regimens and the development of\nCassie's learned behaviors relative to infants who are learning to walk. This\ninterdisciplinary baby-robot approach provides inspiration for future research\ndesigned to systematically test effects of training on the development of\ncomplex learned robot behaviors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u53d1\u5c55\u5fc3\u7406\u5b66\u65b9\u6cd5\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u884c\u4e3a\u5b66\u4e60\u7684\u7814\u7a76\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bbe\u8ba1\u8bad\u7ec3\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u8bad\u7ec3\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u53d1\u5c55\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u63a7\u5236\u5668\u8bad\u7ec3\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\uff0c\u4e14\u8bc4\u4f30\u6307\u6807\u7c97\u7cd9\uff0c\u65e0\u6cd5\u6df1\u5165\u7406\u89e3\u8bad\u7ec3\u65b9\u6848\u5bf9\u884c\u4e3a\u7684\u5f71\u54cd\u3002\u53d1\u5c55\u5fc3\u7406\u5b66\u5bf9\u5a74\u513f\u884c\u4e3a\u7684\u7814\u7a76\u65b9\u6cd5\u7cbe\u7ec6\uff0c\u4f46\u53d7\u9650\u4e8e\u5b9e\u9645\u7ea6\u675f\u3002", "method": "\u91c7\u7528\u53d1\u5c55\u5fc3\u7406\u5b66\u65b9\u6cd5\uff0c\u7cfb\u7edf\u8bbe\u8ba1\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6848\uff0c\u5e76\u5728\u6a21\u62df\u73af\u5883\u4e2d\u6d4b\u8bd5\u53cc\u8db3\u673a\u5668\u4ebaCassie\u7684\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540c\u8bad\u7ec3\u65b9\u6848\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u5a74\u513f\u5b66\u6b65\u884c\u4e3a\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "conclusion": "\u8fd9\u79cd\u8de8\u5b66\u79d1\u65b9\u6cd5\u4e3a\u672a\u6765\u7cfb\u7edf\u7814\u7a76\u590d\u6742\u673a\u5668\u4eba\u884c\u4e3a\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.06326", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.06326", "abs": "https://arxiv.org/abs/2507.06326", "authors": ["Harsh Ravivarapu", "Gaurav Bagwe", "Xiaoyong Yuan", "Chunxiu Yu", "Lan Zhang"], "title": "Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease", "comment": "Accepted by IEEE IMC 2025", "summary": "Deep brain stimulation (DBS) is an established intervention for Parkinson's\ndisease (PD), but conventional open-loop systems lack adaptability, are\nenergy-inefficient due to continuous stimulation, and provide limited\npersonalization to individual neural dynamics. Adaptive DBS (aDBS) offers a\nclosed-loop alternative, using biomarkers such as beta-band oscillations to\ndynamically modulate stimulation. While reinforcement learning (RL) holds\npromise for personalized aDBS control, existing methods suffer from high sample\ncomplexity, unstable exploration in binary action spaces, and limited\ndeployability on resource-constrained hardware.\n  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses\nthe core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a\npredictive reward model to reduce reliance on real-time feedback and employs\nGumbel Softmax-based exploration for stable, differentiable policy updates in\nbinary action spaces. Together, these components improve sample efficiency,\nexploration robustness, and compatibility with resource-constrained\nneuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic\nsimulation of Parkinsonian basal ganglia activity, demonstrating faster\nconvergence, stronger suppression of pathological beta-band power, and\nresilience to post-training FP16 quantization. Our results show that SEA-DBS\noffers a practical and effective RL-based aDBS framework for real-time,\nresource-constrained neuromodulation.", "AI": {"tldr": "SEA-DBS\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u8111\u6df1\u90e8\u523a\u6fc0\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u5956\u52b1\u6a21\u578b\u548cGumbel Softmax\u63a2\u7d22\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u786c\u4ef6\u517c\u5bb9\u6027\u3002", "motivation": "\u4f20\u7edf\u5f00\u73afDBS\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u4e2a\u6027\u5316\uff0c\u800c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u548c\u786c\u4ef6\u90e8\u7f72\u9650\u5236\u7684\u95ee\u9898\u3002", "method": "SEA-DBS\u7ed3\u5408\u9884\u6d4b\u5956\u52b1\u6a21\u578b\u548cGumbel Softmax\u63a2\u7d22\uff0c\u4f18\u5316\u4e86\u6837\u672c\u6548\u7387\u548c\u4e8c\u8fdb\u5236\u52a8\u4f5c\u7a7a\u95f4\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u5e15\u91d1\u68ee\u75c5\u57fa\u5e95\u8282\u6d3b\u52a8\u6a21\u62df\u4e2d\uff0cSEA-DBS\u8868\u73b0\u51fa\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u5f3a\u7684\u75c5\u7406\u03b2\u6ce2\u6bb5\u6291\u5236\u80fd\u529b\uff0c\u5e76\u652f\u6301FP16\u91cf\u5316\u3002", "conclusion": "SEA-DBS\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u65f6\u795e\u7ecf\u8c03\u63a7\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06267", "categories": ["cs.LG", "34C60, 92B05, 68T07, 93C15, 65K10"], "pdf": "https://arxiv.org/pdf/2507.06267", "abs": "https://arxiv.org/abs/2507.06267", "authors": ["Hyeontae Jo", "Kre\u0161imir Josi\u0107", "Jae Kyoung Kim"], "title": "Neural Network-Based Parameter Estimation for Non-Autonomous Differential Equations with Discontinuous Signals", "comment": null, "summary": "Non-autonomous differential equations are crucial for modeling systems\ninfluenced by external signals, yet fitting these models to data becomes\nparticularly challenging when the signals change abruptly. To address this\nproblem, we propose a novel parameter estimation method utilizing functional\napproximations with artificial neural networks. Our approach, termed Harmonic\nApproximation of Discontinuous External Signals using Neural Networks\n(HADES-NN), operates in two iterated stages. In the first stage, the algorithm\nemploys a neural network to approximate the discontinuous signal with a smooth\nfunction. In the second stage, it uses this smooth approximate signal to\nestimate model parameters. HADES-NN gives highly accurate and precise parameter\nestimates across various applications, including circadian clock systems\nregulated by external light inputs measured via wearable devices and the mating\nresponse of yeast to external pheromone signals. HADES-NN greatly extends the\nrange of model systems that can be fit to real-world measurements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHADES-NN\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5e73\u6ed1\u8fd1\u4f3c\u975e\u8fde\u7eed\u5916\u90e8\u4fe1\u53f7\uff0c\u7528\u4e8e\u975e\u81ea\u6cbb\u5fae\u5206\u65b9\u7a0b\u7684\u53c2\u6570\u4f30\u8ba1\u3002", "motivation": "\u975e\u81ea\u6cbb\u5fae\u5206\u65b9\u7a0b\u5728\u5efa\u6a21\u53d7\u5916\u90e8\u4fe1\u53f7\u5f71\u54cd\u7684\u7cfb\u7edf\u65f6\u5f88\u91cd\u8981\uff0c\u4f46\u4fe1\u53f7\u7a81\u53d8\u65f6\u53c2\u6570\u4f30\u8ba1\u53d8\u5f97\u56f0\u96be\u3002", "method": "HADES-NN\u5206\u4e24\u9636\u6bb5\uff1a\u5148\u7528\u795e\u7ecf\u7f51\u7edc\u5e73\u6ed1\u8fd1\u4f3c\u975e\u8fde\u7eed\u4fe1\u53f7\uff0c\u518d\u7528\u5e73\u6ed1\u4fe1\u53f7\u4f30\u8ba1\u6a21\u578b\u53c2\u6570\u3002", "result": "HADES-NN\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u53c2\u6570\u4f30\u8ba1\uff0c\u5982\u663c\u591c\u8282\u5f8b\u7cfb\u7edf\u548c\u9175\u6bcd\u4ea4\u914d\u54cd\u5e94\u3002", "conclusion": "HADES-NN\u663e\u8457\u6269\u5c55\u4e86\u53ef\u62df\u5408\u5b9e\u9645\u6d4b\u91cf\u6570\u636e\u7684\u6a21\u578b\u7cfb\u7edf\u8303\u56f4\u3002"}}
{"id": "2507.06519", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06519", "abs": "https://arxiv.org/abs/2507.06519", "authors": ["Yuhan Liu", "Xinyu Zhang", "Haonan Chang", "Abdeslam Boularias"], "title": "Failure Forecasting Boosts Robustness of Sim2Real Rhythmic Insertion Policies", "comment": "Accepted at IROS2025. Project website:\n  https://jaysparrow.github.io/rit", "summary": "This paper addresses the challenges of Rhythmic Insertion Tasks (RIT), where\na robot must repeatedly perform high-precision insertions, such as screwing a\nnut into a bolt with a wrench. The inherent difficulty of RIT lies in achieving\nmillimeter-level accuracy and maintaining consistent performance over multiple\nrepetitions, particularly when factors like nut rotation and friction introduce\nadditional complexity. We propose a sim-to-real framework that integrates a\nreinforcement learning-based insertion policy with a failure forecasting\nmodule. By representing the wrench's pose in the nut's coordinate frame rather\nthan the robot's frame, our approach significantly enhances sim-to-real\ntransferability. The insertion policy, trained in simulation, leverages\nreal-time 6D pose tracking to execute precise alignment, insertion, and\nrotation maneuvers. Simultaneously, a neural network predicts potential\nexecution failures, triggering a simple recovery mechanism that lifts the\nwrench and retries the insertion. Extensive experiments in both simulated and\nreal-world environments demonstrate that our method not only achieves a high\none-time success rate but also robustly maintains performance over long-horizon\nrepetitive tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u63d2\u5165\u7b56\u7565\u548c\u5931\u8d25\u9884\u6d4b\u6a21\u5757\u7684sim-to-real\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u7cbe\u5ea6\u91cd\u590d\u63d2\u5165\u4efb\u52a1\uff08RIT\uff09\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3RIT\u4efb\u52a1\u4e2d\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u548c\u957f\u65f6\u95f4\u91cd\u590d\u6027\u80fd\u4fdd\u6301\u7684\u96be\u9898\uff0c\u7279\u522b\u662f\u87ba\u6bcd\u65cb\u8f6c\u548c\u6469\u64e6\u5e26\u6765\u7684\u590d\u6742\u6027\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d2\u5165\u7b56\u7565\uff0c\u7ed3\u54086D\u4f4d\u59ff\u8ddf\u8e2a\u548c\u5931\u8d25\u9884\u6d4b\u6a21\u5757\uff0c\u901a\u8fc7\u87ba\u6bcd\u5750\u6807\u7cfb\u8868\u793a\u6273\u624b\u4f4d\u59ff\u63d0\u5347sim-to-real\u8fc1\u79fb\u6027\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u5747\u5b9e\u73b0\u4e86\u9ad8\u4e00\u6b21\u6027\u6210\u529f\u7387\u548c\u957f\u65f6\u95f4\u91cd\u590d\u4efb\u52a1\u7684\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86RIT\u4efb\u52a1\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u3002"}}
{"id": "2507.06562", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06562", "abs": "https://arxiv.org/abs/2507.06562", "authors": ["Keita Yoneda", "Kento Kawaharazuka", "Temma Suzuki", "Takahiro Hattori", "Kei Okada"], "title": "KLEIYN : A Quadruped Robot with an Active Waist for Both Locomotion and Wall Climbing", "comment": "Accepted at IROS2025, website -\n  https://keitayoneda.github.io/kleiyn-chimney-climbing/, YouTube -\n  https://www.youtube.com/watch?v=vDmSfkazAvI", "summary": "In recent years, advancements in hardware have enabled quadruped robots to\noperate with high power and speed, while robust locomotion control using\nreinforcement learning (RL) has also been realized. As a result, expectations\nare rising for the automation of tasks such as material transport and\nexploration in unknown environments. However, autonomous locomotion in rough\nterrains with significant height variations requires vertical movement, and\nrobots capable of performing such movements stably, along with their control\nmethods, have not yet been fully established. In this study, we developed the\nquadruped robot KLEIYN, which features a waist joint, and aimed to expand\nquadruped locomotion by enabling chimney climbing through RL. To facilitate the\nlearning of vertical motion, we introduced Contact-Guided Curriculum Learning\n(CGCL). As a result, KLEIYN successfully climbed walls ranging from 800 mm to\n1000 mm in width at an average speed of 150 mm/s, 50 times faster than\nconventional robots. Furthermore, we demonstrated that the introduction of a\nwaist joint improves climbing performance, particularly enhancing tracking\nability on narrow walls.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u5177\u6709\u8170\u90e8\u5173\u8282\u7684\u56db\u8db3\u673a\u5668\u4ebaKLEIYN\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u5782\u76f4\u8fd0\u52a8\uff08\u5982\u70df\u56f1\u6500\u722c\uff09\uff0c\u5e76\u5f15\u5165\u63a5\u89e6\u5f15\u5bfc\u8bfe\u7a0b\u5b66\u4e60\uff08CGCL\uff09\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6500\u722c\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u56db\u8db3\u673a\u5668\u4eba\u5728\u5e73\u5766\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u63a7\u5236\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u5177\u6709\u663e\u8457\u9ad8\u5ea6\u53d8\u5316\u7684\u5d0e\u5c96\u5730\u5f62\u4e2d\u7a33\u5b9a\u5782\u76f4\u8fd0\u52a8\u7684\u673a\u5668\u4eba\u53ca\u63a7\u5236\u65b9\u6cd5\u5c1a\u672a\u6210\u719f\u3002", "method": "\u5f00\u53d1\u4e86\u5177\u6709\u8170\u90e8\u5173\u8282\u7684\u56db\u8db3\u673a\u5668\u4ebaKLEIYN\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u63a5\u89e6\u5f15\u5bfc\u8bfe\u7a0b\u5b66\u4e60\uff08CGCL\uff09\u65b9\u6cd5\uff0c\u8bad\u7ec3\u673a\u5668\u4eba\u8fdb\u884c\u5782\u76f4\u8fd0\u52a8\uff08\u5982\u70df\u56f1\u6500\u722c\uff09\u3002", "result": "KLEIYN\u6210\u529f\u6500\u722c\u5bbd\u5ea6\u4e3a800\u81f31000\u6beb\u7c73\u7684\u5899\u58c1\uff0c\u5e73\u5747\u901f\u5ea6\u4e3a150\u6beb\u7c73/\u79d2\uff0c\u6bd4\u4f20\u7edf\u673a\u5668\u4eba\u5feb50\u500d\uff1b\u8170\u90e8\u5173\u8282\u7684\u5f15\u5165\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5728\u72ed\u7a84\u5899\u58c1\u4e0a\u7684\u8ddf\u8e2a\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8170\u90e8\u5173\u8282\u548cCGCL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u5782\u76f4\u8fd0\u52a8\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u5730\u5f62\u4e0b\u7684\u81ea\u52a8\u5316\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002"}}
{"id": "2507.06535", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06535", "abs": "https://arxiv.org/abs/2507.06535", "authors": ["Shan Shen", "Shenglu Hua", "Jiajun Zou", "Jiawei Liu", "Jianwang Zhai", "Chuan Shi", "Wenjian Yu"], "title": "Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits", "comment": "Accepted by ICCAD2025. This is the initial version. Minor changes\n  will be made", "summary": "Graph representation learning on Analog-Mixed Signal (AMS) circuits is\ncrucial for various downstream tasks, e.g., parasitic estimation. However, the\nscarcity of design data, the unbalanced distribution of labels, and the\ninherent diversity of circuit implementations pose significant challenges to\nlearning robust and transferable circuit representations. To address these\nlimitations, we propose CircuitGCL, a novel graph contrastive learning\nframework that integrates representation scattering and label rebalancing to\nenhance transferability across heterogeneous circuit graphs. CircuitGCL employs\na self-supervised strategy to learn topology-invariant node embeddings through\nhyperspherical representation scattering, eliminating dependency on large-scale\ndata. Simultaneously, balanced mean squared error (MSE) and softmax\ncross-entropy (bsmCE) losses are introduced to mitigate label distribution\ndisparities between circuits, enabling robust and transferable parasitic\nestimation. Evaluated on parasitic capacitance estimation (edge-level task) and\nground capacitance classification (node-level task) across TSMC 28nm AMS\ndesigns, CircuitGCL outperforms all state-of-the-art (SOTA) methods, with the\n$R^2$ improvement of $33.64\\% \\sim 44.20\\%$ for edge regression and F1-score\ngain of $0.9\\times \\sim 2.1\\times$ for node classification. Our code is\navailable at\n\\href{https://anonymous.4open.science/r/CircuitGCL-099B/README.md}{here}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCircuitGCL\uff0c\u4e00\u79cd\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3AMS\u7535\u8def\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u3001\u6807\u7b7e\u4e0d\u5e73\u8861\u548c\u7535\u8def\u591a\u6837\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bc4\u751f\u4f30\u8ba1\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3AMS\u7535\u8def\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u6570\u636e\u7a00\u7f3a\u3001\u6807\u7b7e\u4e0d\u5e73\u8861\u548c\u7535\u8def\u591a\u6837\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u4ee5\u63d0\u5347\u7535\u8def\u8868\u793a\u7684\u9c81\u68d2\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "method": "\u63d0\u51faCircuitGCL\u6846\u67b6\uff0c\u7ed3\u5408\u8d85\u7403\u8868\u793a\u6563\u5c04\u548c\u6807\u7b7e\u91cd\u5e73\u8861\u6280\u672f\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u548c\u5e73\u8861MSE\u4e0ebsmCE\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728TSMC 28nm AMS\u8bbe\u8ba1\u4e0a\uff0cCircuitGCL\u5728\u5bc4\u751f\u7535\u5bb9\u4f30\u8ba1\u548c\u63a5\u5730\u7535\u5bb9\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cR\u00b2\u63d0\u534733.64%~44.20%\uff0cF1\u5206\u6570\u63d0\u53470.9~2.1\u500d\u3002", "conclusion": "CircuitGCL\u901a\u8fc7\u521b\u65b0\u7684\u8868\u793a\u5b66\u4e60\u548c\u6807\u7b7e\u5e73\u8861\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86AMS\u7535\u8def\u56fe\u7684\u8868\u793a\u5b66\u4e60\u6548\u679c\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2507.06342", "categories": ["cs.LG", "cs.AI", "math.DS", "math.SG"], "pdf": "https://arxiv.org/pdf/2507.06342", "abs": "https://arxiv.org/abs/2507.06342", "authors": ["M. A. Evangelista-Alvarado", "P. Su\u00e1rez-Serrato"], "title": "SymFlux: deep symbolic regression of Hamiltonian vector fields", "comment": "26 pages, 7 figures", "summary": "We present SymFlux, a novel deep learning framework that performs symbolic\nregression to identify Hamiltonian functions from their corresponding vector\nfields on the standard symplectic plane. SymFlux models utilize hybrid CNN-LSTM\narchitectures to learn and output the symbolic mathematical expression of the\nunderlying Hamiltonian. Training and validation are conducted on newly\ndeveloped datasets of Hamiltonian vector fields, a key contribution of this\nwork. Our results demonstrate the model's effectiveness in accurately\nrecovering these symbolic expressions, advancing automated discovery in\nHamiltonian mechanics.", "AI": {"tldr": "SymFlux\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7\u56de\u5f52\u4ece\u6807\u51c6\u8f9b\u5e73\u9762\u4e0a\u7684\u5411\u91cf\u573a\u8bc6\u522b\u54c8\u5bc6\u987f\u51fd\u6570\u3002", "motivation": "\u63a8\u52a8\u54c8\u5bc6\u987f\u529b\u5b66\u4e2d\u7684\u81ea\u52a8\u5316\u53d1\u73b0\uff0c\u89e3\u51b3\u4ece\u5411\u91cf\u573a\u4e2d\u6062\u590d\u54c8\u5bc6\u987f\u51fd\u6570\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u6df7\u5408CNN-LSTM\u67b6\u6784\uff0c\u4ece\u65b0\u5f00\u53d1\u7684\u54c8\u5bc6\u987f\u5411\u91cf\u573a\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u548c\u8f93\u51fa\u7b26\u53f7\u8868\u8fbe\u5f0f\u3002", "result": "\u6a21\u578b\u80fd\u51c6\u786e\u6062\u590d\u7b26\u53f7\u8868\u8fbe\u5f0f\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "SymFlux\u5728\u54c8\u5bc6\u987f\u529b\u5b66\u4e2d\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u53d1\u73b0\u7684\u8fdb\u6b65\u3002"}}
{"id": "2507.06564", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06564", "abs": "https://arxiv.org/abs/2507.06564", "authors": ["Tianshun Li", "Tianyi Huai", "Zhen Li", "Yichun Gao", "Haoang Li", "Xinhu Zheng"], "title": "SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments", "comment": "8 pages, 9 figures, has been accepted by IROS 2025", "summary": "Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across\nvarious sectors, driven by their mobility and adaptability. This paper\nintroduces SkyVLN, a novel framework integrating vision-and-language navigation\n(VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in\ncomplex urban environments. Unlike traditional navigation methods, SkyVLN\nleverages Large Language Models (LLMs) to interpret natural language\ninstructions and visual observations, enabling UAVs to navigate through dynamic\n3D spaces with improved accuracy and robustness. We present a multimodal\nnavigation agent equipped with a fine-grained spatial verbalizer and a history\npath memory mechanism. These components allow the UAV to disambiguate spatial\ncontexts, handle ambiguous instructions, and backtrack when necessary. The\nframework also incorporates an NMPC module for dynamic obstacle avoidance,\nensuring precise trajectory tracking and collision prevention. To validate our\napproach, we developed a high-fidelity 3D urban simulation environment using\nAirSim, featuring realistic imagery and dynamic urban elements. Extensive\nexperiments demonstrate that SkyVLN significantly improves navigation success\nrates and efficiency, particularly in new and unseen environments.", "AI": {"tldr": "SkyVLN\u6846\u67b6\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u548c\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5bfc\u822a\u65b9\u6cd5\u5728\u52a8\u60013D\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u89c6\u89c9\u89c2\u5bdf\uff0c\u914d\u5907\u7a7a\u95f4\u8bed\u8a00\u5316\u5668\u548c\u5386\u53f2\u8def\u5f84\u8bb0\u5fc6\u673a\u5236\uff0c\u5e76\u96c6\u6210NMPC\u6a21\u5757\u8fdb\u884c\u52a8\u6001\u907f\u969c\u3002", "result": "\u5728AirSim\u9ad8\u4fdd\u771f3D\u6a21\u62df\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0cSkyVLN\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "conclusion": "SkyVLN\u4e3a\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06538", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06538", "abs": "https://arxiv.org/abs/2507.06538", "authors": ["Shan Shen", "Yibin Zhang", "Hector Rodriguez Rodriguez", "Wenjian Yu"], "title": "Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction", "comment": "Published in Proceedings of DAC2025", "summary": "Graph representation learning is a powerful method to extract features from\ngraph-structured data, such as analog/mixed-signal (AMS) circuits. However,\ntraining deep learning models for AMS designs is severely limited by the\nscarcity of integrated circuit design data. In this work, we present\nCircuitGPS, a few-shot learning method for parasitic effect prediction in AMS\ncircuits. The circuit netlist is represented as a heterogeneous graph, with the\ncoupling capacitance modeled as a link. CircuitGPS is pre-trained on link\nprediction and fine-tuned on edge regression. The proposed method starts with a\nsmall-hop sampling technique that converts a link or a node into a subgraph.\nThen, the subgraph embeddings are learned with a hybrid graph Transformer.\nAdditionally, CircuitGPS integrates a low-cost positional encoding that\nsummarizes the positional and structural information of the sampled subgraph.\nCircuitGPS improves the accuracy of coupling existence by at least 20\\% and\nreduces the MAE of capacitance estimation by at least 0.067 compared to\nexisting methods. Our method demonstrates strong inherent scalability, enabling\ndirect application to diverse AMS circuit designs through zero-shot learning.\nFurthermore, the ablation studies provide valuable insights into graph models\nfor representation learning.", "AI": {"tldr": "CircuitGPS\u662f\u4e00\u79cd\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4bAMS\u7535\u8def\u4e2d\u7684\u5bc4\u751f\u6548\u5e94\uff0c\u901a\u8fc7\u5c0f\u8df3\u91c7\u6837\u548c\u6df7\u5408\u56feTransformer\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u7531\u4e8e\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u6570\u636e\u7a00\u7f3a\uff0c\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7528\u4e8eAMS\u8bbe\u8ba1\u53d7\u5230\u9650\u5236\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u5bc4\u751f\u6548\u5e94\u3002", "method": "\u5c06\u7535\u8def\u7f51\u8868\u8868\u793a\u4e3a\u5f02\u6784\u56fe\uff0c\u901a\u8fc7\u5c0f\u8df3\u91c7\u6837\u751f\u6210\u5b50\u56fe\uff0c\u4f7f\u7528\u6df7\u5408\u56feTransformer\u5b66\u4e60\u5b50\u56fe\u5d4c\u5165\uff0c\u5e76\u7ed3\u5408\u4f4e\u6210\u672c\u7684\u4f4d\u7f6e\u7f16\u7801\u3002", "result": "CircuitGPS\u5c06\u8026\u5408\u5b58\u5728\u9884\u6d4b\u7684\u51c6\u786e\u6027\u63d0\u9ad8\u4e86\u81f3\u5c1120%\uff0c\u7535\u5bb9\u4f30\u8ba1\u7684MAE\u964d\u4f4e\u4e86\u81f3\u5c110.067\u3002", "conclusion": "CircuitGPS\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u901a\u8fc7\u96f6\u6837\u672c\u5b66\u4e60\u76f4\u63a5\u5e94\u7528\u4e8e\u591a\u6837\u5316\u7684AMS\u7535\u8def\u8bbe\u8ba1\u3002"}}
{"id": "2507.06366", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.06366", "abs": "https://arxiv.org/abs/2507.06366", "authors": ["Yupu Zhang", "Zelin Xu", "Tingsong Xiao", "Gustavo Seabra", "Yanjun Li", "Chenglong Li", "Zhe Jiang"], "title": "DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction", "comment": null, "summary": "Predicting the binding affinity of protein-ligand complexes plays a vital\nrole in drug discovery. Unfortunately, progress has been hindered by the lack\nof large-scale and high-quality binding affinity labels. The widely used\nPDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,\nespecially graph contrastive learning (GCL), provides a unique opportunity to\nbreak the barrier by pre-training graph neural network models based on vast\nunlabeled complexes and fine-tuning the models on much fewer labeled complexes.\nHowever, the problem faces unique challenges, including a lack of a\ncomprehensive unlabeled dataset with well-defined positive/negative complex\npairs and the need to design GCL algorithms that incorporate the unique\ncharacteristics of such data. To fill the gap, we propose DecoyDB, a\nlarge-scale, structure-aware dataset specifically designed for self-supervised\nGCL on protein-ligand complexes. DecoyDB consists of high-resolution ground\ntruth complexes (less than 2.5 Angstrom) and diverse decoy structures with\ncomputationally generated binding poses that range from realistic to suboptimal\n(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation\n(RMSD) from the native pose. We further design a customized GCL framework to\npre-train graph neural networks based on DecoyDB and fine-tune the models with\nlabels from PDBbind. Extensive experiments confirm that models pre-trained with\nDecoyDB achieve superior accuracy, label efficiency, and generalizability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDecoyDB\u6570\u636e\u96c6\u548c\u5b9a\u5236\u5316GCL\u6846\u67b6\uff0c\u7528\u4e8e\u86cb\u767d\u8d28-\u914d\u4f53\u590d\u5408\u7269\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u86cb\u767d\u8d28-\u914d\u4f53\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u4e14\u6807\u7b7e\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u8fdb\u5c55\u3002\u81ea\u76d1\u7763\u5b66\u4e60\uff08\u5c24\u5176\u662f\u56fe\u5bf9\u6bd4\u5b66\u4e60\uff09\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u63d0\u51faDecoyDB\u6570\u636e\u96c6\uff0c\u5305\u542b\u9ad8\u5206\u8fa8\u7387\u771f\u5b9e\u590d\u5408\u7269\u548c\u591a\u6837\u5316\u8bf1\u9975\u7ed3\u6784\uff0c\u5e76\u8bbe\u8ba1\u5b9a\u5236\u5316GCL\u6846\u67b6\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eDecoyDB\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u6807\u7b7e\u6548\u7387\u548c\u6cdb\u5316\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DecoyDB\u548c\u5b9a\u5236\u5316GCL\u6846\u67b6\u586b\u8865\u4e86\u9886\u57df\u7a7a\u767d\uff0c\u4e3a\u86cb\u767d\u8d28-\u914d\u4f53\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06574", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06574", "abs": "https://arxiv.org/abs/2507.06574", "authors": ["Thomas Touma", "Ersin Da\u015f", "Erica Tevere", "Martin Feather", "Ksenia Kolcio", "Maurice Prather", "Alberto Candela", "Ashish Goel", "Erik Kramer", "Hari Nayar", "Lorraine Fesq", "Joel W. Burdick"], "title": "AI Space Cortex: An Experimental System for Future Era Space Exploration", "comment": null, "summary": "Our Robust, Explainable Autonomy for Scientific Icy Moon Operations (REASIMO)\neffort contributes to NASA's Concepts for Ocean worlds Life Detection\nTechnology (COLDTech) program, which explores science platform technologies for\nocean worlds such as Europa and Enceladus. Ocean world missions pose\nsignificant operational challenges. These include long communication lags,\nlimited power, and lifetime limitations caused by radiation damage and hostile\nconditions. Given these operational limitations, onboard autonomy will be vital\nfor future Ocean world missions. Besides the management of nominal lander\noperations, onboard autonomy must react appropriately in the event of\nanomalies. Traditional spacecraft rely on a transition into 'safe-mode' in\nwhich non-essential components and subsystems are powered off to preserve\nsafety and maintain communication with Earth. For a severely time-limited Ocean\nworld mission, resolutions to these anomalies that can be executed without\nEarth-in-the-loop communication and associated delays are paramount for\ncompletion of the mission objectives and science goals. To address these\nchallenges, the REASIMO effort aims to demonstrate a robust level of\nAI-assisted autonomy for such missions, including the ability to detect and\nrecover from anomalies, and to perform missions based on pre-trained behaviors\nrather than hard-coded, predetermined logic like all prior space missions. We\ndeveloped an AI-assisted, personality-driven, intelligent framework for control\nof an Ocean world mission by combining a mix of advanced technologies. To\ndemonstrate the capabilities of the framework, we perform tests of autonomous\nsampling operations on a lander-manipulator testbed at the NASA Jet Propulsion\nLaboratory, approximating possible surface conditions such a mission might\nencounter.", "AI": {"tldr": "REASIMO\u9879\u76ee\u4e3aNASA\u7684COLDTech\u8ba1\u5212\u5f00\u53d1AI\u8f85\u52a9\u81ea\u4e3b\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u6d77\u6d0b\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u901a\u4fe1\u5ef6\u8fdf\u3001\u80fd\u6e90\u9650\u5236\u548c\u8f90\u5c04\u95ee\u9898\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u884c\u4e3a\u548c\u5f02\u5e38\u68c0\u6d4b\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u6d77\u6d0b\u4e16\u754c\u4efb\u52a1\uff08\u5982\u6b27\u7f57\u5df4\u548c\u6069\u585e\u62c9\u8fbe\u65af\uff09\u9762\u4e34\u901a\u4fe1\u5ef6\u8fdf\u3001\u80fd\u6e90\u9650\u5236\u548c\u6076\u52a3\u73af\u5883\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u5b89\u5168\u6a21\u5f0f\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u81ea\u4e3b\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408AI\u6280\u672f\u548c\u9884\u8bad\u7ec3\u884c\u4e3a\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u667a\u80fd\u6846\u67b6\uff0c\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\u548c\u6062\u590d\uff0c\u5e76\u5728NASA\u55b7\u6c14\u63a8\u8fdb\u5b9e\u9a8c\u5ba4\u7684\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u81ea\u4e3b\u91c7\u6837\u64cd\u4f5c\u3002", "result": "\u6210\u529f\u5c55\u793a\u4e86AI\u8f85\u52a9\u81ea\u4e3b\u7cfb\u7edf\u5728\u6a21\u62df\u6d77\u6d0b\u4e16\u754c\u8868\u9762\u6761\u4ef6\u4e0b\u7684\u64cd\u4f5c\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5f02\u5e38\u5904\u7406\u548c\u884c\u4e3a\u6267\u884c\u7684\u53ef\u884c\u6027\u3002", "conclusion": "REASIMO\u6846\u67b6\u4e3a\u672a\u6765\u6d77\u6d0b\u4e16\u754c\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u81ea\u4e3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u79d1\u5b66\u76ee\u6807\u8fbe\u6210\u80fd\u529b\u3002"}}
{"id": "2507.06549", "categories": ["cs.LG", "cs.AR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06549", "abs": "https://arxiv.org/abs/2507.06549", "authors": ["Shan Shen", "Dingcheng Yang", "Yuyang Xie", "Chunyan Pei", "Wenjian Yu", "Bei Yu"], "title": "Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs", "comment": "Published in Proceedings of GLSVLSI2024", "summary": "To achieve higher system energy efficiency, SRAM in SoCs is often customized.\nThe parasitic effects cause notable discrepancies between pre-layout and\npost-layout circuit simulations, leading to difficulty in converging design\nparameters and excessive design iterations. Is it possible to well predict the\nparasitics based on the pre-layout circuit, so as to perform parasitic-aware\npre-layout simulation? In this work, we propose a deep-learning-based 2-stage\nmodel to accurately predict these parasitics in pre-layout stages. The model\ncombines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron\n(MLP) regressors, effectively managing class imbalance of the net parasitics in\nSRAM circuits. We also employ Focal Loss to mitigate the impact of abundant\ninternal net samples and integrate subcircuit information into the graph to\nabstract the hierarchical structure of schematics. Experiments on 4 real SRAM\ndesigns show that our approach not only surpasses the state-of-the-art model in\nparasitic prediction by a maximum of 19X reduction of error but also\nsignificantly boosts the simulation process by up to 598X speedup.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u76842\u9636\u6bb5\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u9884\u5e03\u5c40\u9636\u6bb5\u51c6\u786e\u9884\u6d4bSRAM\u7535\u8def\u4e2d\u7684\u5bc4\u751f\u6548\u5e94\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u62df\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u9884\u5e03\u5c40\u548c\u540e\u5e03\u5c40\u7535\u8def\u6a21\u62df\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u5f02\uff0c\u51cf\u5c11\u8bbe\u8ba1\u8fed\u4ee3\u6b21\u6570\uff0c\u63d0\u9ad8\u7cfb\u7edf\u80fd\u6548\u3002", "method": "\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5206\u7c7b\u5668\u548c\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u56de\u5f52\u5668\uff0c\u4f7f\u7528Focal Loss\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u5e76\u6574\u5408\u5b50\u7535\u8def\u4fe1\u606f\u4ee5\u62bd\u8c61\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u57284\u4e2a\u5b9e\u9645SRAM\u8bbe\u8ba1\u4e2d\uff0c\u6a21\u578b\u5c06\u5bc4\u751f\u9884\u6d4b\u8bef\u5dee\u6700\u5927\u964d\u4f4e19\u500d\uff0c\u6a21\u62df\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe598\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5bc4\u751f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6a21\u62df\u6548\u7387\uff0c\u4e3aSRAM\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.06367", "categories": ["cs.LG", "math.AG"], "pdf": "https://arxiv.org/pdf/2507.06367", "abs": "https://arxiv.org/abs/2507.06367", "authors": ["El Mehdi Achour", "Kathl\u00e9n Kohn", "Holger Rauhut"], "title": "The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks", "comment": null, "summary": "We study geometric properties of the gradient flow for learning deep linear\nconvolutional networks. For linear fully connected networks, it has been shown\nrecently that the corresponding gradient flow on parameter space can be written\nas a Riemannian gradient flow on function space (i.e., on the product of weight\nmatrices) if the initialization satisfies a so-called balancedness condition.\nWe establish that the gradient flow on parameter space for learning linear\nconvolutional networks can be written as a Riemannian gradient flow on function\nspace regardless of the initialization. This result holds for $D$-dimensional\nconvolutions with $D \\geq 2$, and for $D =1$ it holds if all so-called strides\nof the convolutions are greater than one. The corresponding Riemannian metric\ndepends on the initialization.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6df1\u5ea6\u7ebf\u6027\u5377\u79ef\u7f51\u7edc\u68af\u5ea6\u6d41\u7684\u51e0\u4f55\u6027\u8d28\uff0c\u53d1\u73b0\u53c2\u6570\u7a7a\u95f4\u7684\u68af\u5ea6\u6d41\u53ef\u4ee5\u8868\u793a\u4e3a\u51fd\u6570\u7a7a\u95f4\u7684\u9ece\u66fc\u68af\u5ea6\u6d41\uff0c\u4e14\u4e0d\u53d7\u521d\u59cb\u5316\u6761\u4ef6\u9650\u5236\u3002", "motivation": "\u63a2\u7d22\u7ebf\u6027\u5377\u79ef\u7f51\u7edc\u68af\u5ea6\u6d41\u7684\u51e0\u4f55\u7279\u6027\uff0c\u7279\u522b\u662f\u4e0e\u521d\u59cb\u5316\u6761\u4ef6\u7684\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u5206\u6790\u53c2\u6570\u7a7a\u95f4\u548c\u51fd\u6570\u7a7a\u95f4\u7684\u68af\u5ea6\u6d41\u5173\u7cfb\uff0c\u7814\u7a76\u4e0d\u540c\u7ef4\u5ea6\u5377\u79ef\u548c\u6b65\u957f\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u68af\u5ea6\u6d41\u53ef\u4ee5\u8868\u793a\u4e3a\u9ece\u66fc\u68af\u5ea6\u6d41\uff0c\u4e14\u5bf9\u521d\u59cb\u5316\u65e0\u8981\u6c42\uff0c\u9002\u7528\u4e8eD\u22652\u7ef4\u5377\u79ef\u6216\u6b65\u957f\u5927\u4e8e1\u7684\u4e00\u7ef4\u5377\u79ef\u3002", "conclusion": "\u7ebf\u6027\u5377\u79ef\u7f51\u7edc\u7684\u68af\u5ea6\u6d41\u5177\u6709\u51e0\u4f55\u4e0d\u53d8\u6027\uff0c\u9ece\u66fc\u5ea6\u91cf\u4f9d\u8d56\u4e8e\u521d\u59cb\u5316\u3002"}}
{"id": "2507.06605", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06605", "abs": "https://arxiv.org/abs/2507.06605", "authors": ["Xinyu Wu"], "title": "Growing Trees with an Agent: Accelerating RRTs with Learned, Multi-Step Episodic Exploration", "comment": null, "summary": "Classical sampling-based motion planners like the RRTs suffer from\ninefficiencies, particularly in cluttered or high-dimensional spaces, due to\ntheir reliance on undirected, random sampling. This paper introduces the\nEpisodic RRT, a novel hybrid planning framework that replaces the primitive of\na random point with a learned, multi-step \"exploratory episode\" generated by a\nDeep Reinforcement Learning agent. By making the DRL agent the engine of\nexploration, ERRT transforms the search process from a diffuse, volumetric\nexpansion into a directed, branch-like growth. This paradigm shift yields key\nadvantages: it counters the curse of dimensionality with focused exploration,\nminimizes expensive collision checks by proactively proposing locally valid\npaths, and improves connectivity by generating inherently connected path\nsegments. We demonstrate through extensive empirical evaluation across 2D, 3D,\nand 6D environments that ERRT and its variants consistently and significantly\noutperform their classical counterparts. In a challenging 6D robotic arm\nscenario, ERRT achieves a 98% success rate compared to 19% for RRT, is up to\n107x faster, reduces collision checks by over 99.6%, and finds initial paths\nthat are nearly 50% shorter. Furthermore, its asymptotically optimal variant,\nERRT*, demonstrates vastly superior anytime performance, refining solutions to\nnear-optimality up to 29x faster than standard RRT* in 3D environments. Code:\nhttps://xinyuwuu.github.io/Episodic_RRT/.", "AI": {"tldr": "Episodic RRT (ERRT) \u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u66ff\u6362\u968f\u673a\u91c7\u6837\u4e3a\u591a\u6b65\u63a2\u7d22\u6027\u7247\u6bb5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u89c4\u5212\u7684\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u91c7\u6837\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff08\u5982RRT\uff09\u5728\u9ad8\u7ef4\u6216\u590d\u6742\u73af\u5883\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u4e3b\u8981\u4f9d\u8d56\u968f\u673a\u91c7\u6837\u5bfc\u81f4\u63a2\u7d22\u65b9\u5411\u4e0d\u660e\u786e\u3002", "method": "ERRT\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u751f\u6210\u591a\u6b65\u63a2\u7d22\u6027\u7247\u6bb5\uff0c\u5c06\u641c\u7d22\u8fc7\u7a0b\u4ece\u968f\u673a\u6269\u5c55\u8f6c\u53d8\u4e3a\u5b9a\u5411\u5206\u652f\u5f0f\u589e\u957f\u3002", "result": "\u57282D\u30013D\u548c6D\u73af\u5883\u4e2d\uff0cERRT\u53ca\u5176\u53d8\u4f53\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c6D\u673a\u68b0\u81c2\u573a\u666f\u4e2d\u6210\u529f\u7387\u4ece19%\u63d0\u5347\u81f398%\uff0c\u901f\u5ea6\u63d0\u5347107\u500d\uff0c\u78b0\u649e\u68c0\u67e5\u51cf\u5c1199.6%\uff0c\u8def\u5f84\u957f\u5ea6\u7f29\u77ed\u8fd150%\u3002", "conclusion": "ERRT\u901a\u8fc7\u5b9a\u5411\u63a2\u7d22\u548c\u9ad8\u6548\u8def\u5f84\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u89c4\u5212\u7684\u6027\u80fd\uff0c\u5176\u6700\u4f18\u53d8\u4f53ERRT*\u5728\u4f18\u5316\u901f\u5ea6\u4e0a\u6bd4RRT*\u5feb29\u500d\u3002"}}
{"id": "2507.06380", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06380", "abs": "https://arxiv.org/abs/2507.06380", "authors": ["Habibur Rahaman", "Atri Chatterjee", "Swarup Bhunia"], "title": "Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation", "comment": "7 pages, 7 figures", "summary": "Complex neural networks require substantial memory to store a large number of\nsynaptic weights. This work introduces WINGs (Automatic Weight Generator for\nSecure and Storage-Efficient Deep Learning Models), a novel framework that\ndynamically generates layer weights in a fully connected neural network (FC)\nand compresses the weights in convolutional neural networks (CNNs) during\ninference, significantly reducing memory requirements without sacrificing\naccuracy. WINGs framework uses principal component analysis (PCA) for\ndimensionality reduction and lightweight support vector regression (SVR) models\nto predict layer weights in the FC networks, removing the need for storing\nfull-weight matrices and achieving substantial memory savings. It also\npreferentially compresses the weights in low-sensitivity layers of CNNs using\nPCA and SVR with sensitivity analysis. The sensitivity-aware design also offers\nan added level of security, as any bit-flip attack with weights in compressed\nlayers has an amplified and readily detectable effect on accuracy. WINGs\nachieves 53x compression for the FC layers and 28x for AlexNet with MNIST\ndataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.\nThis significant reduction in memory results in higher throughput and lower\nenergy for DNN inference, making it attractive for resource-constrained edge\napplications.", "AI": {"tldr": "WINGs\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u751f\u6210\u5168\u8fde\u63a5\u5c42\u6743\u91cd\u548c\u538b\u7f29\u5377\u79ef\u5c42\u6743\u91cd\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u56e0\u5b58\u50a8\u5927\u91cf\u6743\u91cd\u800c\u5360\u7528\u8fc7\u591a\u5185\u5b58\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u5e94\u7528\u3002", "method": "\u4f7f\u7528PCA\u964d\u7ef4\u548c\u8f7b\u91cf\u7ea7SVR\u6a21\u578b\u9884\u6d4b\u6743\u91cd\uff0c\u7ed3\u5408\u654f\u611f\u6027\u5206\u6790\u4f18\u5148\u538b\u7f29CNN\u4f4e\u654f\u611f\u6027\u5c42\u3002", "result": "\u5b9e\u73b0\u5168\u8fde\u63a5\u5c4253\u500d\u538b\u7f29\uff0cAlexNet\u5728MNIST\u4e0a28\u500d\u538b\u7f29\uff0cCIFAR-10\u4e0a18\u500d\u538b\u7f29\uff0c\u7cbe\u5ea6\u635f\u59311-2%\u3002", "conclusion": "WINGs\u663e\u8457\u964d\u4f4e\u5185\u5b58\u9700\u6c42\uff0c\u63d0\u9ad8\u63a8\u7406\u541e\u5410\u91cf\u5e76\u51cf\u5c11\u80fd\u8017\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u3002"}}
{"id": "2507.06625", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06625", "abs": "https://arxiv.org/abs/2507.06625", "authors": ["Shizhe Cai", "Jayadeep Jacob", "Zeya Yin", "Fabio Ramos"], "title": "Q-STAC: Q-Guided Stein Variational Model Predictive Actor-Critic", "comment": "9 pages, 10 figures", "summary": "Deep reinforcement learning has shown remarkable success in continuous\ncontrol tasks, yet often requires extensive training data, struggles with\ncomplex, long-horizon planning, and fails to maintain safety constraints during\noperation. Meanwhile, Model Predictive Control (MPC) offers explainability and\nconstraint satisfaction, but typically yields only locally optimal solutions\nand demands careful cost function design. This paper introduces the Q-guided\nSTein variational model predictive Actor-Critic (Q-STAC), a novel framework\nthat bridges these approaches by integrating Bayesian MPC with actor-critic\nreinforcement learning through constrained Stein Variational Gradient Descent\n(SVGD). Our method optimizes control sequences directly using learned Q-values\nas objectives, eliminating the need for explicit cost function design while\nleveraging known system dynamics to enhance sample efficiency and ensure\ncontrol signals remain within safe boundaries. Extensive experiments on 2D\nnavigation and robotic manipulation tasks demonstrate that Q-STAC achieves\nsuperior sample efficiency, robustness, and optimality compared to\nstate-of-the-art algorithms, while maintaining the high expressiveness of\npolicy distributions. Experiment videos are available on our website:\nhttps://sites.google.com/view/q-stac", "AI": {"tldr": "Q-STAC\u6846\u67b6\u7ed3\u5408\u8d1d\u53f6\u65afMPC\u4e0e\u6f14\u5458-\u8bc4\u8bba\u5bb6\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u7ea6\u675fStein\u53d8\u5206\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u63a7\u5236\u5e8f\u5217\uff0c\u63d0\u5347\u6837\u672c\u6548\u7387\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u6570\u636e\u9700\u6c42\u5927\u3001\u957f\u65f6\u89c4\u5212\u590d\u6742\u53ca\u5b89\u5168\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5f25\u8865MPC\u4ec5\u5c40\u90e8\u6700\u4f18\u4e14\u9700\u7cbe\u5fc3\u8bbe\u8ba1\u6210\u672c\u51fd\u6570\u7684\u5c40\u9650\u3002", "method": "\u63d0\u51faQ-STAC\u6846\u67b6\uff0c\u96c6\u6210\u8d1d\u53f6\u65afMPC\u4e0e\u6f14\u5458-\u8bc4\u8bba\u5bb6\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u7ea6\u675fSVGD\u4f18\u5316\u63a7\u5236\u5e8f\u5217\uff0c\u4ee5\u5b66\u4e60\u5230\u7684Q\u503c\u66ff\u4ee3\u663e\u5f0f\u6210\u672c\u51fd\u6570\u8bbe\u8ba1\u3002", "result": "\u57282D\u5bfc\u822a\u4e0e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cQ-STAC\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u6700\u4f18\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7b56\u7565\u5206\u5e03\u7684\u9ad8\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "Q-STAC\u6210\u529f\u7ed3\u5408\u4e86\u5f3a\u5316\u5b66\u4e60\u4e0eMPC\u7684\u4f18\u52bf\uff0c\u4e3a\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b89\u5168\u4e14\u65e0\u9700\u663e\u5f0f\u6210\u672c\u8bbe\u8ba1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06694", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06694", "abs": "https://arxiv.org/abs/2507.06694", "authors": ["Raffael Theiler", "Olga Fink"], "title": "Heterogeneous Graph Neural Networks for Short-term State Forecasting in Power Systems across Domains and Time Scales: A Hydroelectric Power Plant Case Study", "comment": "25 pages, 9 figures", "summary": "Accurate short-term state forecasting is essential for efficient and stable\noperation of modern power systems, especially in the context of increasing\nvariability introduced by renewable and distributed energy resources. As these\nsystems evolve rapidly, it becomes increasingly important to reliably predict\ntheir states in the short term to ensure operational stability, support control\ndecisions, and enable interpretable monitoring of sensor and machine behavior.\nModern power systems often span multiple physical domains - including\nelectrical, mechanical, hydraulic, and thermal - posing significant challenges\nfor modeling and prediction. Graph Neural Networks (GNNs) have emerged as a\npromising data-driven framework for system state estimation and state\nforecasting in such settings. By leveraging the topological structure of sensor\nnetworks, GNNs can implicitly learn inter-sensor relationships and propagate\ninformation across the network. However, most existing GNN-based methods are\ndesigned under the assumption of homogeneous sensor relationships and are\ntypically constrained to a single physical domain. This limitation restricts\ntheir ability to integrate and reason over heterogeneous sensor data commonly\nencountered in real-world energy systems, such as those used in energy\nconversion infrastructure. In this work, we propose the use of Heterogeneous\nGraph Attention Networks to address these limitations. Our approach models both\nhomogeneous intra-domain and heterogeneous inter-domain relationships among\nsensor data from two distinct physical domains - hydraulic and electrical -\nwhich exhibit fundamentally different temporal dynamics. Experimental results\ndemonstrate that our method significantly outperforms conventional baselines on\naverage by 35.5% in terms of normalized root mean square error, confirming its\neffectiveness in multi-domain, multi-rate power system state forecasting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f02\u6784\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u9886\u57df\u7535\u529b\u7cfb\u7edf\u72b6\u6001\u9884\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u7535\u529b\u7cfb\u7edf\u56e0\u53ef\u518d\u751f\u80fd\u6e90\u548c\u5206\u5e03\u5f0f\u80fd\u6e90\u7684\u5f15\u5165\u800c\u53d8\u5f97\u590d\u6742\uff0c\u9700\u8981\u53ef\u9760\u7684\u77ed\u671f\u72b6\u6001\u9884\u6d4b\u4ee5\u786e\u4fdd\u7a33\u5b9a\u8fd0\u884c\u3002\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u4f20\u611f\u5668\u5173\u7cfb\u540c\u8d28\u4e14\u5c40\u9650\u4e8e\u5355\u4e00\u9886\u57df\uff0c\u65e0\u6cd5\u5904\u7406\u771f\u5b9e\u7cfb\u7edf\u4e2d\u7684\u5f02\u6784\u6570\u636e\u3002", "method": "\u91c7\u7528\u5f02\u6784\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08Heterogeneous Graph Attention Networks\uff09\uff0c\u5efa\u6a21\u540c\u8d28\u9886\u57df\u5185\u548c\u5f02\u8d28\u9886\u57df\u95f4\u7684\u4f20\u611f\u5668\u5173\u7cfb\uff0c\u7279\u522b\u9488\u5bf9\u6db2\u538b\u548c\u7535\u6c14\u4e24\u4e2a\u9886\u57df\u7684\u4e0d\u540c\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5f52\u4e00\u5316\u5747\u65b9\u6839\u8bef\u5dee\u4e0a\u5e73\u5747\u6bd4\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e8635.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u9886\u57df\u3001\u591a\u901f\u7387\u7535\u529b\u7cfb\u7edf\u72b6\u6001\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.06381", "categories": ["cs.LG", "cs.AI", "math.DS", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.06381", "abs": "https://arxiv.org/abs/2507.06381", "authors": ["James Hazelden", "Laura Driscoll", "Eli Shlizerman", "Eric Shea-Brown"], "title": "KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks", "comment": null, "summary": "Gradient Descent (GD) and its variants are the primary tool for enabling\nefficient training of recurrent dynamical systems such as Recurrent Neural\nNetworks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics\nthat are formed in these models exhibit features such as neural collapse and\nemergence of latent representations that may support the remarkable\ngeneralization properties of networks. In neuroscience, qualitative features of\nthese representations are used to compare learning in biological and artificial\nsystems. Despite recent progress, there remains a need for theoretical tools to\nrigorously understand the mechanisms shaping learned representations,\nespecially in finite, non-linear models. Here, we show that the gradient flow,\nwhich describes how the model's dynamics evolve over GD, can be decomposed into\na product that involves two operators: a Parameter Operator, K, and a\nLinearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in\nfeed-forward neural networks, while P appears in Lyapunov stability and optimal\ncontrol theory. We demonstrate two applications of our decomposition. First, we\nshow how their interplay gives rise to low-dimensional latent dynamics under\nGD, and, specifically, how the collapse is a result of the network structure,\nover and above the nature of the underlying task. Second, for multi-task\ntraining, we show that the operators can be used to measure how objectives\nrelevant to individual sub-tasks align. We experimentally and theoretically\nvalidate these findings, providing an efficient Pytorch package, \\emph{KPFlow},\nimplementing robust analysis tools for general recurrent architectures. Taken\ntogether, our work moves towards building a next stage of understanding of GD\nlearning in non-linear recurrent models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u68af\u5ea6\u6d41\u5206\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u5faa\u73af\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u5b66\u4e60\u673a\u5236\uff0c\u63ed\u793a\u4e86\u4f4e\u7ef4\u6f5c\u5728\u52a8\u6001\u548c\u591a\u4efb\u52a1\u5bf9\u9f50\u7684\u673a\u5236\u3002", "motivation": "\u7406\u89e3\u68af\u5ea6\u4e0b\u964d\u5728\u975e\u7ebf\u6027\u5faa\u73af\u6a21\u578b\uff08\u5982RNNs\u3001Neural ODEs\u3001GRUs\uff09\u4e2d\u5982\u4f55\u5851\u9020\u5b66\u4e60\u8868\u793a\uff0c\u5c24\u5176\u662f\u795e\u7ecf\u5d29\u6e83\u548c\u6f5c\u5728\u52a8\u6001\u7684\u673a\u5236\u3002", "method": "\u5c06\u68af\u5ea6\u6d41\u5206\u89e3\u4e3a\u4e24\u4e2a\u7b97\u5b50\uff08\u53c2\u6570\u7b97\u5b50K\u548c\u7ebf\u6027\u5316\u6d41\u4f20\u64ad\u5b50P\uff09\uff0c\u5206\u6790\u5176\u76f8\u4e92\u4f5c\u7528\u53ca\u5176\u5bf9\u5b66\u4e60\u52a8\u6001\u7684\u5f71\u54cd\u3002", "result": "\u63ed\u793a\u4e86\u4f4e\u7ef4\u6f5c\u5728\u52a8\u6001\u7684\u5f62\u6210\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u4efb\u52a1\u8bad\u7ec3\u4e2d\u76ee\u6807\u5bf9\u9f50\u7684\u5ea6\u91cf\u65b9\u6cd5\u3002\u5b9e\u9a8c\u548c\u7406\u8bba\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u53d1\u73b0\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7406\u89e3\u975e\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u4e2d\u7684\u68af\u5ea6\u4e0b\u964d\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5e94\u7684\u5206\u6790\u5de5\u5177\u5305KPFlow\u3002"}}
{"id": "2507.06690", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06690", "abs": "https://arxiv.org/abs/2507.06690", "authors": ["Guobin Zhu", "Rui Zhou", "Wenkang Ji", "Hongyin Zhang", "Donglin Wang", "Shiyu Zhao"], "title": "Multi-Task Multi-Agent Reinforcement Learning via Skill Graphs", "comment": "Conditionally accepted by IEEE Robotics and Automation Letters", "summary": "Multi-task multi-agent reinforcement learning (MT-MARL) has recently gained\nattention for its potential to enhance MARL's adaptability across multiple\ntasks. However, it is challenging for existing multi-task learning methods to\nhandle complex problems, as they are unable to handle unrelated tasks and\npossess limited knowledge transfer capabilities. In this paper, we propose a\nhierarchical approach that efficiently addresses these challenges. The\nhigh-level module utilizes a skill graph, while the low-level module employs a\nstandard MARL algorithm. Our approach offers two contributions. First, we\nconsider the MT-MARL problem in the context of unrelated tasks, expanding the\nscope of MTRL. Second, the skill graph is used as the upper layer of the\nstandard hierarchical approach, with training independent of the lower layer,\neffectively handling unrelated tasks and enhancing knowledge transfer\ncapabilities. Extensive experiments are conducted to validate these advantages\nand demonstrate that the proposed method outperforms the latest hierarchical\nMAPPO algorithms. Videos and code are available at\nhttps://github.com/WindyLab/MT-MARL-SG", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u65b9\u6cd5\uff0c\u901a\u8fc7\u6280\u80fd\u56fe\u548c\u9ad8\u5c42\u6a21\u5757\u89e3\u51b3\u591a\u4efb\u52a1\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u65e0\u5173\u4efb\u52a1\u548c\u77e5\u8bc6\u8f6c\u79fb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u65e0\u5173\u4efb\u52a1\u4e14\u77e5\u8bc6\u8f6c\u79fb\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\uff0c\u9ad8\u5c42\u6a21\u5757\u4f7f\u7528\u6280\u80fd\u56fe\uff0c\u4f4e\u5c42\u6a21\u5757\u91c7\u7528\u6807\u51c6MARL\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u65b0\u7684\u5206\u5c42MAPPO\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7684\u8303\u56f4\uff0c\u63d0\u5347\u4e86\u65e0\u5173\u4efb\u52a1\u5904\u7406\u80fd\u529b\u548c\u77e5\u8bc6\u8f6c\u79fb\u6548\u679c\u3002"}}
{"id": "2507.06750", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06750", "abs": "https://arxiv.org/abs/2507.06750", "authors": ["Tohid Kargar Tasooji", "Ramviyas Parasuraman"], "title": "Distributed Fault-Tolerant Multi-Robot Cooperative Localization in Adversarial Environments", "comment": "Accepted to IROS 2025 Conference", "summary": "In multi-robot systems (MRS), cooperative localization is a crucial task for\nenhancing system robustness and scalability, especially in GPS-denied or\ncommunication-limited environments. However, adversarial attacks, such as\nsensor manipulation, and communication jamming, pose significant challenges to\nthe performance of traditional localization methods. In this paper, we propose\na novel distributed fault-tolerant cooperative localization framework to\nenhance resilience against sensor and communication disruptions in adversarial\nenvironments. We introduce an adaptive event-triggered communication strategy\nthat dynamically adjusts communication thresholds based on real-time sensing\nand communication quality. This strategy ensures optimal performance even in\nthe presence of sensor degradation or communication failure. Furthermore, we\nconduct a rigorous analysis of the convergence and stability properties of the\nproposed algorithm, demonstrating its resilience against bounded adversarial\nzones and maintaining accurate state estimation. Robotarium-based experiment\nresults show that our proposed algorithm significantly outperforms traditional\nmethods in terms of localization accuracy and communication efficiency,\nparticularly in adversarial settings. Our approach offers improved scalability,\nreliability, and fault tolerance for MRS, making it suitable for large-scale\ndeployments in real-world, challenging environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u5bb9\u9519\u534f\u4f5c\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4e8b\u4ef6\u89e6\u53d1\u901a\u4fe1\u7b56\u7565\u63d0\u5347\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5728GPS\u7f3a\u5931\u6216\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u5b9a\u4f4d\u65b9\u6cd5\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff08\u5982\u4f20\u611f\u5668\u64cd\u7eb5\u548c\u901a\u4fe1\u5e72\u6270\uff09\u5f71\u54cd\uff0c\u9700\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u4e8b\u4ef6\u89e6\u53d1\u901a\u4fe1\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u901a\u4fe1\u9608\u503c\uff0c\u5e76\u7ed3\u5408\u5b9e\u65f6\u611f\u77e5\u548c\u901a\u4fe1\u8d28\u91cf\u5206\u6790\uff0c\u786e\u4fdd\u7b97\u6cd5\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u901a\u4fe1\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3001\u53ef\u9760\u6027\u548c\u5bb9\u9519\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2507.06402", "categories": ["cs.LG", "cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.06402", "abs": "https://arxiv.org/abs/2507.06402", "authors": ["Siddhant Deshpande", "Yalemzerf Getnet", "Waltenegus Dargie"], "title": "Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning", "comment": null, "summary": "With the proliferation of wireless electrocardiogram (ECG) systems for health\nmonitoring and authentication, protecting signal integrity against tampering is\nbecoming increasingly important. This paper analyzes the performance of CNN,\nResNet, and hybrid Transformer-CNN models for tamper detection. It also\nevaluates the performance of a Siamese network for ECG based identity\nverification. Six tampering strategies, including structured segment\nsubstitutions and random insertions, are emulated to mimic real world attacks.\nThe one-dimensional ECG signals are transformed into a two dimensional\nrepresentation in the time frequency domain using the continuous wavelet\ntransform (CWT). The models are trained and evaluated using ECG data from 54\nsubjects recorded in four sessions 2019 to 2025 outside of clinical settings\nwhile the subjects performed seven different daily activities. Experimental\nresults show that in highly fragmented manipulation scenarios, CNN,\nFeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding\n99.5 percent . Similarly, for subtle manipulations (for example, 50 percent\nfrom A and 50 percent from B and, 75 percent from A and 25 percent from B\nsubstitutions) our FeatCNN-TranCNN model demonstrated consistently reliable\nperformance, achieving an average accuracy of 98 percent . For identity\nverification, the pure Transformer-Siamese network achieved an average accuracy\nof 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model\ndelivered perfect verification performance with 100 percent accuracy.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u7ebf\u5fc3\u7535\u56fe\uff08ECG\uff09\u4fe1\u53f7\u7684\u9632\u7be1\u6539\u68c0\u6d4b\u548c\u8eab\u4efd\u9a8c\u8bc1\uff0c\u6bd4\u8f83\u4e86CNN\u3001ResNet\u548cTransformer-CNN\u6df7\u5408\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u79cd\u7be1\u6539\u7b56\u7565\u4e0b\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "motivation": "\u968f\u7740\u65e0\u7ebfECG\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u4fdd\u62a4\u4fe1\u53f7\u5b8c\u6574\u6027\u514d\u53d7\u7be1\u6539\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528CNN\u3001ResNet\u548cTransformer-CNN\u6a21\u578b\u8fdb\u884c\u7be1\u6539\u68c0\u6d4b\uff0cSiamese\u7f51\u7edc\u8fdb\u884c\u8eab\u4efd\u9a8c\u8bc1\u3002ECG\u4fe1\u53f7\u901a\u8fc7\u8fde\u7eed\u5c0f\u6ce2\u53d8\u6362\uff08CWT\uff09\u8f6c\u6362\u4e3a\u65f6\u9891\u57df\u4e8c\u7ef4\u8868\u793a\u3002", "result": "\u5728\u9ad8\u5ea6\u788e\u7247\u5316\u7be1\u6539\u573a\u666f\u4e0b\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8d85\u8fc799.5%\uff1b\u5bf9\u4e8e\u7ec6\u5fae\u7be1\u6539\uff0cFeatCNN-TranCNN\u6a21\u578b\u5e73\u5747\u51c6\u786e\u7387\u4e3a98%\u3002\u8eab\u4efd\u9a8c\u8bc1\u4e2d\uff0cCNN-Transformer\u6df7\u5408Siamese\u6a21\u578b\u8fbe\u5230100%\u51c6\u786e\u7387\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u5728ECG\u9632\u7be1\u6539\u548c\u8eab\u4efd\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662fCNN-Transformer Siamese\u6a21\u578b\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u9a8c\u8bc1\u3002"}}
{"id": "2507.06700", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.06700", "abs": "https://arxiv.org/abs/2507.06700", "authors": ["Pranav Pandey", "Ramviyas Parasuraman", "Prashant Doshi"], "title": "Integrating Perceptions: A Human-Centered Physical Safety Model for Human-Robot Interaction", "comment": "Accepted to IEEE RO-MAN 2025 Conference", "summary": "Ensuring safety in human-robot interaction (HRI) is essential to foster user\ntrust and enable the broader adoption of robotic systems. Traditional safety\nmodels primarily rely on sensor-based measures, such as relative distance and\nvelocity, to assess physical safety. However, these models often fail to\ncapture subjective safety perceptions, which are shaped by individual traits\nand contextual factors. In this paper, we introduce and analyze a parameterized\ngeneral safety model that bridges the gap between physical and perceived safety\nby incorporating a personalization parameter, $\\rho$, into the safety\nmeasurement framework to account for individual differences in safety\nperception. Through a series of hypothesis-driven human-subject studies in a\nsimulated rescue scenario, we investigate how emotional state, trust, and robot\nbehavior influence perceived safety. Our results show that $\\rho$ effectively\ncaptures meaningful individual differences, driven by affective responses,\ntrust in task consistency, and clustering into distinct user types.\nSpecifically, our findings confirm that predictable and consistent robot\nbehavior as well as the elicitation of positive emotional states, significantly\nenhance perceived safety. Moreover, responses cluster into a small number of\nuser types, supporting adaptive personalization based on shared safety models.\nNotably, participant role significantly shapes safety perception, and repeated\nexposure reduces perceived safety for participants in the casualty role,\nemphasizing the impact of physical interaction and experiential change. These\nfindings highlight the importance of adaptive, human-centered safety models\nthat integrate both psychological and behavioral dimensions, offering a pathway\ntoward more trustworthy and effective HRI in safety-critical domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u901a\u7528\u5b89\u5168\u6a21\u578b\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u53c2\u6570\u03c1\u7ed3\u5408\u7269\u7406\u4e0e\u611f\u77e5\u5b89\u5168\uff0c\u7814\u7a76\u53d1\u73b0\u673a\u5668\u4eba\u884c\u4e3a\u7684\u53ef\u9884\u6d4b\u6027\u3001\u60c5\u611f\u72b6\u6001\u548c\u7528\u6237\u7c7b\u578b\u5bf9\u611f\u77e5\u5b89\u5168\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u5b89\u5168\u6a21\u578b\u4ec5\u4f9d\u8d56\u4f20\u611f\u5668\u6570\u636e\uff0c\u672a\u80fd\u6355\u6349\u4e3b\u89c2\u5b89\u5168\u611f\u77e5\uff0c\u9700\u7ed3\u5408\u4e2a\u4f53\u5dee\u5f02\u548c\u60c5\u5883\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u6551\u63f4\u573a\u666f\u7684\u4eba\u4f53\u5b9e\u9a8c\uff0c\u7814\u7a76\u60c5\u611f\u72b6\u6001\u3001\u4fe1\u4efb\u548c\u673a\u5668\u4eba\u884c\u4e3a\u5bf9\u611f\u77e5\u5b89\u5168\u7684\u5f71\u54cd\uff0c\u5f15\u5165\u53c2\u6570\u03c1\u91cf\u5316\u4e2a\u4f53\u5dee\u5f02\u3002", "result": "\u03c1\u80fd\u6709\u6548\u6355\u6349\u4e2a\u4f53\u5dee\u5f02\uff0c\u673a\u5668\u4eba\u884c\u4e3a\u7684\u53ef\u9884\u6d4b\u6027\u548c\u79ef\u6781\u60c5\u611f\u72b6\u6001\u663e\u8457\u63d0\u5347\u611f\u77e5\u5b89\u5168\uff0c\u7528\u6237\u54cd\u5e94\u53ef\u5206\u4e3a\u5c11\u6570\u7c7b\u578b\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u5f00\u53d1\u7ed3\u5408\u5fc3\u7406\u4e0e\u884c\u4e3a\u7ef4\u5ea6\u7684\u81ea\u9002\u5e94\u5b89\u5168\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u53ef\u4fe1\u5ea6\u548c\u6548\u679c\u3002"}}
{"id": "2507.06787", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06787", "abs": "https://arxiv.org/abs/2507.06787", "authors": ["Sean Smith", "Emmanuel Witrant", "Ya-Jun Pan"], "title": "Stream Function-Based Navigation for Complex Quadcopter Obstacle Avoidance", "comment": null, "summary": "This article presents a novel stream function-based navigational control\nsystem for obstacle avoidance, where obstacles are represented as\ntwo-dimensional (2D) rigid surfaces in inviscid, incompressible flows. The\napproach leverages the vortex panel method (VPM) and incorporates safety\nmargins to control the stream function and flow properties around virtual\nsurfaces, enabling navigation in complex, partially observed environments using\nreal-time sensing. To address the limitations of the VPM in managing relative\ndistance and avoiding rapidly accelerating obstacles at close proximity, the\nsystem integrates a model predictive controller (MPC) based on higher-order\ncontrol barrier functions (HOCBF). This integration incorporates VPM trajectory\ngeneration, state estimation, and constraint handling into a receding-horizon\noptimization problem. The 2D rigid surfaces are enclosed using minimum bounding\nellipses (MBEs), while an adaptive Kalman filter (AKF) captures and predicts\nobstacle dynamics, propagating these estimates into the MPC-HOCBF for rapid\navoidance maneuvers. Evaluation is conducted using a PX4-powered Clover drone\nGazebo simulator and real-time experiments involving a COEX Clover quadcopter\nequipped with a 360 degree LiDAR sensor.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u51fd\u6570\u7684\u5bfc\u822a\u63a7\u5236\u7cfb\u7edf\uff0c\u7ed3\u5408\u6da1\u6d41\u9762\u677f\u6cd5\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u5b9e\u73b0\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u907f\u969c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6da1\u6d41\u9762\u677f\u6cd5\u5728\u8fd1\u8ddd\u79bb\u907f\u969c\u548c\u5feb\u901f\u79fb\u52a8\u969c\u788d\u7269\u5904\u7406\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408\u6da1\u6d41\u9762\u677f\u6cd5\uff08VPM\uff09\u548c\u57fa\u4e8e\u9ad8\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08HOCBF\uff09\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff08MPC\uff09\uff0c\u5229\u7528\u6700\u5c0f\u5305\u56f4\u692d\u5706\uff08MBE\uff09\u548c\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08AKF\uff09\u5904\u7406\u969c\u788d\u7269\u52a8\u6001\u3002", "result": "\u5728PX4\u9a71\u52a8\u7684Clover\u65e0\u4eba\u673aGazebo\u6a21\u62df\u5668\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5904\u7406\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u907f\u969c\u95ee\u9898\u3002"}}
{"id": "2507.06432", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06432", "abs": "https://arxiv.org/abs/2507.06432", "authors": ["Mingcheng Zhu", "Yu Liu", "Zhiyao Luo", "Tingting Zhu"], "title": "Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction", "comment": null, "summary": "Artificial Intelligence has revolutionised critical care for common\nconditions. Yet, rare conditions in the intensive care unit (ICU), including\nrecognised rare diseases and low-prevalence conditions in the ICU, remain\nunderserved due to data scarcity and intra-condition heterogeneity. To bridge\nsuch gaps, we developed KnowRare, a domain adaptation-based deep learning\nframework for predicting clinical outcomes for rare conditions in the ICU.\nKnowRare mitigates data scarcity by initially learning condition-agnostic\nrepresentations from diverse electronic health records through self-supervised\npre-training. It addresses intra-condition heterogeneity by selectively\nadapting knowledge from clinically similar conditions with a developed\ncondition knowledge graph. Evaluated on two ICU datasets across five clinical\nprediction tasks (90-day mortality, 30-day readmission, ICU mortality,\nremaining length of stay, and phenotyping), KnowRare consistently outperformed\nexisting state-of-the-art models. Additionally, KnowRare demonstrated superior\npredictive performance compared to established ICU scoring systems, including\nAPACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in\nadapting its parameters to accommodate dataset-specific and task-specific\ncharacteristics, its generalisation to common conditions under limited data\nscenarios, and its rationality in selecting source conditions. These findings\nhighlight KnowRare's potential as a robust and practical solution for\nsupporting clinical decision-making and improving care for rare conditions in\nthe ICU.", "AI": {"tldr": "KnowRare\u662f\u4e00\u4e2a\u57fa\u4e8e\u9886\u57df\u9002\u5e94\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4bICU\u4e2d\u7f55\u89c1\u75be\u75c5\u7684\u4e34\u5e8a\u7ed3\u679c\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u6761\u4ef6\u77e5\u8bc6\u56fe\u8c31\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u548cICU\u8bc4\u5206\u7cfb\u7edf\u3002", "motivation": "ICU\u4e2d\u7f55\u89c1\u75be\u75c5\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u5f02\u8d28\u6027\u672a\u5f97\u5230\u5145\u5206\u670d\u52a1\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u65b9\u6cd5\u4ee5\u6539\u5584\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "KnowRare\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5b66\u4e60\u6761\u4ef6\u65e0\u5173\u8868\u793a\uff0c\u5e76\u5229\u7528\u6761\u4ef6\u77e5\u8bc6\u56fe\u8c31\u9009\u62e9\u6027\u9002\u5e94\u4e34\u5e8a\u76f8\u4f3c\u6761\u4ef6\u7684\u77e5\u8bc6\u3002", "result": "\u5728\u4e94\u4e2a\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cKnowRare\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u548cICU\u8bc4\u5206\u7cfb\u7edf\uff0c\u5e76\u5c55\u793a\u4e86\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "KnowRare\u662f\u652f\u6301ICU\u7f55\u89c1\u75be\u75c5\u4e34\u5e8a\u51b3\u7b56\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u6f5c\u529b\u6539\u5584\u62a4\u7406\u8d28\u91cf\u3002"}}
{"id": "2507.06710", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06710", "abs": "https://arxiv.org/abs/2507.06710", "authors": ["Zhenyang Liu", "Yikai Wang", "Kuanning Wang", "Longfei Liang", "Xiangyang Xue", "Yanwei Fu"], "title": "Spatial-Temporal Aware Visuomotor Diffusion Policy Learning", "comment": null, "summary": "Visual imitation learning is effective for robots to learn versatile tasks.\nHowever, many existing methods rely on behavior cloning with supervised\nhistorical trajectories, limiting their 3D spatial and 4D spatiotemporal\nawareness. Consequently, these methods struggle to capture the 3D structures\nand 4D spatiotemporal relationships necessary for real-world deployment. In\nthis work, we propose 4D Diffusion Policy (DP4), a novel visual imitation\nlearning method that incorporates spatiotemporal awareness into diffusion-based\npolicies. Unlike traditional approaches that rely on trajectory cloning, DP4\nleverages a dynamic Gaussian world model to guide the learning of 3D spatial\nand 4D spatiotemporal perceptions from interactive environments. Our method\nconstructs the current 3D scene from a single-view RGB-D observation and\npredicts the future 3D scene, optimizing trajectory generation by explicitly\nmodeling both spatial and temporal dependencies. Extensive experiments across\n17 simulation tasks with 173 variants and 3 real-world robotic tasks\ndemonstrate that the 4D Diffusion Policy (DP4) outperforms baseline methods,\nimproving the average simulation task success rate by 16.4% (Adroit), 14%\n(DexArt), and 6.45% (RLBench), and the average real-world robotic task success\nrate by 8.6%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a4D Diffusion Policy (DP4)\u7684\u65b0\u578b\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u7a7a\u611f\u77e5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u57283D\u7a7a\u95f4\u548c4D\u65f6\u7a7a\u5173\u7cfb\u4e2d\u7684\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u884c\u4e3a\u514b\u9686\u548c\u5386\u53f2\u8f68\u8ff9\u76d1\u7763\uff0c\u7f3a\u4e4f\u5bf93D\u7ed3\u6784\u548c4D\u65f6\u7a7a\u5173\u7cfb\u7684\u6355\u6349\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "method": "DP4\u91c7\u7528\u52a8\u6001\u9ad8\u65af\u4e16\u754c\u6a21\u578b\uff0c\u4ece\u4ea4\u4e92\u73af\u5883\u4e2d\u5b66\u4e603D\u7a7a\u95f4\u548c4D\u65f6\u7a7a\u611f\u77e5\uff0c\u901a\u8fc7\u5355\u89c6\u89d2RGB-D\u89c2\u6d4b\u6784\u5efa\u5f53\u524d3D\u573a\u666f\u5e76\u9884\u6d4b\u672a\u67653D\u573a\u666f\uff0c\u4f18\u5316\u8f68\u8ff9\u751f\u6210\u3002", "result": "\u572817\u4e2a\u4eff\u771f\u4efb\u52a1\u548c3\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0cDP4\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4eff\u771f\u4efb\u52a1\u5e73\u5747\u6210\u529f\u7387\u63d0\u534716.4%\uff08Adroit\uff09\u300114%\uff08DexArt\uff09\u548c6.45%\uff08RLBench\uff09\uff0c\u771f\u5b9e\u4efb\u52a1\u63d0\u53478.6%\u3002", "conclusion": "DP4\u901a\u8fc7\u5f15\u5165\u65f6\u7a7a\u611f\u77e5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u679c\uff0c\u4e3a\u673a\u5668\u4eba\u4efb\u52a1\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2507.06433", "categories": ["cs.LG", "eess.SP", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.06433", "abs": "https://arxiv.org/abs/2507.06433", "authors": ["Niloy Sikder", "Paul Zerr", "Mahdad Jafarzadeh Esfahani", "Martin Dresler", "Matthias Krauledat"], "title": "eegFloss: A Python package for refining sleep EEG recordings using machine learning models", "comment": "The eegFloss package is available under the MIT License at\n  https://github.com/Niloy333/eegFloss", "summary": "Electroencephalography (EEG) allows monitoring of brain activity, providing\ninsights into the functional dynamics of various brain regions and their roles\nin cognitive processes. EEG is a cornerstone in sleep research, serving as the\nprimary modality of polysomnography, the gold standard in the field. However,\nEEG signals are prone to artifacts caused by both internal (device-specific)\nfactors and external (environmental) interferences. As sleep studies are\nbecoming larger, most rely on automatic sleep staging, a process highly\nsusceptible to artifacts, leading to erroneous sleep scores. This paper\naddresses this challenge by introducing eegFloss, an open-source Python package\nto utilize eegUsability, a novel machine learning (ML) model designed to detect\nsegments with artifacts in sleep EEG recordings. eegUsability has been trained\nand evaluated on manually artifact-labeled EEG data collected from 15\nparticipants over 127 nights using the Zmax headband. It demonstrates solid\noverall classification performance (F1-score is approximately 0.85, Cohens\nkappa is 0.78), achieving a high recall rate of approximately 94% in\nidentifying channel-wise usable EEG data, and extends beyond Zmax.\nAdditionally, eegFloss offers features such as automatic time-in-bed detection\nusing another ML model named eegMobility, filtering out certain artifacts, and\ngenerating hypnograms and sleep statistics. By addressing a fundamental\nchallenge faced by most sleep studies, eegFloss can enhance the precision and\nrigor of their analysis as well as the accuracy and reliability of their\noutcomes.", "AI": {"tldr": "eegFloss\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5305\uff0c\u7528\u4e8e\u68c0\u6d4b\u7761\u7720EEG\u8bb0\u5f55\u4e2d\u7684\u4f2a\u5f71\uff0c\u63d0\u9ad8\u7761\u7720\u7814\u7a76\u7684\u51c6\u786e\u6027\u3002", "motivation": "EEG\u4fe1\u53f7\u6613\u53d7\u4f2a\u5f71\u5e72\u6270\uff0c\u5f71\u54cd\u81ea\u52a8\u7761\u7720\u5206\u671f\u7684\u51c6\u786e\u6027\uff0ceegFloss\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528eegUsability\u673a\u5668\u5b66\u4e60\u6a21\u578b\u68c0\u6d4b\u4f2a\u5f71\uff0c\u5e76\u7ed3\u5408eegMobility\u6a21\u578b\u81ea\u52a8\u68c0\u6d4b\u5367\u5e8a\u65f6\u95f4\u3002", "result": "eegUsability\u8868\u73b0\u51fa\u9ad8\u5206\u7c7b\u6027\u80fd\uff08F1-score\u7ea60.85\uff09\uff0c\u80fd\u51c6\u786e\u8bc6\u522b\u53ef\u7528EEG\u6570\u636e\uff08\u53ec\u56de\u7387\u7ea694%\uff09\u3002", "conclusion": "eegFloss\u901a\u8fc7\u51cf\u5c11\u4f2a\u5f71\u5e72\u6270\uff0c\u63d0\u5347\u4e86\u7761\u7720\u7814\u7a76\u7684\u5206\u6790\u7cbe\u5ea6\u548c\u7ed3\u679c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.06747", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06747", "abs": "https://arxiv.org/abs/2507.06747", "authors": ["Daojie Peng", "Jiahang Cao", "Qiang Zhang", "Jun Ma"], "title": "LOVON: Legged Open-Vocabulary Object Navigator", "comment": "9 pages, 10 figures; Project Page:\n  https://daojiepeng.github.io/LOVON/", "summary": "Object navigation in open-world environments remains a formidable and\npervasive challenge for robotic systems, particularly when it comes to\nexecuting long-horizon tasks that require both open-world object detection and\nhigh-level task planning. Traditional methods often struggle to integrate these\ncomponents effectively, and this limits their capability to deal with complex,\nlong-range navigation missions. In this paper, we propose LOVON, a novel\nframework that integrates large language models (LLMs) for hierarchical task\nplanning with open-vocabulary visual detection models, tailored for effective\nlong-range object navigation in dynamic, unstructured environments. To tackle\nreal-world challenges including visual jittering, blind zones, and temporary\ntarget loss, we design dedicated solutions such as Laplacian Variance Filtering\nfor visual stabilization. We also develop a functional execution logic for the\nrobot that guarantees LOVON's capabilities in autonomous navigation, task\nadaptation, and robust task completion. Extensive evaluations demonstrate the\nsuccessful completion of long-sequence tasks involving real-time detection,\nsearch, and navigation toward open-vocabulary dynamic targets. Furthermore,\nreal-world experiments across different legged robots (Unitree Go2, B2, and\nH1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.", "AI": {"tldr": "LOVON\u662f\u4e00\u4e2a\u96c6\u6210\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5f00\u653e\u8bcd\u6c47\u89c6\u89c9\u68c0\u6d4b\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u957f\u8ddd\u79bb\u7269\u4f53\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u6574\u5408\u7269\u4f53\u68c0\u6d4b\u548c\u9ad8\u7ea7\u4efb\u52a1\u89c4\u5212\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408LLMs\u8fdb\u884c\u5206\u5c42\u4efb\u52a1\u89c4\u5212\uff0c\u91c7\u7528\u5f00\u653e\u8bcd\u6c47\u89c6\u89c9\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u89c6\u89c9\u7a33\u5b9a\u5316\u548c\u529f\u80fd\u6267\u884c\u903b\u8f91\u3002", "result": "\u6210\u529f\u5b8c\u6210\u6d89\u53ca\u5b9e\u65f6\u68c0\u6d4b\u3001\u641c\u7d22\u548c\u5bfc\u822a\u7684\u957f\u5e8f\u5217\u4efb\u52a1\uff0c\u5e76\u5728\u4e0d\u540c\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u517c\u5bb9\u6027\u3002", "conclusion": "LOVON\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.06445", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06445", "abs": "https://arxiv.org/abs/2507.06445", "authors": ["Victoria R. Li", "Jenny Kaufmann", "Martin Wattenberg", "David Alvarez-Melis", "Naomi Saphra"], "title": "Can Interpretation Predict Behavior on Unseen Data?", "comment": null, "summary": "Interpretability research often aims to predict how a model will respond to\ntargeted interventions on specific mechanisms. However, it rarely predicts how\na model will respond to unseen input data. This paper explores the promises and\nchallenges of interpretability as a tool for predicting out-of-distribution\n(OOD) model behavior. Specifically, we investigate the correspondence between\nattention patterns and OOD generalization in hundreds of Transformer models\nindependently trained on a synthetic classification task. These models exhibit\nseveral distinct systematic generalization rules OOD, forming a diverse\npopulation for correlational analysis. In this setting, we find that simple\nobservational tools from interpretability can predict OOD performance. In\nparticular, when in-distribution attention exhibits hierarchical patterns, the\nmodel is likely to generalize hierarchically on OOD data -- even when the\nrule's implementation does not rely on these hierarchical patterns, according\nto ablation tests. Our findings offer a proof-of-concept to motivate further\ninterpretability work on predicting unseen model behavior.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5728\u9884\u6d4b\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\uff08OOD\uff09\u4e0a\u884c\u4e3a\u7684\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u901a\u8fc7\u5206\u6790Transformer\u6a21\u578b\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u4e0eOOD\u6cdb\u5316\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u7b80\u5355\u7684\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u53ef\u4ee5\u9884\u6d4bOOD\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u53ef\u89e3\u91ca\u6027\u662f\u5426\u80fd\u9884\u6d4b\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u884c\u4e3a\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u5728\u9884\u6d4bOOD\u884c\u4e3a\u4e0a\u7684\u7a7a\u767d\u3002", "method": "\u5728\u5408\u6210\u5206\u7c7b\u4efb\u52a1\u4e0a\u72ec\u7acb\u8bad\u7ec3\u6570\u767e\u4e2aTransformer\u6a21\u578b\uff0c\u5206\u6790\u5176\u6ce8\u610f\u529b\u6a21\u5f0f\u4e0eOOD\u6cdb\u5316\u7684\u76f8\u5173\u6027\u3002", "result": "\u53d1\u73b0\u5f53\u6a21\u578b\u5728\u5206\u5e03\u5185\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u5c42\u6b21\u5316\u6ce8\u610f\u529b\u6a21\u5f0f\u65f6\uff0c\u5176\u5728OOD\u6570\u636e\u4e0a\u4e5f\u53ef\u80fd\u5c42\u6b21\u5316\u6cdb\u5316\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u9884\u6d4b\u672a\u89c1\u6a21\u578b\u884c\u4e3a\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u9f13\u52b1\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.06449", "categories": ["cs.LG", "cs.AI", "cs.DC", "68T05, 68T07, 68Q85, 94A08", "I.2.6; I.2.11; C.2.4"], "pdf": "https://arxiv.org/pdf/2507.06449", "abs": "https://arxiv.org/abs/2507.06449", "authors": ["Qianyu Long", "Qiyuan Wang", "Christos Anagnostopoulos", "Daning Bi"], "title": "FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models", "comment": "12 pages, 8 figures, 5 tables. This paper introduces FedPhD, a novel\n  hierarchical federated learning framework for training diffusion models that\n  addresses data heterogeneity and communication costs through\n  homogeneity-aware aggregation and structured pruning. Submitted to IEEE\n  Transactions on Cybernetics and is under review", "summary": "Federated Learning (FL), as a distributed learning paradigm, trains models\nover distributed clients' data. FL is particularly beneficial for distributed\ntraining of Diffusion Models (DMs), which are high-quality image generators\nthat require diverse data. However, challenges such as high communication costs\nand data heterogeneity persist in training DMs similar to training Transformers\nand Convolutional Neural Networks. Limited research has addressed these issues\nin FL environments. To address this gap and challenges, we introduce a novel\napproach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD\nleverages Hierarchical FL with homogeneity-aware model aggregation and\nselection policy to tackle data heterogeneity while reducing communication\ncosts. The distributed structured pruning of FedPhD enhances computational\nefficiency and reduces model storage requirements in clients. Our experiments\nacross multiple datasets demonstrate that FedPhD achieves high model\nperformance regarding Fr\\'echet Inception Distance (FID) scores while reducing\ncommunication costs by up to $88\\%$. FedPhD outperforms baseline methods\nachieving at least a $34\\%$ improvement in FID, while utilizing only $56\\%$ of\nthe total computation and communication resources.", "AI": {"tldr": "FedPhD\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u6548\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff08DMs\uff09\uff0c\u901a\u8fc7\u5206\u5c42\u8054\u90a6\u5b66\u4e60\u548c\u540c\u8d28\u6027\u611f\u77e5\u6a21\u578b\u805a\u5408\u89e3\u51b3\u6570\u636e\u5f02\u8d28\u6027\u548c\u9ad8\u901a\u4fe1\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u5728\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u73af\u5883\u4e2d\u8bad\u7ec3\u65f6\u9762\u4e34\u6570\u636e\u5f02\u8d28\u6027\u548c\u9ad8\u901a\u4fe1\u6210\u672c\u7684\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u5bf9\u6b64\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "FedPhD\u91c7\u7528\u5206\u5c42\u8054\u90a6\u5b66\u4e60\uff0c\u7ed3\u5408\u540c\u8d28\u6027\u611f\u77e5\u6a21\u578b\u805a\u5408\u548c\u9009\u62e9\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u5f0f\u7ed3\u6784\u5316\u526a\u679d\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u51cf\u5c11\u5ba2\u6237\u7aef\u5b58\u50a8\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedPhD\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff08\u9ad8\u8fbe88%\uff09\uff0c\u540c\u65f6FID\u5206\u6570\u63d0\u5347\u81f3\u5c1134%\uff0c\u4ec5\u4f7f\u752856%\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u8d44\u6e90\u3002", "conclusion": "FedPhD\u4e3aFL\u73af\u5883\u4e2d\u9ad8\u6548\u8bad\u7ec3DMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2507.06458", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.06458", "abs": "https://arxiv.org/abs/2507.06458", "authors": ["Arjun Banerjee", "David Martinez", "Camille Dang", "Ethan Tam"], "title": "Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models", "comment": "15 pages, 13 figures. Accepted to Proceedings of the Workshop on\n  Generative AI for Biology at the 42nd International Conference on Machine\n  Learning (Spotlight)", "summary": "Protein language models (PLMs) encode rich biological information, yet their\ninternal neuron representations are poorly understood. We introduce the first\nautomated framework for labeling every neuron in a PLM with biologically\ngrounded natural language descriptions. Unlike prior approaches relying on\nsparse autoencoders or manual annotation, our method scales to hundreds of\nthousands of neurons, revealing individual neurons are selectively sensitive to\ndiverse biochemical and structural properties. We then develop a novel neuron\nactivation-guided steering method to generate proteins with desired traits,\nenabling convergence to target biochemical properties like molecular weight and\ninstability index as well as secondary and tertiary structural motifs,\nincluding alpha helices and canonical Zinc Fingers. We finally show that\nanalysis of labeled neurons in different model sizes reveals PLM scaling laws\nand a structured neuron space distribution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u4e2d\u7684\u6bcf\u4e2a\u795e\u7ecf\u5143\u63d0\u4f9b\u751f\u7269\u5b66\u57fa\u7840\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5143\u6fc0\u6d3b\u7684\u5f15\u5bfc\u65b9\u6cd5\uff0c\u4ee5\u751f\u6210\u5177\u6709\u76ee\u6807\u7279\u6027\u7684\u86cb\u767d\u8d28\u3002", "motivation": "\u7406\u89e3PLM\u5185\u90e8\u795e\u7ecf\u5143\u7684\u751f\u7269\u5b66\u610f\u4e49\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u6307\u5bfc\u86cb\u767d\u8d28\u8bbe\u8ba1\u3002", "method": "\u5f15\u5165\u81ea\u52a8\u5316\u6846\u67b6\u6807\u6ce8\u795e\u7ecf\u5143\uff0c\u5f00\u53d1\u795e\u7ecf\u5143\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u751f\u6210\u76ee\u6807\u86cb\u767d\u8d28\u3002", "result": "\u63ed\u793a\u4e86\u795e\u7ecf\u5143\u5bf9\u591a\u6837\u751f\u5316\u7279\u6027\u7684\u9009\u62e9\u6027\u654f\u611f\uff0c\u5b9e\u73b0\u4e86\u76ee\u6807\u751f\u5316\u7279\u6027\u548c\u7ed3\u6784\u57fa\u5e8f\u7684\u751f\u6210\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63ed\u793a\u4e86PLM\u7684\u795e\u7ecf\u5143\u5206\u5e03\u89c4\u5f8b\uff0c\u8fd8\u4e3a\u86cb\u767d\u8d28\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.06822", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06822", "abs": "https://arxiv.org/abs/2507.06822", "authors": ["Wei Xu", "Yanchao Zhao", "Weichao Guo", "Xinjun Sheng"], "title": "Hierarchical Reinforcement Learning for Articulated Tool Manipulation with Multifingered Hand", "comment": "Accepted by 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025). copyright 2025 IEEE. Final version to appear\n  in IEEE Xplore", "summary": "Manipulating articulated tools, such as tweezers or scissors, has rarely been\nexplored in previous research. Unlike rigid tools, articulated tools change\ntheir shape dynamically, creating unique challenges for dexterous robotic\nhands. In this work, we present a hierarchical, goal-conditioned reinforcement\nlearning (GCRL) framework to improve the manipulation capabilities of\nanthropomorphic robotic hands using articulated tools. Our framework comprises\ntwo policy layers: (1) a low-level policy that enables the dexterous hand to\nmanipulate the tool into various configurations for objects of different sizes,\nand (2) a high-level policy that defines the tool's goal state and controls the\nrobotic arm for object-picking tasks. We employ an encoder, trained on\nsynthetic pointclouds, to estimate the tool's affordance states--specifically,\nhow different tool configurations (e.g., tweezer opening angles) enable\ngrasping of objects of varying sizes--from input point clouds, thereby enabling\nprecise tool manipulation. We also utilize a privilege-informed heuristic\npolicy to generate replay buffer, improving the training efficiency of the\nhigh-level policy. We validate our approach through real-world experiments,\nshowing that the robot can effectively manipulate a tweezer-like tool to grasp\nobjects of diverse shapes and sizes with a 70.8 % success rate. This study\nhighlights the potential of RL to advance dexterous robotic manipulation of\narticulated tools.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u62df\u4eba\u5316\u673a\u68b0\u624b\u64cd\u4f5c\u94f0\u63a5\u5de5\u5177\u7684\u80fd\u529b\uff0c\u5b9e\u9a8c\u6210\u529f\u7387\u4e3a70.8%\u3002", "motivation": "\u94f0\u63a5\u5de5\u5177\uff08\u5982\u954a\u5b50\u6216\u526a\u5200\uff09\u7684\u52a8\u6001\u5f62\u72b6\u53d8\u5316\u4e3a\u673a\u68b0\u624b\u64cd\u4f5c\u5e26\u6765\u72ec\u7279\u6311\u6218\uff0c\u6b64\u524d\u7814\u7a76\u8f83\u5c11\u6d89\u53ca\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7b56\u7565\uff1a\u4f4e\u5c42\u7b56\u7565\u63a7\u5236\u5de5\u5177\u914d\u7f6e\uff0c\u9ad8\u5c42\u7b56\u7565\u5b9a\u4e49\u76ee\u6807\u72b6\u6001\u5e76\u63a7\u5236\u673a\u68b0\u81c2\uff1b\u4f7f\u7528\u7f16\u7801\u5668\u4f30\u8ba1\u5de5\u5177\u7684\u53ef\u64cd\u4f5c\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u542f\u53d1\u5f0f\u7b56\u7565\u751f\u6210\u56de\u653e\u7f13\u51b2\u533a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u673a\u5668\u4eba\u80fd\u6709\u6548\u64cd\u4f5c\u954a\u5b50\u72b6\u5de5\u5177\u6293\u53d6\u4e0d\u540c\u5f62\u72b6\u548c\u5927\u5c0f\u7684\u7269\u4f53\uff0c\u6210\u529f\u7387\u4e3a70.8%\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u94f0\u63a5\u5de5\u5177\u64cd\u4f5c\u80fd\u529b\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.06461", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.06461", "abs": "https://arxiv.org/abs/2507.06461", "authors": ["Risi Jaiswal", "Supriyo Datta", "Joseph G. Makin"], "title": "Energy-Efficient Supervised Learning with a Binary Stochastic Forward-Forward Algorithm", "comment": "24 pages, 5 figures, 4 tables. Under review", "summary": "Reducing energy consumption has become a pressing need for modern machine\nlearning, which has achieved many of its most impressive results by scaling to\nlarger and more energy-consumptive neural networks. Unfortunately, the main\nalgorithm for training such networks, backpropagation, poses significant\nchallenges for custom hardware accelerators, due to both its serial\ndependencies and the memory footprint needed to store forward activations for\nthe backward pass. Alternatives to backprop, although less effective, do exist;\nhere the main computational bottleneck becomes matrix multiplication. In this\nstudy, we derive forward-forward algorithms for binary, stochastic units.\nBinarization of the activations transforms matrix multiplications into indexing\noperations, which can be executed efficiently in hardware. Stochasticity,\ncombined with tied weights across units with different biases, bypasses the\ninformation bottleneck imposed by binary units. Furthermore, although slow and\nexpensive in traditional hardware, binary sampling that is very fast can be\nimplemented cheaply with p-bits (probabilistic bits), novel devices made up of\nunstable magnets. We evaluate our proposed algorithms on the MNIST,\nFashion-MNIST, and CIFAR-10 datasets, showing that its performance is close to\nreal-valued forward-forward, but with an estimated energy savings of about one\norder of magnitude.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u524d\u5411-\u524d\u5411\u7b97\u6cd5\u7684\u4e8c\u8fdb\u5236\u968f\u673a\u5355\u5143\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u91cf\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a5\u8fd1\u5b9e\u6570\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u56e0\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u80fd\u8017\u95ee\u9898\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\uff0c\u4f20\u7edf\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u5728\u786c\u4ef6\u52a0\u901f\u4e0a\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e8c\u8fdb\u5236\u968f\u673a\u5355\u5143\u7684\u524d\u5411-\u524d\u5411\u7b97\u6cd5\uff0c\u901a\u8fc7\u6fc0\u6d3b\u4e8c\u503c\u5316\u548c\u968f\u673a\u6027\u4f18\u5316\u77e9\u9635\u4e58\u6cd5\uff0c\u5229\u7528p-bit\u786c\u4ef6\u9ad8\u6548\u5b9e\u73b0\u3002", "result": "\u5728MNIST\u3001Fashion-MNIST\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u63a5\u8fd1\u5b9e\u6570\u7b97\u6cd5\uff0c\u80fd\u8017\u964d\u4f4e\u7ea6\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u4e8c\u8fdb\u5236\u968f\u673a\u5355\u5143\u7684\u524d\u5411-\u524d\u5411\u7b97\u6cd5\u662f\u4e00\u79cd\u9ad8\u6548\u8282\u80fd\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u5408\u786c\u4ef6\u52a0\u901f\u3002"}}
{"id": "2507.06824", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06824", "abs": "https://arxiv.org/abs/2507.06824", "authors": ["Gabriel Arslan Waltersson", "Yiannis Karayiannidis"], "title": "Friction Estimation for In-Hand Planar Motion", "comment": null, "summary": "This paper presents a method for online estimation of contact properties\nduring in-hand sliding manipulation with a parallel gripper. We estimate the\nstatic and Coulomb friction as well as the contact radius from tactile\nmeasurements of contact forces and sliding velocities. The method is validated\nin both simulation and real-world experiments. Furthermore, we propose a\nheuristic to deal with fast slip-stick dynamics which can adversely affect the\nestimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u4f30\u8ba1\u5e73\u884c\u5939\u6301\u5668\u6ed1\u52a8\u64cd\u4f5c\u4e2d\u63a5\u89e6\u7279\u6027\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u9759\u6469\u64e6\u3001\u5e93\u4ed1\u6469\u64e6\u548c\u63a5\u89e6\u534a\u5f84\u3002", "motivation": "\u4e3a\u4e86\u5728\u6ed1\u52a8\u64cd\u4f5c\u4e2d\u5b9e\u65f6\u4f30\u8ba1\u63a5\u89e6\u7279\u6027\uff0c\u4ee5\u63d0\u9ad8\u5939\u6301\u5668\u7684\u63a7\u5236\u7cbe\u5ea6\u548c\u9002\u5e94\u6027\u3002", "method": "\u901a\u8fc7\u89e6\u89c9\u6d4b\u91cf\u63a5\u89e6\u529b\u548c\u6ed1\u52a8\u901f\u5ea6\uff0c\u4f30\u8ba1\u9759\u6469\u64e6\u3001\u5e93\u4ed1\u6469\u64e6\u548c\u63a5\u89e6\u534a\u5f84\uff0c\u5e76\u63d0\u51fa\u4e86\u5904\u7406\u5feb\u901f\u6ed1\u79fb-\u7c98\u9644\u52a8\u529b\u5b66\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u63a5\u89e6\u7279\u6027\uff0c\u5e76\u5904\u7406\u6ed1\u79fb-\u7c98\u9644\u52a8\u529b\u5b66\u95ee\u9898\u3002"}}
{"id": "2507.06464", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06464", "abs": "https://arxiv.org/abs/2507.06464", "authors": ["Hanyang Peng", "Shuang Qin", "Yue Yu", "Fangqing Jiang", "Hui Wang", "Wen Gao"], "title": "SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam", "comment": "20pages, 11pages", "summary": "Adam has proven remarkable successful in training deep neural networks, but\nthe mechanisms underlying its empirical successes and limitations remain\nunderexplored. In this study, we demonstrate that the effectiveness of Adam\nstems largely from its similarity to SignSGD in robustly handling large\ngradient fluctuations, yet it is also vulnerable to destabilizing loss spikes\ndue to its uncontrolled update scaling. To enhance the advantage of Adam and\nmitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with\nthree key innovations. \\emph{First}, S3 generalizes the sign-like update by\nemploying a flexible $p$-th order momentum ($p \\geq 1$) in the denominator,\ndeparting from the conventional second-order momentum (variance)\npreconditioning. This design enables enhanced performance while achieving\nstable training even with aggressive learning rates. \\emph{Second}, S3\nminimizes the occurrences of loss spikes through unified exponential moving\naverage coefficients for numerator and denominator momenta, which inherently\nbound updates to $[-1, 1]$ and simplify hyperparameter tuning. \\emph{Third}, S3\nincorporates an equivalent Nesterov's accelerated gradient(NAG) module,\naccelerating convergence without memory overhead. Theoretically, we prove that\nS3 achieves the optimal convergence rate of\n$O\\left(\\frac{1}{T^{\\sfrac{1}{4}}}\\right)$ for general nonconvex stochastic\noptimization under weak assumptions. Extensive experiments across a range of\nvision and language tasks show that \\textsf{\\small S3} not only converges more\nrapidly and improves performance but also rarely experiences loss spikes, even\nwith a \\textbf{$\\bm{10 \\times}$} larger learning rate. In fact, S3 delivers\nperformance comparable to or better than AdamW with \\textbf{$2 \\times$} the\ntraining steps, establishing its efficacy in both efficiency and final task\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSignSoftSGD\uff08S3\uff09\u7684\u65b0\u578b\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u6539\u8fdbAdam\u7684\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u5176\u5728\u5927\u68af\u5ea6\u6ce2\u52a8\u4e0b\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "Adam\u5728\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6210\u529f\u673a\u5236\u548c\u5c40\u9650\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5206\u6790Adam\u7684\u673a\u5236\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u4ee5\u589e\u5f3a\u5176\u4f18\u52bf\u5e76\u51cf\u5c11\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faSignSoftSGD\uff08S3\uff09\u4f18\u5316\u5668\uff0c\u91c7\u7528\u4e09\u65b9\u9762\u521b\u65b0\uff1a1\uff09\u4f7f\u7528\u7075\u6d3b\u7684p\u9636\u52a8\u91cf\uff1b2\uff09\u7edf\u4e00\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7cfb\u6570\u4ee5\u51cf\u5c11\u635f\u5931\u5cf0\u503c\uff1b3\uff09\u96c6\u6210Nesterov\u52a0\u901f\u68af\u5ea6\u6a21\u5757\u3002", "result": "S3\u5728\u975e\u51f8\u968f\u673a\u4f18\u5316\u4e2d\u8fbe\u5230\u6700\u4f18\u6536\u655b\u7387\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6536\u655b\u66f4\u5feb\u3001\u6027\u80fd\u66f4\u4f18\uff0c\u4e14\u5728\u9ad8\u5b66\u4e60\u7387\u4e0b\u4ecd\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "S3\u5728\u6548\u7387\u548c\u6700\u7ec8\u4efb\u52a1\u6027\u80fd\u4e0a\u5747\u4f18\u4e8eAdamW\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u4f18\u5316\u5668\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.06884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06884", "abs": "https://arxiv.org/abs/2507.06884", "authors": ["Dong Bi", "Yongqi Zhao", "Zhengguo Gu", "Tomislav Mihalj", "Jia Hu", "Arno Eichberger"], "title": "Toward a Full-Stack Co-Simulation Platform for Testing of Automated Driving Systems", "comment": "IEEE International Conference on Intelligent Transportation Systems\n  (ITSC) 2025", "summary": "Virtual testing has emerged as an effective approach to accelerate the\ndeployment of automated driving systems. Nevertheless, existing simulation\ntoolchains encounter difficulties in integrating rapid, automated scenario\ngeneration with simulation environments supporting advanced automated driving\ncapabilities. To address this limitation, a full-stack toolchain is presented,\nenabling automatic scenario generation from real-world datasets and efficient\nvalidation through a co-simulation platform based on CarMaker, ROS, and Apollo.\nThe simulation results demonstrate the effectiveness of the proposed toolchain.\nA demonstration video showcasing the toolchain is available at the provided\nlink: https://youtu.be/taJw_-CmSiY.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u6808\u5de5\u5177\u94fe\uff0c\u7528\u4e8e\u4ece\u771f\u5b9e\u6570\u636e\u81ea\u52a8\u751f\u6210\u573a\u666f\u5e76\u901a\u8fc7\u57fa\u4e8eCarMaker\u3001ROS\u548cApollo\u7684\u534f\u540c\u4eff\u771f\u5e73\u53f0\u9ad8\u6548\u9a8c\u8bc1\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u4eff\u771f\u5de5\u5177\u94fe\u96be\u4ee5\u6574\u5408\u5feb\u901f\u3001\u81ea\u52a8\u5316\u7684\u573a\u666f\u751f\u6210\u4e0e\u652f\u6301\u9ad8\u7ea7\u81ea\u52a8\u9a7e\u9a76\u80fd\u529b\u7684\u4eff\u771f\u73af\u5883\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5168\u6808\u5de5\u5177\u94fe\uff0c\u7ed3\u5408\u771f\u5b9e\u6570\u636e\u96c6\u81ea\u52a8\u751f\u6210\u573a\u666f\uff0c\u5e76\u901a\u8fc7CarMaker\u3001ROS\u548cApollo\u7684\u534f\u540c\u4eff\u771f\u5e73\u53f0\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u5de5\u5177\u94fe\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u5177\u94fe\u89e3\u51b3\u4e86\u73b0\u6709\u4eff\u771f\u5de5\u5177\u94fe\u7684\u5c40\u9650\u6027\uff0c\u52a0\u901f\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u90e8\u7f72\u3002"}}
{"id": "2507.06466", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06466", "abs": "https://arxiv.org/abs/2507.06466", "authors": ["Aaron Dharna", "Cong Lu", "Jeff Clune"], "title": "Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models", "comment": "67 pages, accepted to RLC 2025", "summary": "Multi-agent interactions have long fueled innovation, from natural\npredator-prey dynamics to the space race. Self-play (SP) algorithms try to\nharness these dynamics by pitting agents against ever-improving opponents,\nthereby creating an implicit curriculum toward learning high-quality solutions.\nHowever, SP often fails to produce diverse solutions and can get stuck in\nlocally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a\nnew direction that leverages the code-generation capabilities and vast\nknowledge of foundation models (FMs) to overcome these challenges by leaping\nacross local optima in policy space. We propose a family of approaches: (1)\n\\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent\npolicies via competitive self-play; (2) \\textbf{Novelty-Search Self-Play\n(NSSP)} builds a diverse population of strategies, ignoring performance; and\n(3) the most promising variant, \\textbf{Quality-Diveristy Self-Play (QDSP)},\ncreates a diverse set of high-quality policies by combining the diversity of\nNSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a\ncontinuous-control pursuer-evader setting, and in Gandalf, a simple AI safety\nsimulation in which an attacker tries to jailbreak an LLM's defenses. In Car\nTag, FMSPs explore a wide variety of reinforcement learning, tree search, and\nheuristic-based methods, to name just a few. In terms of discovered policy\nquality, \\ouralgo and vFMSP surpass strong human-designed strategies. In\nGandalf, FMSPs can successfully automatically red-team an LLM, breaking through\nand jailbreaking six different, progressively stronger levels of defense.\nFurthermore, FMSPs can automatically proceed to patch the discovered\nvulnerabilities. Overall, FMSPs represent a promising new research frontier of\nimproving self-play with foundation models, opening fresh paths toward more\ncreative and open-ended strategy discovery", "AI": {"tldr": "FMSP\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u6539\u8fdb\u81ea\u535a\u5f08\u7b97\u6cd5\uff0c\u514b\u670d\u5c40\u90e8\u6700\u4f18\u95ee\u9898\uff0c\u63d0\u51fa\u4e09\u79cd\u65b9\u6cd5\uff08vFMSP\u3001NSSP\u3001QDSP\uff09\uff0c\u5728Car Tag\u548cGandalf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u81ea\u535a\u5f08\u7b97\u6cd5\uff08SP\uff09\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u4e14\u7f3a\u4e4f\u591a\u6837\u6027\uff0c\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u548c\u5e7f\u6cdb\u77e5\u8bc6\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51fa\u4e09\u79cdFMSP\u65b9\u6cd5\uff1avFMSP\u901a\u8fc7\u7ade\u4e89\u81ea\u535a\u5f08\u4f18\u5316\u7b56\u7565\uff1bNSSP\u4e13\u6ce8\u4e8e\u7b56\u7565\u591a\u6837\u6027\uff1bQDSP\u7ed3\u5408\u591a\u6837\u6027\u548c\u9ad8\u8d28\u91cf\u7b56\u7565\u3002", "result": "\u5728Car Tag\u4e2d\uff0cFMSP\u63a2\u7d22\u4e86\u591a\u79cd\u65b9\u6cd5\u5e76\u8d85\u8d8a\u4eba\u5de5\u7b56\u7565\uff1b\u5728Gandalf\u4e2d\uff0cFMSP\u6210\u529f\u653b\u7834LLM\u9632\u5fa1\u5e76\u81ea\u52a8\u4fee\u590d\u6f0f\u6d1e\u3002", "conclusion": "FMSP\u4e3a\u57fa\u7840\u6a21\u578b\u6539\u8fdb\u81ea\u535a\u5f08\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u4e3a\u7b56\u7565\u53d1\u73b0\u63d0\u4f9b\u4e86\u66f4\u5f00\u653e\u548c\u521b\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2507.06905", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06905", "abs": "https://arxiv.org/abs/2507.06905", "authors": ["Wandong Sun", "Luying Feng", "Baoshi Cao", "Yang Liu", "Yaochu Jin", "Zongwu Xie"], "title": "ULC: A Unified and Fine-Grained Controller for Humanoid Loco-Manipulation", "comment": null, "summary": "Loco-Manipulation for humanoid robots aims to enable robots to integrate\nmobility with upper-body tracking capabilities. Most existing approaches adopt\nhierarchical architectures that decompose control into isolated upper-body\n(manipulation) and lower-body (locomotion) policies. While this decomposition\nreduces training complexity, it inherently limits coordination between\nsubsystems and contradicts the unified whole-body control exhibited by humans.\nWe demonstrate that a single unified policy can achieve a combination of\ntracking accuracy, large workspace, and robustness for humanoid\nloco-manipulation. We propose the Unified Loco-Manipulation Controller (ULC), a\nsingle-policy framework that simultaneously tracks root velocity, root height,\ntorso rotation, and dual-arm joint positions in an end-to-end manner, proving\nthe feasibility of unified control without sacrificing performance. We achieve\nthis unified control through key technologies: sequence skill acquisition for\nprogressive learning complexity, residual action modeling for fine-grained\ncontrol adjustments, command polynomial interpolation for smooth motion\ntransitions, random delay release for robustness to deploy variations, load\nrandomization for generalization to external disturbances, and\ncenter-of-gravity tracking for providing explicit policy gradients to maintain\nstability. We validate our method on the Unitree G1 humanoid robot with 3-DOF\n(degrees-of-freedom) waist. Compared with strong baselines, ULC shows better\ntracking performance to disentangled methods and demonstrating larger workspace\ncoverage. The unified dual-arm tracking enables precise manipulation under\nexternal loads while maintaining coordinated whole-body control for complex\nloco-manipulation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u4e0e\u64cd\u4f5c\u63a7\u5236\u6846\u67b6\uff08ULC\uff09\uff0c\u901a\u8fc7\u5355\u4e00\u7b56\u7565\u5b9e\u73b0\u5168\u8eab\u534f\u8c03\u63a7\u5236\uff0c\u4f18\u4e8e\u4f20\u7edf\u5206\u5c42\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5206\u5c42\u63a7\u5236\u65b9\u6cd5\u9650\u5236\u4e86\u5b50\u7cfb\u7edf\u95f4\u7684\u534f\u8c03\uff0c\u65e0\u6cd5\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5168\u8eab\u7edf\u4e00\u63a7\u5236\u3002", "method": "\u91c7\u7528\u5e8f\u5217\u6280\u80fd\u83b7\u53d6\u3001\u6b8b\u5dee\u52a8\u4f5c\u5efa\u6a21\u3001\u547d\u4ee4\u591a\u9879\u5f0f\u63d2\u503c\u7b49\u6280\u672f\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u7edf\u4e00\u63a7\u5236\u3002", "result": "ULC\u5728\u8ddf\u8e2a\u7cbe\u5ea6\u3001\u5de5\u4f5c\u7a7a\u95f4\u8303\u56f4\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u652f\u6301\u590d\u6742\u4efb\u52a1\u4e0b\u7684\u7cbe\u786e\u64cd\u4f5c\u3002", "conclusion": "\u7edf\u4e00\u63a7\u5236\u6846\u67b6\u53ef\u884c\u4e14\u9ad8\u6548\uff0c\u4e3a\u590d\u6742\u4eba\u5f62\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.06469", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.06469", "abs": "https://arxiv.org/abs/2507.06469", "authors": ["Yudan Song", "Yuecen Wei", "Yuhang Lu", "Qingyun Sun", "Minglai Shao", "Li-e Wang", "Chunming Hu", "Xianxian Li", "Xingcheng Fu"], "title": "Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning", "comment": null, "summary": "Graph representation learning has become a mainstream method for fraud\ndetection due to its strong expressive power, which focuses on enhancing node\nrepresentations through improved neighborhood knowledge capture. However, the\nfocus on local interactions leads to imbalanced transmission of global\ntopological information and increased risk of node-specific information being\noverwhelmed during aggregation due to the imbalance between fraud and benign\nnodes. In this paper, we first summarize the impact of topology and class\nimbalance on downstream tasks in GNN-based fraud detection, as the problem of\nimbalanced supervisory messages is caused by fraudsters' topological behavior\nobfuscation and identity feature concealment. Based on statistical validation,\nwe propose a novel dual-view graph representation learning method to mitigate\nMessage imbalance in Fraud Detection(MimbFD). Specifically, we design a\ntopological message reachability module for high-quality node representation\nlearning to penetrate fraudsters' camouflage and alleviate insufficient\npropagation. Then, we introduce a local confounding debiasing module to adjust\nnode representations, enhancing the stable association between node\nrepresentations and labels to balance the influence of different classes.\nFinally, we conducted experiments on three public fraud datasets, and the\nresults demonstrate that MimbFD exhibits outstanding performance in fraud\ndetection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u89c6\u56fe\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff08MimbFD\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u56e0\u62d3\u6251\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684\u4fe1\u606f\u4f20\u64ad\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u56e0\u5c40\u90e8\u4ea4\u4e92\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5bfc\u81f4\u5168\u5c40\u62d3\u6251\u4fe1\u606f\u4f20\u64ad\u4e0d\u5747\uff0c\u8282\u70b9\u4fe1\u606f\u6613\u88ab\u6df9\u6ca1\u3002", "method": "\u8bbe\u8ba1\u4e86\u62d3\u6251\u6d88\u606f\u53ef\u8fbe\u6027\u6a21\u5757\u548c\u5c40\u90e8\u6df7\u6dc6\u53bb\u504f\u6a21\u5757\uff0c\u5206\u522b\u7528\u4e8e\u7a7f\u900f\u6b3a\u8bc8\u8005\u4f2a\u88c5\u548c\u5e73\u8861\u7c7b\u522b\u5f71\u54cd\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6b3a\u8bc8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMimbFD\u5728\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MimbFD\u901a\u8fc7\u53cc\u89c6\u56fe\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u4fe1\u606f\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6b3a\u8bc8\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.06960", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06960", "abs": "https://arxiv.org/abs/2507.06960", "authors": ["Samuel Matloob", "Ayan Dutta", "O. Patrick Kreidl", "Swapnonel Roy", "Ladislau B\u00f6l\u00f6ni"], "title": "Bounomodes: the grazing ox algorithm for exploration of clustered anomalies", "comment": null, "summary": "A common class of algorithms for informative path planning (IPP) follows\nboustrophedon (\"as the ox turns\") patterns, which aim to achieve uniform area\ncoverage. However, IPP is often applied in scenarios where anomalies, such as\nplant diseases, pollution, or hurricane damage, appear in clusters. In such\ncases, prioritizing the exploration of anomalous regions over uniform coverage\nis beneficial. This work introduces a class of algorithms referred to as\nbounom\\=odes (\"as the ox grazes\"), which alternates between uniform\nboustrophedon sampling and targeted exploration of detected anomaly clusters.\nWhile uniform sampling can be designed using geometric principles, close\nexploration of clusters depends on the spatial distribution of anomalies and\nmust be learned. In our implementation, the close exploration behavior is\nlearned using deep reinforcement learning algorithms. Experimental evaluations\ndemonstrate that the proposed approach outperforms several established\nbaselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cbounom=odes\u201d\u7684\u7b97\u6cd5\uff0c\u7ed3\u5408\u5747\u5300\u91c7\u6837\u548c\u5f02\u5e38\u533a\u57df\u63a2\u7d22\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfIPP\u7b97\u6cd5\u91c7\u7528\u5747\u5300\u8986\u76d6\u7b56\u7565\uff0c\u4f46\u5728\u5f02\u5e38\u96c6\u7fa4\u573a\u666f\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u4f18\u5148\u63a2\u7d22\u5f02\u5e38\u533a\u57df\u3002", "method": "\u7ed3\u5408\u5747\u5300\u91c7\u6837\uff08boustrophedon\uff09\u548c\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5f02\u5e38\u533a\u57df\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u7b97\u6cd5\u3002", "conclusion": "bounom=odes\u7b97\u6cd5\u5728\u5f02\u5e38\u96c6\u7fa4\u573a\u666f\u4e0b\u66f4\u9ad8\u6548\u3002"}}
{"id": "2507.06482", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06482", "abs": "https://arxiv.org/abs/2507.06482", "authors": ["Huan Wang", "Haoran Li", "Huaming Chen", "Jun Yan", "Jiahua Shi", "Jun Shen"], "title": "FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning", "comment": "19 Pages, ICCV 2025", "summary": "Federated learning aims at training models collaboratively across\nparticipants while protecting privacy. However, one major challenge for this\nparadigm is the data heterogeneity issue, where biased data preferences across\nmultiple clients, harming the model's convergence and performance. In this\npaper, we first introduce powerful diffusion models into the federated learning\nparadigm and show that diffusion representations are effective steers during\nfederated training. To explore the possibility of using diffusion\nrepresentations in handling data heterogeneity, we propose a novel\ndiffusion-inspired Federated paradigm with Diffusion Representation\nCollaboration, termed FedDifRC, leveraging meaningful guidance of diffusion\nmodels to mitigate data heterogeneity. The key idea is to construct text-driven\ndiffusion contrasting and noise-driven diffusion regularization, aiming to\nprovide abundant class-related semantic information and consistent convergence\nsignals. On the one hand, we exploit the conditional feedback from the\ndiffusion model for different text prompts to build a text-driven contrastive\nlearning strategy. On the other hand, we introduce a noise-driven consistency\nregularization to align local instances with diffusion denoising\nrepresentations, constraining the optimization region in the feature space. In\naddition, FedDifRC can be extended to a self-supervised scheme without relying\non any labeled data. We also provide a theoretical analysis for FedDifRC to\nensure convergence under non-convex objectives. The experiments on different\nscenarios validate the effectiveness of FedDifRC and the efficiency of crucial\ncomponents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedDifRC\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6269\u6563\u6a21\u578b\u6765\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5229\u7528\u6587\u672c\u9a71\u52a8\u7684\u5bf9\u6bd4\u5b66\u4e60\u548c\u566a\u58f0\u9a71\u52a8\u7684\u4e00\u81f4\u6027\u6b63\u5219\u5316\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\u5bfc\u81f4\u6a21\u578b\u6536\u655b\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faFedDifRC\u65b9\u6cd5\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u9a71\u52a8\u5bf9\u6bd4\u5b66\u4e60\u548c\u566a\u58f0\u9a71\u52a8\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u4ee5\u63d0\u4f9b\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\u548c\u4e00\u81f4\u7684\u6536\u655b\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FedDifRC\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5173\u952e\u7ec4\u4ef6\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "FedDifRC\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u6307\u5bfc\u6210\u529f\u7f13\u89e3\u4e86\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.06502", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06502", "abs": "https://arxiv.org/abs/2507.06502", "authors": ["Yiwen Liu", "Chenyu Zhang", "Junjie Song", "Siqi Chen", "Sun Yin", "Zihan Wang", "Lingming Zeng", "Yuji Cao", "Junming Jiao"], "title": "MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models", "comment": null, "summary": "As a prominent data modality task, time series forecasting plays a pivotal\nrole in diverse applications. With the remarkable advancements in Large\nLanguage Models (LLMs), the adoption of LLMs as the foundational architecture\nfor time series modeling has gained significant attention. Although existing\nmodels achieve some success, they rarely both model time and frequency\ncharacteristics in a pretraining-finetuning paradigm leading to suboptimal\nperformance in predictions of complex time series, which requires both modeling\nperiodicity and prior pattern knowledge of signals. We propose MoFE-Time, an\ninnovative time series forecasting model that integrates time and frequency\ndomain features within a Mixture of Experts (MoE) network. Moreover, we use the\npretraining-finetuning paradigm as our training framework to effectively\ntransfer prior pattern knowledge across pretraining and finetuning datasets\nwith different periodicity distributions. Our method introduces both frequency\nand time cells as experts after attention modules and leverages the MoE routing\nmechanism to construct multidimensional sparse representations of input\nsignals. In experiments on six public benchmarks, MoFE-Time has achieved new\nstate-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared\nto the representative methods Time-MoE. Beyond the existing evaluation\nbenchmarks, we have developed a proprietary dataset, NEV-sales, derived from\nreal-world business scenarios. Our method achieves outstanding results on this\ndataset, underscoring the effectiveness of the MoFE-Time model in practical\ncommercial applications.", "AI": {"tldr": "MoFE-Time\u662f\u4e00\u79cd\u521b\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u548c\u9891\u7387\u57df\u7279\u5f81\uff0c\u5728MoE\u7f51\u7edc\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u9884\u8bad\u7ec3-\u5fae\u8c03\u8303\u5f0f\u4e2d\u672a\u80fd\u540c\u65f6\u5efa\u6a21\u65f6\u95f4\u548c\u9891\u7387\u7279\u5f81\uff0c\u5bfc\u81f4\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faMoFE-Time\u6a21\u578b\uff0c\u96c6\u6210\u65f6\u95f4\u548c\u9891\u7387\u57df\u7279\u5f81\uff0c\u5229\u7528MoE\u8def\u7531\u673a\u5236\u6784\u5efa\u591a\u7ef4\u7a00\u758f\u8868\u793a\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u65b0\u6027\u80fd\uff0cMSE\u548cMAE\u5206\u522b\u964d\u4f4e6.95%\u548c6.02%\u3002", "conclusion": "MoFE-Time\u5728\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.06516", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.06516", "abs": "https://arxiv.org/abs/2507.06516", "authors": ["Yunrui Zhang", "Gustavo Batista", "Salil S. Kanhere"], "title": "Instance-Wise Monotonic Calibration by Constrained Transformation", "comment": "Accepted to Conference on Uncertainty in Artificial Intelligence\n  (UAI)", "summary": "Deep neural networks often produce miscalibrated probability estimates,\nleading to overconfident predictions. A common approach for calibration is\nfitting a post-hoc calibration map on unseen validation data that transforms\npredicted probabilities. A key desirable property of the calibration map is\ninstance-wise monotonicity (i.e., preserving the ranking of probability\noutputs). However, most existing post-hoc calibration methods do not guarantee\nmonotonicity. Previous monotonic approaches either use an under-parameterized\ncalibration map with limited expressive ability or rely on black-box neural\nnetworks, which lack interpretability and robustness. In this paper, we propose\na family of novel monotonic post-hoc calibration methods, which employs a\nconstrained calibration map parameterized linearly with respect to the number\nof classes. Our proposed approach ensures expressiveness, robustness, and\ninterpretability while preserving the relative ordering of the probability\noutput by formulating the proposed calibration map as a constrained\noptimization problem. Our proposed methods achieve state-of-the-art performance\nacross datasets with different deep neural network models, outperforming\nexisting calibration methods while being data and computation-efficient. Our\ncode is available at\nhttps://github.com/YunruiZhang/Calibration-by-Constrained-Transformation", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u8c03\u540e\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u53c2\u6570\u5316\u7684\u7ea6\u675f\u6821\u51c6\u6620\u5c04\uff0c\u786e\u4fdd\u8868\u8fbe\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6982\u7387\u8f93\u51fa\u7684\u76f8\u5bf9\u987a\u5e8f\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5e38\u4ea7\u751f\u6821\u51c6\u9519\u8bef\u7684\u6982\u7387\u4f30\u8ba1\uff0c\u5bfc\u81f4\u9884\u6d4b\u8fc7\u4e8e\u81ea\u4fe1\u3002\u73b0\u6709\u540e\u6821\u51c6\u65b9\u6cd5\u5927\u591a\u65e0\u6cd5\u4fdd\u8bc1\u5355\u8c03\u6027\uff0c\u6216\u727a\u7272\u8868\u8fbe\u80fd\u529b\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ebf\u6027\u53c2\u6570\u5316\u7684\u7ea6\u675f\u6821\u51c6\u6620\u5c04\uff0c\u5c06\u5176\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u786e\u4fdd\u5355\u8c03\u6027\u3001\u8868\u8fbe\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\uff0c\u4e14\u6570\u636e\u4e0e\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6982\u7387\u8f93\u51fa\u987a\u5e8f\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u540e\u6821\u51c6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.06525", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.06525", "abs": "https://arxiv.org/abs/2507.06525", "authors": ["Huiqi Zhang", "Fang Xie"], "title": "AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks", "comment": null, "summary": "Differential privacy has been proven effective for stochastic gradient\ndescent; however, existing methods often suffer from performance degradation in\nhigh-dimensional settings, as the scale of injected noise increases with\ndimensionality. To tackle this challenge, we propose AdaDPIGU--a new\ndifferentially private SGD framework with importance-based gradient updates\ntailored for deep neural networks. In the pretraining stage, we apply a\ndifferentially private Gaussian mechanism to estimate the importance of each\nparameter while preserving privacy. During the gradient update phase, we prune\nlow-importance coordinates and introduce a coordinate-wise adaptive clipping\nmechanism, enabling sparse and noise-efficient gradient updates. Theoretically,\nwe prove that AdaDPIGU satisfies $(\\varepsilon, \\delta)$-differential privacy\nand retains convergence guarantees. Extensive experiments on standard\nbenchmarks validate the effectiveness of AdaDPIGU. All results are reported\nunder a fixed retention ratio of 60%. On MNIST, our method achieves a test\naccuracy of 99.12% under a privacy budget of $\\epsilon = 8$, nearly matching\nthe non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at\n$\\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating\nthat adaptive sparsification can enhance both privacy and utility.", "AI": {"tldr": "AdaDPIGU\u662f\u4e00\u79cd\u9488\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u578b\u5dee\u5206\u9690\u79c1SGD\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u68af\u5ea6\u66f4\u65b0\u548c\u9ad8\u7ef4\u7a00\u758f\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5dee\u5206\u9690\u79c1\u65b9\u6cd5\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u566a\u58f0\u968f\u7ef4\u5ea6\u589e\u52a0\u800c\u589e\u5927\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u3002", "method": "\u63d0\u51faAdaDPIGU\u6846\u67b6\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u53c2\u6570\u91cd\u8981\u6027\u4f30\u8ba1\u548c\u68af\u5ea6\u66f4\u65b0\u9636\u6bb5\u7684\u7a00\u758f\u5316\u4e0e\u81ea\u9002\u5e94\u88c1\u526a\u673a\u5236\u3002", "result": "\u5728MNIST\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\uff0cAdaDPIGU\u5728\u9690\u79c1\u9884\u7b97\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u975e\u9690\u79c1\u57fa\u7ebf\u3002", "conclusion": "AdaDPIGU\u901a\u8fc7\u81ea\u9002\u5e94\u7a00\u758f\u5316\u540c\u65f6\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u9ad8\u7ef4\u6570\u636e\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06529", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06529", "abs": "https://arxiv.org/abs/2507.06529", "authors": ["Fengxue Zhang", "Yuxin Chen"], "title": "Direct Regret Optimization in Bayesian Optimization", "comment": null, "summary": "Bayesian optimization (BO) is a powerful paradigm for optimizing expensive\nblack-box functions. Traditional BO methods typically rely on separate\nhand-crafted acquisition functions and surrogate models for the underlying\nfunction, and often operate in a myopic manner. In this paper, we propose a\nnovel direct regret optimization approach that jointly learns the optimal model\nand non-myopic acquisition by distilling from a set of candidate models and\nacquisitions, and explicitly targets minimizing the multi-step regret. Our\nframework leverages an ensemble of Gaussian Processes (GPs) with varying\nhyperparameters to generate simulated BO trajectories, each guided by an\nacquisition function chosen from a pool of conventional choices, until a\nBayesian early stop criterion is met. These simulated trajectories, capturing\nmulti-step exploration strategies, are used to train an end-to-end decision\ntransformer that directly learns to select next query points aimed at improving\nthe ultimate objective. We further adopt a dense training--sparse learning\nparadigm: The decision transformer is trained offline with abundant simulated\ndata sampled from ensemble GPs and acquisitions, while a limited number of real\nevaluations refine the GPs online. Experimental results on synthetic and\nreal-world benchmarks suggest that our method consistently outperforms BO\nbaselines, achieving lower simple regret and demonstrating more robust\nexploration in high-dimensional or noisy settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u6700\u4f18\u6a21\u578b\u548c\u975e\u8fd1\u89c6\u91c7\u96c6\u7b56\u7565\uff0c\u76f4\u63a5\u4f18\u5316\u591a\u6b65\u9057\u61be\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u91c7\u96c6\u51fd\u6570\u548c\u4ee3\u7406\u6a21\u578b\uff0c\u4e14\u901a\u5e38\u662f\u8fd1\u89c6\u7684\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\uff08GPs\uff09\u96c6\u5408\u751f\u6210\u6a21\u62df\u8f68\u8ff9\uff0c\u8bad\u7ec3\u7aef\u5230\u7aef\u51b3\u7b56\u53d8\u6362\u5668\uff0c\u91c7\u7528\u79bb\u7ebf\u5bc6\u96c6\u8bad\u7ec3\u548c\u5728\u7ebf\u7a00\u758f\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u7b80\u5355\u9057\u61be\u548c\u66f4\u9c81\u68d2\u7684\u63a2\u7d22\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u548c\u975e\u8fd1\u89c6\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06542", "categories": ["cs.LG", "cs.DC", "cs.MA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.06542", "abs": "https://arxiv.org/abs/2507.06542", "authors": ["Tongtian Zhu", "Tianyu Zhang", "Mingze Wang", "Zhanpeng Zhou", "Can Wang"], "title": "A Single Merging Suffices: Recovering Server-based Learning Performance in Decentralized Learning", "comment": "We discover and theoretically explain why and when a single global\n  parameter merging in decentralized learning can recover the performance of\n  server-based learning, even in highly heterogeneous and\n  communication-constrained environments", "summary": "Decentralized learning provides a scalable alternative to traditional\nparameter-server-based training, yet its performance is often hindered by\nlimited peer-to-peer communication. In this paper, we study how communication\nshould be scheduled over time, including determining when and how frequently\ndevices synchronize. Our empirical results show that concentrating\ncommunication budgets in the later stages of decentralized training markedly\nimproves global generalization. Surprisingly, we uncover that fully connected\ncommunication at the final step, implemented by a single global merging, is\nsufficient to match the performance of server-based training. We further show\nthat low communication in decentralized learning preserves the\n\\textit{mergeability} of local models throughout training. Our theoretical\ncontributions, which explains these phenomena, are first to establish that the\nglobally merged model of decentralized SGD can converge faster than centralized\nmini-batch SGD. Technically, we novelly reinterpret part of the discrepancy\namong local models, which were previously considered as detrimental noise, as\nconstructive components that accelerate convergence. This work challenges the\ncommon belief that decentralized learning generalizes poorly under data\nheterogeneity and limited communication, while offering new insights into model\nmerging and neural network loss landscapes.", "AI": {"tldr": "\u7814\u7a76\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4e2d\u901a\u4fe1\u8c03\u5ea6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u540e\u671f\u96c6\u4e2d\u901a\u4fe1\u548c\u6700\u7ec8\u5168\u5c40\u5408\u5e76\u80fd\u663e\u8457\u63d0\u5347\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u6311\u6218\u4e86\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u5728\u6570\u636e\u5f02\u6784\u548c\u6709\u9650\u901a\u4fe1\u4e0b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u5e38\u89c1\u89c2\u70b9\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u56e0\u901a\u4fe1\u53d7\u9650\u6027\u80fd\u4e0d\u4f73\uff0c\u7814\u7a76\u5982\u4f55\u4f18\u5316\u901a\u4fe1\u8c03\u5ea6\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u901a\u4fe1\u8c03\u5ea6\u7b56\u7565\uff0c\u5305\u62ec\u901a\u4fe1\u65f6\u673a\u548c\u9891\u7387\uff0c\u4ee5\u53ca\u6700\u7ec8\u5168\u5c40\u5408\u5e76\u7684\u6548\u679c\u3002", "result": "\u540e\u671f\u96c6\u4e2d\u901a\u4fe1\u548c\u6700\u7ec8\u5168\u5c40\u5408\u5e76\u80fd\u5339\u914d\u670d\u52a1\u5668\u8bad\u7ec3\u6027\u80fd\uff0c\u4f4e\u901a\u4fe1\u4e0b\u672c\u5730\u6a21\u578b\u4ecd\u53ef\u5408\u5e76\u3002\u7406\u8bba\u8bc1\u660e\u53bb\u4e2d\u5fc3\u5316SGD\u7684\u5168\u5c40\u5408\u5e76\u6a21\u578b\u6536\u655b\u901f\u5ea6\u53ef\u80fd\u5feb\u4e8e\u96c6\u4e2d\u5f0fSGD\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u5728\u4f18\u5316\u901a\u4fe1\u8c03\u5ea6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u89c2\u70b9\uff0c\u4e3a\u6a21\u578b\u5408\u5e76\u548c\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u666f\u89c2\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2507.06558", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06558", "abs": "https://arxiv.org/abs/2507.06558", "authors": ["Zicheng Zhang", "Haoran Li", "Yifeng Zhang", "Guoqiang Gong", "Jiaxing Wang", "Pengzhang Liu", "Qixia Jiang", "Junxing Hu"], "title": "The Primacy of Magnitude in Low-Rank Adaptation", "comment": null, "summary": "Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning\nlarge models. While recent spectral initialization methods improve convergence\nand performance over the naive \"Noise & Zeros\" scheme, their extra\ncomputational and storage overhead undermines efficiency. In this paper, we\nestablish update magnitude as the fundamental driver of LoRA performance and\npropose LoRAM, a magnitude-driven \"Basis & Basis\" initialization scheme that\nmatches spectral methods without their inefficiencies. Our key contributions\nare threefold: (i) Magnitude of weight updates determines convergence. We prove\nlow-rank structures intrinsically bound update magnitudes, unifying\nhyperparameter tuning in learning rate, scaling factor, and initialization as\nmechanisms to optimize magnitude regulation. (ii) Spectral initialization\nsucceeds via magnitude amplification. We demystify that the presumed\nknowledge-driven benefit of the spectral component essentially arises from the\nboost in the weight update magnitude. (iii) A novel and compact initialization\nstrategy, LoRAM, scales deterministic orthogonal bases using pretrained weight\nmagnitudes to simulate spectral gains. Extensive experiments show that LoRAM\nserves as a strong baseline, retaining the full efficiency of LoRA while\nmatching or outperforming spectral initialization across benchmarks.", "AI": {"tldr": "LoRAM\u662f\u4e00\u79cd\u57fa\u4e8e\u66f4\u65b0\u5e45\u5ea6\u7684\u521d\u59cb\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u6a21\u62df\u9891\u8c31\u589e\u76ca\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301LoRA\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u9891\u8c31\u521d\u59cb\u5316\u65b9\u6cd5\u867d\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u548c\u5b58\u50a8\u5f00\u9500\u5927\uff0cLoRAM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faLoRAM\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u5e45\u5ea6\u7f29\u653e\u786e\u5b9a\u6027\u6b63\u4ea4\u57fa\uff0c\u6a21\u62df\u9891\u8c31\u589e\u76ca\u3002", "result": "LoRAM\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\uff0c\u6027\u80fd\u5339\u914d\u6216\u4f18\u4e8e\u9891\u8c31\u521d\u59cb\u5316\u3002", "conclusion": "\u66f4\u65b0\u5e45\u5ea6\u662fLoRA\u6027\u80fd\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0cLoRAM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u521d\u59cb\u5316\u65b9\u6848\u3002"}}
{"id": "2507.06567", "categories": ["cs.LG", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.06567", "abs": "https://arxiv.org/abs/2507.06567", "authors": ["Qian Chen", "Xianhao Chen", "Kaibin Huang"], "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference", "comment": "14 pages, 10 figures", "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u8fb9\u7f18\u7f51\u7edc\u4e2d\u4e13\u5bb6\u7f13\u5b58\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08MoE\uff09\u7684\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3MoE\u6a21\u578b\u4e2d\u4e13\u5bb6\u7f51\u7edc\u6570\u91cf\u5e9e\u5927\u5bfc\u81f4\u7684\u5b58\u50a8\u8d1f\u62c5\u548c\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8d2a\u5a6a\u7b97\u6cd5\u548c\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\u4f18\u5316\u4e13\u5bb6\u7f13\u5b58\uff0c\u5e76\u8bbe\u8ba1\u52a0\u901f\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8fb9\u7f18\u7f51\u7edc\u4e2d\u6709\u6548\u4f18\u5316\u4e86MoE\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2507.06573", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06573", "abs": "https://arxiv.org/abs/2507.06573", "authors": ["Xinjie Chen", "Minpeng Liao", "Guoxin Chen", "Chengxi Li", "Biao Fu", "Kai Fan", "Xinggao Liu"], "title": "From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization", "comment": "Work in progress", "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently advanced\nthe reasoning capabilities of large language models (LLMs). While prior work\nhas emphasized algorithmic design, data curation, and reward shaping, we\ninvestigate RLVR from a sample-centric perspective and introduce LPPO\n(Learning-Progress and Prefix-guided Optimization), a framework of progressive\noptimization techniques. Our work addresses a critical question: how to best\nleverage a small set of trusted, high-quality demonstrations, rather than\nsimply scaling up data volume. First, motivated by how hints aid human\nproblem-solving, we propose prefix-guided sampling, an online data augmentation\nmethod that incorporates partial solution prefixes from expert demonstrations\nto guide the policy, particularly for challenging instances. Second, inspired\nby how humans focus on important questions aligned with their current\ncapabilities, we introduce learning-progress weighting, a dynamic strategy that\nadjusts each training sample's influence based on model progression. We\nestimate sample-level learning progress via an exponential moving average of\nper-sample pass rates, promoting samples that foster learning and\nde-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks\ndemonstrate that our methods outperform strong baselines, yielding faster\nconvergence and a higher performance ceiling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLPPO\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u524d\u7f00\u5f15\u5bfc\u91c7\u6837\u548c\u5b66\u4e60\u8fdb\u5ea6\u52a0\u6743\u6280\u672f\uff0c\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6837\u672c\u5229\u7528\u6548\u7387\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u9ad8\u6548\u5229\u7528\u5c11\u91cf\u9ad8\u8d28\u91cf\u793a\u8303\u6570\u636e\uff0c\u800c\u975e\u7b80\u5355\u589e\u52a0\u6570\u636e\u91cf\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u524d\u7f00\u5f15\u5bfc\u91c7\u6837\u548c\u5b66\u4e60\u8fdb\u5ea6\u52a0\u6743\u4e24\u79cd\u6280\u672f\uff0c\u5206\u522b\u901a\u8fc7\u4e13\u5bb6\u793a\u8303\u7684\u90e8\u5206\u89e3\u524d\u7f00\u5f15\u5bfc\u7b56\u7565\uff0c\u4ee5\u53ca\u52a8\u6001\u8c03\u6574\u6837\u672c\u6743\u91cd\u4ee5\u4fc3\u8fdb\u5b66\u4e60\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u6027\u80fd\u4e0a\u9650\u3002", "conclusion": "LPPO\u6846\u67b6\u901a\u8fc7\u6837\u672c\u4f18\u5316\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.06582", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06582", "abs": "https://arxiv.org/abs/2507.06582", "authors": ["Peter N. Loxley", "Friedrich T. Sommer"], "title": "Learning controllable dynamics through informative exploration", "comment": null, "summary": "Environments with controllable dynamics are usually understood in terms of\nexplicit models. However, such models are not always available, but may\nsometimes be learned by exploring an environment. In this work, we investigate\nusing an information measure called \"predicted information gain\" to determine\nthe most informative regions of an environment to explore next. Applying\nmethods from reinforcement learning allows good suboptimal exploring policies\nto be found, and leads to reliable estimates of the underlying controllable\ndynamics. This approach is demonstrated by comparing with several myopic\nexploration approaches.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u9884\u6d4b\u4fe1\u606f\u589e\u76ca\u201d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u786e\u5b9a\u73af\u5883\u4e2d\u6700\u5177\u4fe1\u606f\u91cf\u7684\u63a2\u7d22\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u627e\u5230\u9ad8\u6548\u7684\u63a2\u7d22\u7b56\u7565\u3002", "motivation": "\u5728\u7f3a\u4e4f\u660e\u786e\u52a8\u6001\u6a21\u578b\u7684\u73af\u5883\u4e2d\uff0c\u63a2\u7d22\u662f\u7406\u89e3\u53ef\u63a7\u52a8\u6001\u7684\u5173\u952e\u3002", "method": "\u4f7f\u7528\u201c\u9884\u6d4b\u4fe1\u606f\u589e\u76ca\u201d\u4f5c\u4e3a\u4fe1\u606f\u5ea6\u91cf\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5bfb\u627e\u9ad8\u6548\u7684\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u4e0e\u51e0\u79cd\u77ed\u89c6\u63a2\u7d22\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u66f4\u53ef\u9760\u5730\u4f30\u8ba1\u73af\u5883\u7684\u53ef\u63a7\u52a8\u6001\u3002", "conclusion": "\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u7684\u63a2\u7d22\u7b56\u7565\u5728\u7f3a\u4e4f\u660e\u786e\u6a21\u578b\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.06602", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06602", "abs": "https://arxiv.org/abs/2507.06602", "authors": ["Burak Demirel", "Yu Wang", "Cristian Tatino", "Pablo Soldati"], "title": "Generalization in Reinforcement Learning for Radio Access Networks", "comment": null, "summary": "Modern RAN operate in highly dynamic and heterogeneous environments, where\nhand-tuned, rule-based RRM algorithms often underperform. While RL can surpass\nsuch heuristics in constrained settings, the diversity of deployments and\nunpredictable radio conditions introduce major generalization challenges.\nData-driven policies frequently overfit to training conditions, degrading\nperformance in unseen scenarios. To address this, we propose a\ngeneralization-centered RL framework for RAN control that: (i) encodes cell\ntopology and node attributes via attention-based graph representations; (ii)\napplies domain randomization to broaden the training distribution; and (iii)\ndistributes data generation across multiple actors while centralizing training\nin a cloud-compatible architecture aligned with O-RAN principles. Although\ngeneralization increases computational and data-management complexity, our\ndistributed design mitigates this by scaling data collection and training\nacross diverse network conditions. Applied to downlink link adaptation in five\n5G benchmarks, our policy improves average throughput and spectral efficiency\nby ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and\nby >20% under high mobility. It matches specialized RL in full-buffer traffic\nand achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,\nrespectively. In nine-cell deployments, GAT models offer 30% higher throughput\nover MLP baselines. These results, combined with our scalable architecture,\noffer a path toward AI-native 6G RAN using a single, generalizable RL agent.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u4ee3\u65e0\u7ebf\u63a5\u5165\u7f51\uff08RAN\uff09\u4e2d\u7684\u8d44\u6e90\u7ba1\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u57df\u968f\u673a\u5316\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3RAN\u73af\u5883\u52a8\u6001\u4e14\u5f02\u6784\uff0c\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u8d44\u6e90\u7ba1\u7406\u7b97\u6cd5\u6027\u80fd\u4e0d\u8db3\uff0c\u800c\u73b0\u6709RL\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u4e0a\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u56fe\u8868\u793a\u7f51\u7edc\u62d3\u6251\u548c\u8282\u70b9\u5c5e\u6027\uff0c\u7ed3\u5408\u57df\u968f\u673a\u5316\u6269\u5c55\u8bad\u7ec3\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u5f0f\u6570\u636e\u751f\u6210\u548c\u96c6\u4e2d\u8bad\u7ec3\u67b6\u6784\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u3002", "result": "\u57285G\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u7b56\u7565\u5728\u541e\u5410\u91cf\u548c\u9891\u8c31\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u539f\u751f\u76846G RAN\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06613", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06613", "abs": "https://arxiv.org/abs/2507.06613", "authors": ["Anshuk Uppal", "Yuhta Takida", "Chieh-Hsin Lai", "Yuki Mitsufuji"], "title": "Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation", "comment": "24 pages, 8 figures and 7 tables", "summary": "Disentangled and interpretable latent representations in generative models\ntypically come at the cost of generation quality. The $\\beta$-VAE framework\nintroduces a hyperparameter $\\beta$ to balance disentanglement and\nreconstruction quality, where setting $\\beta > 1$ introduces an information\nbottleneck that favors disentanglement over sharp, accurate reconstructions. To\naddress this trade-off, we propose a novel generative modeling framework that\nleverages a range of $\\beta$ values to learn multiple corresponding latent\nrepresentations. First, we obtain a slew of representations by training a\nsingle variational autoencoder (VAE), with a new loss function that controls\nthe information retained in each latent representation such that the higher\n$\\beta$ value prioritize disentanglement over reconstruction fidelity. We then,\nintroduce a non-linear diffusion model that smoothly transitions latent\nrepresentations corresponding to different $\\beta$ values. This model denoises\ntowards less disentangled and more informative representations, ultimately\nleading to (almost) lossless representations, enabling sharp reconstructions.\nFurthermore, our model supports sample generation without input images,\nfunctioning as a standalone generative model. We evaluate our framework in\nterms of both disentanglement and generation quality. Additionally, we observe\nsmooth transitions in the latent spaces with respect to changes in $\\beta$,\nfacilitating consistent manipulation of generated outputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u03b2\u503c\u7684VAE\u548c\u975e\u7ebf\u6027\u6269\u6563\u6a21\u578b\uff0c\u5e73\u8861\u89e3\u8026\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u6a21\u578b\u4e2d\u89e3\u8026\u8868\u793a\u4e0e\u9ad8\u8d28\u91cf\u91cd\u5efa\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5355\u4e00VAE\u8bad\u7ec3\u591a\u4e2a\u03b2\u503c\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u5f15\u5165\u975e\u7ebf\u6027\u6269\u6563\u6a21\u578b\u5e73\u6ed1\u8fc7\u6e21\u4e0d\u540c\u03b2\u503c\u7684\u8868\u793a\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u91cd\u5efa\u548c\u89e3\u8026\u8868\u793a\uff0c\u652f\u6301\u65e0\u8f93\u5165\u56fe\u50cf\u7684\u6837\u672c\u751f\u6210\uff0c\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u89c2\u5bdf\u5230\u5e73\u6ed1\u8fc7\u6e21\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u89e3\u8026\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.06615", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06615", "abs": "https://arxiv.org/abs/2507.06615", "authors": ["Jinmin He", "Kai Li", "Yifan Zang", "Haobo Fu", "Qiang Fu", "Junliang Xing", "Jian Cheng"], "title": "Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance", "comment": "NeurIPS2024", "summary": "Multi-task reinforcement learning endeavors to efficiently leverage shared\ninformation across various tasks, facilitating the simultaneous learning of\nmultiple tasks. Existing approaches primarily focus on parameter sharing with\ncarefully designed network structures or tailored optimization procedures.\nHowever, they overlook a direct and complementary way to exploit cross-task\nsimilarities: the control policies of tasks already proficient in some skills\ncan provide explicit guidance for unmastered tasks to accelerate skills\nacquisition. To this end, we present a novel framework called Cross-Task Policy\nGuidance (CTPG), which trains a guide policy for each task to select the\nbehavior policy interacting with the environment from all tasks' control\npolicies, generating better training trajectories. In addition, we propose two\ngating mechanisms to improve the learning efficiency of CTPG: one gate filters\nout control policies that are not beneficial for guidance, while the other gate\nblocks tasks that do not necessitate guidance. CTPG is a general framework\nadaptable to existing parameter sharing approaches. Empirical evaluations\ndemonstrate that incorporating CTPG with these approaches significantly\nenhances performance in manipulation and locomotion benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCTPG\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u4efb\u52a1\u7b56\u7565\u6307\u5bfc\u52a0\u901f\u6280\u80fd\u83b7\u53d6\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u79cd\u95e8\u63a7\u673a\u5236\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u53c2\u6570\u5171\u4eab\uff0c\u4f46\u5ffd\u7565\u4e86\u5229\u7528\u5df2\u638c\u63e1\u4efb\u52a1\u7684\u7b56\u7565\u76f4\u63a5\u6307\u5bfc\u672a\u638c\u63e1\u4efb\u52a1\u7684\u53ef\u80fd\u6027\u3002", "method": "CTPG\u6846\u67b6\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u8bad\u7ec3\u4e00\u4e2a\u6307\u5bfc\u7b56\u7565\uff0c\u4ece\u6240\u6709\u4efb\u52a1\u7684\u63a7\u5236\u7b56\u7565\u4e2d\u9009\u62e9\u884c\u4e3a\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u95e8\u63a7\u673a\u5236\u4f18\u5316\u5b66\u4e60\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCTPG\u4e0e\u73b0\u6709\u53c2\u6570\u5171\u4eab\u65b9\u6cd5\u7ed3\u5408\u540e\uff0c\u5728\u64cd\u4f5c\u548c\u8fd0\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "CTPG\u662f\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u80fd\u6709\u6548\u5229\u7528\u8de8\u4efb\u52a1\u76f8\u4f3c\u6027\uff0c\u63d0\u5347\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u3002"}}
{"id": "2507.06619", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.06619", "abs": "https://arxiv.org/abs/2507.06619", "authors": ["Xiaobo Huang", "Fang Xie"], "title": "Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000", "comment": null, "summary": "When applying machine learning to medical image classification, data leakage\nis a critical issue. Previous methods, such as adding noise to gradients for\ndifferential privacy, work well on large datasets like MNIST and CIFAR-100, but\nfail on small, imbalanced medical datasets like HAM10000. This is because the\nimbalanced distribution causes gradients from minority classes to be clipped\nand lose crucial information, while majority classes dominate. This leads the\nmodel to fall into suboptimal solutions early. To address this, we propose\nSAD-DPSGD, which uses a linear decaying mechanism for noise and clipping\nthresholds. By allocating more privacy budget and using higher clipping\nthresholds in the initial training phases, the model avoids suboptimal\nsolutions and enhances performance. Experiments show that SAD-DPSGD outperforms\nAuto-DPSGD on HAM10000, improving accuracy by 2.15% under $\\epsilon = 3.0$ ,\n$\\delta = 10^{-3}$.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSAD-DPSGD\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u566a\u58f0\u548c\u88c1\u526a\u9608\u503c\uff0c\u89e3\u51b3\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u5728HAM10000\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eAuto-DPSGD\u3002", "motivation": "\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u6570\u636e\u6cc4\u9732\u95ee\u9898\u4e25\u91cd\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u4e0d\u5e73\u8861\u6570\u636e\u96c6\uff08\u5982HAM10000\uff09\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u5bfc\u81f4\u6a21\u578b\u9677\u5165\u6b21\u4f18\u89e3\u3002", "method": "\u63d0\u51faSAD-DPSGD\uff0c\u91c7\u7528\u7ebf\u6027\u8870\u51cf\u673a\u5236\u52a8\u6001\u8c03\u6574\u566a\u58f0\u548c\u88c1\u526a\u9608\u503c\uff0c\u521d\u671f\u5206\u914d\u66f4\u591a\u9690\u79c1\u9884\u7b97\u548c\u66f4\u9ad8\u88c1\u526a\u9608\u503c\u3002", "result": "\u5728HAM10000\u6570\u636e\u96c6\u4e0a\uff0cSAD-DPSGD\u5728\u03b5=3.0\u3001\u03b4=10^-3\u6761\u4ef6\u4e0b\uff0c\u51c6\u786e\u7387\u6bd4Auto-DPSGD\u63d0\u9ad82.15%\u3002", "conclusion": "SAD-DPSGD\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u5e73\u8861\u533b\u7597\u6570\u636e\u96c6\u4e0a\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.06624", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06624", "abs": "https://arxiv.org/abs/2507.06624", "authors": ["Dazhi Fu", "Jicong Fan"], "title": "UniOD: A Universal Model for Outlier Detection across Diverse Domains", "comment": "20 pages, 4 figures", "summary": "Outlier detection (OD) seeks to distinguish inliers and outliers in\ncompletely unlabeled datasets and plays a vital role in science and\nengineering. Most existing OD methods require troublesome dataset-specific\nhyperparameter tuning and costly model training before they can be deployed to\nidentify outliers. In this work, we propose UniOD, a universal OD framework\nthat leverages labeled datasets to train a single model capable of detecting\noutliers of datasets from diverse domains. Specifically, UniOD converts each\ndataset into multiple graphs, produces consistent node features, and frames\noutlier detection as a node-classification task, and is able to generalize to\nunseen domains. As a result, UniOD avoids effort on model selection and\nhyperparameter tuning, reduces computational cost, and effectively utilizes the\nknowledge from historical datasets, which improves the convenience and accuracy\nin real applications. We evaluate UniOD on 15 benchmark OD datasets against 15\nstate-of-the-art baselines, demonstrating its effectiveness.", "AI": {"tldr": "UniOD\u662f\u4e00\u4e2a\u901a\u7528\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6807\u8bb0\u6570\u636e\u96c6\u8bad\u7ec3\u5355\u4e00\u6a21\u578b\uff0c\u80fd\u591f\u68c0\u6d4b\u6765\u81ea\u4e0d\u540c\u9886\u57df\u7684\u5f02\u5e38\u6570\u636e\uff0c\u907f\u514d\u4e86\u6a21\u578b\u9009\u62e9\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u9ebb\u70e6\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u6570\u636e\u96c6\u8fdb\u884c\u7e41\u7410\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u7684\u4fbf\u5229\u6027\u548c\u6548\u7387\u3002", "method": "UniOD\u5c06\u6bcf\u4e2a\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u591a\u4e2a\u56fe\uff0c\u751f\u6210\u4e00\u81f4\u7684\u8282\u70b9\u7279\u5f81\uff0c\u5e76\u5c06\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u8f6c\u5316\u4e3a\u8282\u70b9\u5206\u7c7b\u95ee\u9898\uff0c\u4ece\u800c\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u9886\u57df\u3002", "result": "\u572815\u4e2a\u57fa\u51c6\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u4e0e15\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cUniOD\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "UniOD\u51cf\u5c11\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u6709\u6548\u5229\u7528\u4e86\u5386\u53f2\u6570\u636e\u96c6\u7684\u77e5\u8bc6\uff0c\u63d0\u9ad8\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4fbf\u5229\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.06628", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06628", "abs": "https://arxiv.org/abs/2507.06628", "authors": ["Jinmin He", "Kai Li", "Yifan Zang", "Haobo Fu", "Qiang Fu", "Junliang Xing", "Jian Cheng"], "title": "Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning", "comment": "ICML2025", "summary": "Offline multi-task reinforcement learning aims to learn a unified policy\ncapable of solving multiple tasks using only pre-collected task-mixed datasets,\nwithout requiring any online interaction with the environment. However, it\nfaces significant challenges in effectively sharing knowledge across tasks.\nInspired by the efficient knowledge abstraction observed in human learning, we\npropose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed\nto extract and utilize reusable skills to enhance knowledge transfer and task\nperformance. Our approach uncovers reusable skills through a goal-oriented\nskill extraction process and leverages vector quantization to construct a\ndiscrete skill library. To mitigate class imbalances between broadly applicable\nand task-specific skills, we introduce a skill enhancement phase to refine the\nextracted skills. Furthermore, we integrate these skills using hierarchical\npolicy learning, enabling the construction of a high-level policy that\ndynamically orchestrates discrete skills to accomplish specific tasks.\nExtensive experiments on diverse robotic manipulation tasks within the\nMetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.", "AI": {"tldr": "GO-Skill\u662f\u4e00\u79cd\u79bb\u7ebf\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u76ee\u6807\u5bfc\u5411\u7684\u6280\u80fd\u63d0\u53d6\u548c\u5206\u5c42\u7b56\u7565\u5b66\u4e60\uff0c\u63d0\u5347\u77e5\u8bc6\u5171\u4eab\u548c\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u8de8\u4efb\u52a1\u77e5\u8bc6\u5171\u4eab\u7684\u6311\u6218\uff0c\u53d7\u4eba\u7c7b\u5b66\u4e60\u542f\u53d1\uff0c\u63d0\u51faGO-Skill\u4ee5\u63d0\u53d6\u548c\u5229\u7528\u53ef\u91cd\u7528\u6280\u80fd\u3002", "method": "\u901a\u8fc7\u76ee\u6807\u5bfc\u5411\u6280\u80fd\u63d0\u53d6\u548c\u5411\u91cf\u91cf\u5316\u6784\u5efa\u79bb\u6563\u6280\u80fd\u5e93\uff0c\u5f15\u5165\u6280\u80fd\u589e\u5f3a\u9636\u6bb5\u4f18\u5316\u6280\u80fd\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u7b56\u7565\u5b66\u4e60\u52a8\u6001\u534f\u8c03\u6280\u80fd\u3002", "result": "\u5728MetaWorld\u57fa\u51c6\u6d4b\u8bd5\u7684\u591a\u6837\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86GO-Skill\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "GO-Skill\u901a\u8fc7\u6280\u80fd\u62bd\u8c61\u548c\u5206\u5c42\u7b56\u7565\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06631", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06631", "abs": "https://arxiv.org/abs/2507.06631", "authors": ["Enda D. V. Bigarella"], "title": "Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator", "comment": null, "summary": "This document reports on a method for detecting and preventing overfitting on\ndata regressions, herein applied to mesh-like data structures. The mesh\nstructure allows for the straightforward computation of the Laplace-operator\nsecond-order derivatives in a finite-difference fashion for noiseless data.\nDerivatives of the training data are computed on the original training mesh to\nserve as a true label of the entropy of the training data. Derivatives of the\ntrained data are computed on a staggered mesh to identify oscillations in the\ninterior of the original training mesh cells. The loss of the Laplace-operator\nderivatives is used for hyperparameter optimisation, achieving a reduction of\nunwanted oscillation through the minimisation of the entropy of the trained\nmodel. In this setup, testing does not require the splitting of points from the\ntraining data, and training is thus directly performed on all available\ntraining points. The Laplace operator applied to the trained data on a\nstaggered mesh serves as a surrogate testing metric based on diffusion\nproperties.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f51\u683c\u6570\u636e\u7684\u8fc7\u62df\u5408\u68c0\u6d4b\u4e0e\u9884\u9632\u65b9\u6cd5\uff0c\u5229\u7528\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u4e8c\u9636\u5bfc\u6570\u4f18\u5316\u6a21\u578b\u71b5\u3002", "motivation": "\u89e3\u51b3\u56de\u5f52\u4efb\u52a1\u4e2d\u6570\u636e\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7f51\u683c\u7ed3\u6784\u6570\u636e\u4e2d\uff0c\u901a\u8fc7\u4f18\u5316\u6a21\u578b\u71b5\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u632f\u8361\u3002", "method": "\u5728\u539f\u59cb\u8bad\u7ec3\u7f51\u683c\u4e0a\u8ba1\u7b97\u5bfc\u6570\u4f5c\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u5728\u4ea4\u9519\u7f51\u683c\u4e0a\u8ba1\u7b97\u8bad\u7ec3\u6570\u636e\u7684\u5bfc\u6570\u4ee5\u8bc6\u522b\u632f\u8361\uff0c\u5229\u7528\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5bfc\u6570\u635f\u5931\u8fdb\u884c\u8d85\u53c2\u6570\u4f18\u5316\u3002", "result": "\u5b9e\u73b0\u4e86\u901a\u8fc7\u6700\u5c0f\u5316\u6a21\u578b\u71b5\u51cf\u5c11\u632f\u8361\u7684\u76ee\u6807\uff0c\u65e0\u9700\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u62c6\u5206\u6d4b\u8bd5\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u8fc7\u62df\u5408\uff0c\u5e76\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5728\u4ea4\u9519\u7f51\u683c\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7279\u6027\u7684\u6d4b\u8bd5\u6307\u6807\u3002"}}
{"id": "2507.06650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06650", "abs": "https://arxiv.org/abs/2507.06650", "authors": ["Hui Meng", "Keping Yang", "Xuyu Peng", "Bo Zheng"], "title": "Deep Disentangled Representation Network for Treatment Effect Estimation", "comment": "Under Review", "summary": "Estimating individual-level treatment effect from observational data is a\nfundamental problem in causal inference and has attracted increasing attention\nin the fields of education, healthcare, and public policy.In this work, we\nconcentrate on the study of disentangled representation methods that have shown\npromising outcomes by decomposing observed covariates into instrumental,\nconfounding, and adjustment factors. However, most of the previous work has\nprimarily revolved around generative models or hard decomposition methods for\ncovariates, which often struggle to guarantee the attainment of precisely\ndisentangled factors. In order to effectively model different causal\nrelationships, we propose a novel treatment effect estimation algorithm that\nincorporates a mixture of experts with multi-head attention and a linear\northogonal regularizer to softly decompose the pre-treatment variables, and\nsimultaneously eliminates selection bias via importance sampling re-weighting\ntechniques. We conduct extensive experiments on both public semi-synthetic and\nreal-world production datasets. The experimental results clearly demonstrate\nthat our algorithm outperforms the state-of-the-art methods focused on\nindividual treatment effects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u7b97\u6cd5\uff0c\u7ed3\u5408\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u548c\u7ebf\u6027\u6b63\u4ea4\u6b63\u5219\u5316\u5668\uff0c\u901a\u8fc7\u8f6f\u5206\u89e3\u9884\u5904\u7406\u53d8\u91cf\u5e76\u6d88\u9664\u9009\u62e9\u504f\u5dee\u3002", "motivation": "\u89e3\u51b3\u89c2\u6d4b\u6570\u636e\u4e2d\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u5206\u89e3\u53d8\u91cf\u3002", "method": "\u91c7\u7528\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u548c\u7ebf\u6027\u6b63\u4ea4\u6b63\u5219\u5316\u5668\u8f6f\u5206\u89e3\u53d8\u91cf\uff0c\u7ed3\u5408\u91cd\u8981\u6027\u91c7\u6837\u91cd\u52a0\u6743\u6280\u672f\u6d88\u9664\u9009\u62e9\u504f\u5dee\u3002", "result": "\u5728\u516c\u5f00\u534a\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u5728\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.06652", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06652", "abs": "https://arxiv.org/abs/2507.06652", "authors": ["Arthur Alexander Lim", "Zhen Bin It", "Jovan Bowen Heng", "Tee Hui Teo"], "title": "Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making", "comment": null, "summary": "Fuzzy systems are a way to allow machines, systems and frameworks to deal\nwith uncertainty, which is not possible in binary systems that most computers\nuse. These systems have already been deployed for certain use cases, and fuzzy\nsystems could be further improved as proposed in this paper. Such technologies\nto draw inspiration from include machine learning and federated learning.\nMachine learning is one of the recent breakthroughs of technology and could be\napplied to fuzzy systems to further improve the results it produces. Federated\nlearning is also one of the recent technologies that have huge potential, which\nallows machine learning training to improve by reducing privacy risk, reducing\nburden on networking infrastructure, and reducing latency of the latest model.\nAspects from federated learning could be used to improve federated learning,\nsuch as applying the idea of updating the fuzzy rules that make up a key part\nof fuzzy systems, to further improve it over time. This paper discusses how\nthese improvements would be implemented in fuzzy systems, and how it would\nimprove fuzzy systems. It also discusses certain limitations on the potential\nimprovements. It concludes that these proposed ideas and improvements require\nfurther investigation to see how far the improvements are, but the potential is\nthere to improve fuzzy systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u8054\u90a6\u5b66\u4e60\u6539\u8fdb\u6a21\u7cca\u7cfb\u7edf\uff0c\u4ee5\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5206\u6790\u5176\u6f5c\u5728\u6539\u8fdb\u4e0e\u5c40\u9650\u6027\u3002", "motivation": "\u6a21\u7cca\u7cfb\u7edf\u80fd\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u673a\u5668\u5b66\u4e60\u548c\u8054\u90a6\u5b66\u4e60\u7684\u65b0\u6280\u672f\u53ef\u4e3a\u5176\u63d0\u4f9b\u7075\u611f\u3002", "method": "\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u8054\u90a6\u5b66\u4e60\u7684\u7406\u5ff5\uff0c\u5982\u66f4\u65b0\u6a21\u7cca\u89c4\u5219\uff0c\u4ee5\u9010\u6b65\u4f18\u5316\u6a21\u7cca\u7cfb\u7edf\u3002", "result": "\u6f5c\u5728\u6539\u8fdb\u663e\u8457\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u9a8c\u8bc1\u5176\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "\u6539\u8fdb\u6a21\u7cca\u7cfb\u7edf\u7684\u6f5c\u529b\u5b58\u5728\uff0c\u4f46\u9700\u66f4\u591a\u7814\u7a76\u4ee5\u786e\u5b9a\u5176\u8303\u56f4\u548c\u6548\u679c\u3002"}}
{"id": "2507.06701", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06701", "abs": "https://arxiv.org/abs/2507.06701", "authors": ["Michael Bloesch", "Markus Wulfmeier", "Philemon Brakel", "Todor Davchev", "Martina Zambelli", "Jost Tobias Springenberg", "Abbas Abdolmaleki", "William F Whitney", "Nicolas Heess", "Roland Hafner", "Martin Riedmiller"], "title": "Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement", "comment": null, "summary": "Imitation Learning from Observation (IfO) offers a powerful way to learn\nbehaviors at large-scale: Unlike behavior cloning or offline reinforcement\nlearning, IfO can leverage action-free demonstrations and thus circumvents the\nneed for costly action-labeled demonstrations or reward functions. However,\ncurrent IfO research focuses on idealized scenarios with mostly bimodal-quality\ndata distributions, restricting the meaningfulness of the results. In contrast,\nthis paper investigates more nuanced distributions and introduces a method to\nlearn from such data, moving closer to a paradigm in which imitation learning\ncan be performed iteratively via self-improvement. Our method adapts RL-based\nimitation learning to action-free demonstrations, using a value function to\ntransfer information between expert and non-expert data. Through comprehensive\nevaluation, we delineate the relation between different data distributions and\nthe applicability of algorithms and highlight the limitations of established\nmethods. Our findings provide valuable insights for developing more robust and\npractical IfO techniques on a path to scalable behaviour learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6a21\u4eff\u5b66\u4e60\u89c2\u5bdf\u65b9\u6cd5\uff08IfO\uff09\uff0c\u901a\u8fc7\u5229\u7528\u65e0\u52a8\u4f5c\u6f14\u793a\u6570\u636e\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u52a8\u4f5c\u6807\u8bb0\u6216\u5956\u52b1\u51fd\u6570\u7684\u9700\u6c42\uff0c\u5e76\u7814\u7a76\u4e86\u66f4\u590d\u6742\u7684\u6570\u636e\u5206\u5e03\u3002", "motivation": "\u4f20\u7edfIfO\u7814\u7a76\u5c40\u9650\u4e8e\u7406\u60f3\u5316\u7684\u53cc\u5cf0\u6570\u636e\u5206\u5e03\uff0c\u9650\u5236\u4e86\u7ed3\u679c\u7684\u5b9e\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66f4\u590d\u6742\u7684\u6570\u636e\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u4f7f\u6a21\u4eff\u5b66\u4e60\u80fd\u591f\u901a\u8fc7\u81ea\u6211\u6539\u8fdb\u8fed\u4ee3\u8fdb\u884c\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6a21\u4eff\u5b66\u4e60\u9002\u5e94\u4e8e\u65e0\u52a8\u4f5c\u6f14\u793a\uff0c\u5229\u7528\u4ef7\u503c\u51fd\u6570\u5728\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u6570\u636e\u4e4b\u95f4\u4f20\u9012\u4fe1\u606f\u3002", "result": "\u901a\u8fc7\u5168\u9762\u8bc4\u4f30\uff0c\u8bba\u6587\u63ed\u793a\u4e86\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0e\u7b97\u6cd5\u9002\u7528\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u6307\u51fa\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u5b9e\u7528\u7684IfO\u6280\u672f\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\uff0c\u4e3a\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u884c\u4e3a\u5b66\u4e60\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.06712", "categories": ["cs.LG", "math.DS", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2507.06712", "abs": "https://arxiv.org/abs/2507.06712", "authors": ["Ayoub Farkane", "Mohamed Boutayeb", "Mustapha Oudani", "Mounir Ghogho"], "title": "PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems", "comment": null, "summary": "State estimation for nonlinear dynamical systems is a critical challenge in\ncontrol and engineering applications, particularly when only partial and noisy\nmeasurements are available. This paper introduces a novel Adaptive\nPhysics-Informed Neural Network-based Observer (PINN-Obs) for accurate state\nestimation in nonlinear systems. Unlike traditional model-based observers,\nwhich require explicit system transformations or linearization, the proposed\nframework directly integrates system dynamics and sensor data into a\nphysics-informed learning process. The observer adaptively learns an optimal\ngain matrix, ensuring convergence of the estimated states to the true system\nstates. A rigorous theoretical analysis establishes formal convergence\nguarantees, demonstrating that the proposed approach achieves uniform error\nminimization under mild observability conditions. The effectiveness of PINN-Obs\nis validated through extensive numerical simulations on diverse nonlinear\nsystems, including an induction motor model, a satellite motion system, and\nbenchmark academic examples. Comparative experimental studies against existing\nobserver designs highlight its superior accuracy, robustness, and adaptability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff08PINN-Obs\uff09\uff0c\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u89c2\u6d4b\u5668\u3002", "motivation": "\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\u5728\u90e8\u5206\u548c\u566a\u58f0\u6d4b\u91cf\u4e0b\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u7cfb\u7edf\u52a8\u529b\u5b66\u548c\u4f20\u611f\u5668\u6570\u636e\uff0c\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u8fc7\u7a0b\u81ea\u9002\u5e94\u5b66\u4e60\u6700\u4f18\u589e\u76ca\u77e9\u9635\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u8bef\u5dee\u6700\u5c0f\u5316\uff0c\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u79cd\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "PINN-Obs\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u89c2\u6d4b\u5668\u8bbe\u8ba1\u3002"}}
{"id": "2507.06752", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML", "68T07, 35J05", "I.2.6; G.1.8; G.4"], "pdf": "https://arxiv.org/pdf/2507.06752", "abs": "https://arxiv.org/abs/2507.06752", "authors": ["Heng Wu", "Benzhuo Lu"], "title": "Mathematical artificial data for operator learning", "comment": "22 pages, 5 figures", "summary": "Machine learning has emerged as a transformative tool for solving\ndifferential equations (DEs), yet prevailing methodologies remain constrained\nby dual limitations: data-driven methods demand costly labeled datasets while\nmodel-driven techniques face efficiency-accuracy trade-offs. We present the\nMathematical Artificial Data (MAD) framework, a new paradigm that integrates\nphysical laws with data-driven learning to facilitate large-scale operator\ndiscovery. By exploiting DEs' intrinsic mathematical structure to generate\nphysics-embedded analytical solutions and associated synthetic data, MAD\nfundamentally eliminates dependence on experimental or simulated training data.\nThis enables computationally efficient operator learning across multi-parameter\nsystems while maintaining mathematical rigor. Through numerical demonstrations\nspanning 2D parametric problems where both the boundary values and source term\nare functions, we showcase MAD's generalizability and superior\nefficiency/accuracy across various DE scenarios. This\nphysics-embedded-data-driven framework and its capacity to handle complex\nparameter spaces gives it the potential to become a universal paradigm for\nphysics-informed machine intelligence in scientific computing.", "AI": {"tldr": "MAD\u6846\u67b6\u7ed3\u5408\u7269\u7406\u5b9a\u5f8b\u4e0e\u6570\u636e\u9a71\u52a8\u5b66\u4e60\uff0c\u901a\u8fc7\u751f\u6210\u7269\u7406\u5d4c\u5165\u7684\u89e3\u6790\u89e3\u548c\u5408\u6210\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6807\u8bb0\u6570\u636e\u6216\u6548\u7387-\u7cbe\u5ea6\u6743\u8861\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u7b97\u5b50\u5b66\u4e60\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u89e3\u51b3\u5fae\u5206\u65b9\u7a0b\u65f6\u9762\u4e34\u6570\u636e\u9700\u6c42\u9ad8\u6216\u6548\u7387\u4e0e\u7cbe\u5ea6\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0cMAD\u65e8\u5728\u901a\u8fc7\u7269\u7406\u5d4c\u5165\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "MAD\u5229\u7528\u5fae\u5206\u65b9\u7a0b\u7684\u6570\u5b66\u7ed3\u6784\u751f\u6210\u7269\u7406\u5d4c\u5165\u7684\u89e3\u6790\u89e3\u548c\u5408\u6210\u6570\u636e\uff0c\u65e0\u9700\u5b9e\u9a8c\u6216\u6a21\u62df\u8bad\u7ec3\u6570\u636e\uff0c\u652f\u6301\u591a\u53c2\u6570\u7cfb\u7edf\u7684\u9ad8\u6548\u7b97\u5b50\u5b66\u4e60\u3002", "result": "\u57282D\u53c2\u6570\u5316\u95ee\u9898\u4e2d\uff0cMAD\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\u548c\u5728\u6548\u7387\u4e0e\u7cbe\u5ea6\u4e0a\u7684\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5fae\u5206\u65b9\u7a0b\u573a\u666f\u3002", "conclusion": "MAD\u6846\u67b6\u6709\u671b\u6210\u4e3a\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7269\u7406\u4fe1\u606f\u673a\u5668\u667a\u80fd\u7684\u901a\u7528\u8303\u5f0f\uff0c\u5c24\u5176\u5728\u590d\u6742\u53c2\u6570\u7a7a\u95f4\u5904\u7406\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.06765", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06765", "abs": "https://arxiv.org/abs/2507.06765", "authors": ["Enda D. V. Bigarella"], "title": "Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric", "comment": null, "summary": "This document proposes a parametric activation function (ac.f.) aimed at\nimproving multidimensional nonlinear data regression. It is a established\nknowledge that nonlinear ac.f.'s are required for learning nonlinear datasets.\nThis work shows that smoothness and gradient properties of the ac.f. further\nimpact the performance of large neural networks in terms of overfitting and\nsensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as\nELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and\nLeaky-RELU further impart discontinuity in the trained model. Improved\nperformance is demonstrated with a smooth \"Leaky Exponential Linear Unit\", with\nnon-zero gradient that can be trained. A novel diffusion-loss metric is also\nproposed to gauge the performance of the trained models in terms of\noverfitting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u6fc0\u6d3b\u51fd\u6570\uff08Leaky Exponential Linear Unit\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u591a\u7ef4\u975e\u7ebf\u6027\u6570\u636e\u56de\u5f52\uff0c\u5e76\u901a\u8fc7\u5e73\u6ed1\u6027\u548c\u68af\u5ea6\u7279\u6027\u63d0\u5347\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u3002", "motivation": "\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u5bf9\u5b66\u4e60\u975e\u7ebf\u6027\u6570\u636e\u96c6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u51fd\u6570\u5982ELU\u3001SiLU\u3001RELU\u7b49\u5728\u5e73\u6ed1\u6027\u548c\u68af\u5ea6\u7279\u6027\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5e73\u6ed1\u4e14\u68af\u5ea6\u975e\u96f6\u7684Leaky Exponential Linear Unit\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u6269\u6563\u635f\u5931\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u8fc7\u62df\u5408\u3002", "result": "\u65b0\u6fc0\u6d3b\u51fd\u6570\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u51fd\u6570\uff0c\u51cf\u5c11\u4e86\u8fc7\u62df\u5408\u548c\u5bf9\u6a21\u578b\u53c2\u6570\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u5e73\u6ed1\u4e14\u68af\u5ea6\u975e\u96f6\u7684\u6fc0\u6d3b\u51fd\u6570\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u6269\u6563\u635f\u5931\u6307\u6807\u4e3a\u8bc4\u4f30\u8fc7\u62df\u5408\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.06775", "categories": ["cs.LG", "math.AT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.06775", "abs": "https://arxiv.org/abs/2507.06775", "authors": ["Mario Tuci", "Lennart Bastian", "Benjamin Dupuis", "Nassir Navab", "Tolga Birdal", "Umut \u015eim\u015fekli"], "title": "Mutual Information Free Topological Generalization Bounds via Stability", "comment": "25 pages, 5 figures", "summary": "Providing generalization guarantees for stochastic optimization algorithms is\na major challenge in modern learning theory. Recently, several studies\nhighlighted the impact of the geometry of training trajectories on the\ngeneralization error, both theoretically and empirically. Among these works, a\nseries of topological generalization bounds have been proposed, relating the\ngeneralization error to notions of topological complexity that stem from\ntopological data analysis (TDA). Despite their empirical success, these bounds\nrely on intricate information-theoretic (IT) terms that can be bounded in\nspecific cases but remain intractable for practical algorithms (such as ADAM),\npotentially reducing the relevance of the derived bounds. In this paper, we\nseek to formulate comprehensive and interpretable topological generalization\nbounds free of intractable mutual information terms. To this end, we introduce\na novel learning theoretic framework that departs from the existing strategies\nvia proof techniques rooted in algorithmic stability. By extending an existing\nnotion of \\textit{hypothesis set stability}, to \\textit{trajectory stability},\nwe prove that the generalization error of trajectory-stable algorithms can be\nupper bounded in terms of (i) TDA quantities describing the complexity of the\ntrajectory of the optimizer in the parameter space, and (ii) the trajectory\nstability parameter of the algorithm. Through a series of experimental\nevaluations, we demonstrate that the TDA terms in the bound are of great\nimportance, especially as the number of training samples grows. This ultimately\nforms an explanation of the empirical success of the topological generalization\nbounds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u62d3\u6251\u6cdb\u5316\u754c\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u4fe1\u606f\u8bba\u9879\uff0c\u901a\u8fc7\u8f68\u8ff9\u7a33\u5b9a\u6027\u6846\u67b6\u5c06\u6cdb\u5316\u8bef\u5dee\u4e0e\u62d3\u6251\u6570\u636e\u5206\u6790\u548c\u7b97\u6cd5\u7a33\u5b9a\u6027\u8054\u7cfb\u8d77\u6765\u3002", "motivation": "\u73b0\u6709\u62d3\u6251\u6cdb\u5316\u754c\u4f9d\u8d56\u590d\u6742\u7684\u4fe1\u606f\u8bba\u9879\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5b9e\u9645\u7b97\u6cd5\uff08\u5982ADAM\uff09\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u76f4\u89c2\u4e14\u53ef\u89e3\u91ca\u7684\u6cdb\u5316\u754c\u3002", "method": "\u5f15\u5165\u8f68\u8ff9\u7a33\u5b9a\u6027\u6846\u67b6\uff0c\u6269\u5c55\u5047\u8bbe\u96c6\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u901a\u8fc7\u62d3\u6251\u6570\u636e\u5206\u6790\u548c\u7b97\u6cd5\u7a33\u5b9a\u6027\u53c2\u6570\u4e0a\u754c\u6cdb\u5316\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u62d3\u6251\u6570\u636e\u5206\u6790\u9879\u5bf9\u6cdb\u5316\u8bef\u5dee\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5c24\u5176\u5728\u6837\u672c\u91cf\u589e\u52a0\u65f6\u3002", "conclusion": "\u65b0\u6846\u67b6\u63d0\u4f9b\u4e86\u76f4\u89c2\u4e14\u53ef\u89e3\u91ca\u7684\u62d3\u6251\u6cdb\u5316\u754c\uff0c\u89e3\u91ca\u4e86\u5176\u5b9e\u9645\u6210\u529f\u7684\u539f\u56e0\u3002"}}
{"id": "2507.06780", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.06780", "abs": "https://arxiv.org/abs/2507.06780", "authors": ["George Papadopoulos", "George A. Vouros"], "title": "Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm", "comment": null, "summary": "This article introduces an imitation learning method for learning maximum\nentropy policies that comply with constraints demonstrated by expert\ntrajectories executing a task. The formulation of the method takes advantage of\nresults connecting performance to bounds for the KL-divergence between\ndemonstrated and learned policies, and its objective is rigorously justified\nthrough a connection to a probabilistic inference framework for reinforcement\nlearning, incorporating the reinforcement learning objective and the objective\nto abide by constraints in an entropy maximization setting. The proposed\nalgorithm optimizes the learning objective with dual gradient descent,\nsupporting effective and stable training. Experiments show that the proposed\nmethod can learn effective policy models for constraints-abiding behaviour, in\nsettings with multiple constraints of different types, accommodating different\nmodalities of demonstrated behaviour, and with abilities to generalize.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b66\u4e60\u7b26\u5408\u4e13\u5bb6\u8f68\u8ff9\u7ea6\u675f\u7684\u6700\u5927\u71b5\u7b56\u7565\uff0c\u901a\u8fc7KL\u6563\u5ea6\u8fde\u63a5\u6027\u80fd\u4e0e\u7b56\u7565\u5dee\u5f02\uff0c\u4f18\u5316\u76ee\u6807\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u7ea6\u675f\u9075\u5faa\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u7ed3\u5408\u6700\u5927\u71b5\u7b56\u7565\u548c\u7ea6\u675f\u9075\u5faa\uff0c\u4ee5\u751f\u6210\u66f4\u7a33\u5065\u4e14\u7b26\u5408\u4e13\u5bb6\u884c\u4e3a\u7684\u7b56\u7565\u3002", "method": "\u5229\u7528KL\u6563\u5ea6\u8fde\u63a5\u4e13\u5bb6\u4e0e\u5b66\u4e60\u7b56\u7565\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u4e0e\u7ea6\u675f\u9075\u5faa\uff0c\u91c7\u7528\u5bf9\u5076\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u7b26\u5408\u7ea6\u675f\u7684\u7b56\u7565\uff0c\u9002\u5e94\u591a\u79cd\u7ea6\u675f\u7c7b\u578b\u548c\u884c\u4e3a\u6a21\u6001\uff0c\u5e76\u5177\u5907\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u6210\u529f\u7ed3\u5408\u4e86\u6700\u5927\u71b5\u7b56\u7565\u4e0e\u7ea6\u675f\u9075\u5faa\uff0c\u8868\u73b0\u51fa\u7a33\u5b9a\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2507.06802", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06802", "abs": "https://arxiv.org/abs/2507.06802", "authors": ["Wonjin Jung", "Sungil Kang", "Dong-Yeon Cho"], "title": "Speech Tokenizer is Key to Consistent Representation", "comment": null, "summary": "Speech tokenization is crucial in digital speech processing, converting\ncontinuous speech signals into discrete units for various computational tasks.\nThis paper introduces a novel speech tokenizer with broad applicability across\ndownstream tasks. While recent advances in residual vector quantization (RVQ)\nhave incorporated semantic elements, they often neglect critical acoustic\nfeatures. We propose an advanced approach that simultaneously encodes both\nlinguistic and acoustic information, preserving prosodic and emotional content.\nOur method significantly enhances speech representation fidelity across diverse\napplications. Empirical evaluations demonstrate its effectiveness in speech\ncoding, voice conversion, emotion recognition, and multimodal language\nmodeling, without requiring additional training. This versatility underscores\nits potential as a key tool for advancing AI-driven speech processing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8bed\u97f3\u5206\u8bcd\u5668\uff0c\u80fd\u591f\u540c\u65f6\u7f16\u7801\u8bed\u8a00\u548c\u58f0\u5b66\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u8868\u5f81\u7684\u4fdd\u771f\u5ea6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\uff08RVQ\uff09\u65b9\u6cd5\u867d\u7136\u5f15\u5165\u4e86\u8bed\u4e49\u5143\u7d20\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u4e86\u5173\u952e\u7684\u58f0\u5b66\u7279\u5f81\uff0c\u5f71\u54cd\u4e86\u8bed\u97f3\u5904\u7406\u7684\u5168\u9762\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u7f16\u7801\u8bed\u8a00\u548c\u58f0\u5b66\u4fe1\u606f\uff0c\u4fdd\u7559\u97f5\u5f8b\u548c\u60c5\u611f\u5185\u5bb9\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bed\u97f3\u7f16\u7801\u3001\u8bed\u97f3\u8f6c\u6362\u3001\u60c5\u611f\u8bc6\u522b\u548c\u591a\u6a21\u6001\u8bed\u8a00\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u6709\u671b\u6210\u4e3a\u63a8\u52a8AI\u8bed\u97f3\u5904\u7406\u7684\u5173\u952e\u5de5\u5177\u3002"}}
{"id": "2507.06813", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06813", "abs": "https://arxiv.org/abs/2507.06813", "authors": ["Cosimo Fiorini", "Matteo Mosconi", "Pietro Buzzega", "Riccardo Salami", "Simone Calderara"], "title": "Intrinsic Training Signals for Federated Learning Aggregation", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy. While existing approaches\nfor aggregating client-specific classification heads and adapted backbone\nparameters require architectural modifications or loss function changes, our\nmethod uniquely leverages intrinsic training signals already available during\nstandard optimization. We present LIVAR (Layer Importance and VARiance-based\nmerging), which introduces: i) a variance-weighted classifier aggregation\nscheme using naturally emergent feature statistics, and ii) an\nexplainability-driven LoRA merging technique based on SHAP analysis of existing\nupdate parameter patterns. Without any architectural overhead, LIVAR achieves\nstate-of-the-art performance on multiple benchmarks while maintaining seamless\nintegration with existing FL methods. This work demonstrates that effective\nmodel merging can be achieved solely through existing training signals,\nestablishing a new paradigm for efficient federated model aggregation. The code\nwill be made publicly available upon acceptance.", "AI": {"tldr": "LIVAR\u662f\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u67b6\u6784\u6216\u635f\u5931\u51fd\u6570\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u8bad\u7ec3\u4fe1\u53f7\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u805a\u5408\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u4fee\u6539\u67b6\u6784\u6216\u635f\u5931\u51fd\u6570\uff0cLIVAR\u65e8\u5728\u5229\u7528\u73b0\u6709\u8bad\u7ec3\u4fe1\u53f7\u5b9e\u73b0\u9ad8\u6548\u805a\u5408\u3002", "method": "LIVAR\u91c7\u7528\u65b9\u5dee\u52a0\u6743\u7684\u5206\u7c7b\u5668\u805a\u5408\u548c\u57fa\u4e8eSHAP\u5206\u6790\u7684LoRA\u5408\u5e76\u6280\u672f\u3002", "result": "LIVAR\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u4e0e\u73b0\u6709\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "LIVAR\u8bc1\u660e\u4ec5\u901a\u8fc7\u73b0\u6709\u8bad\u7ec3\u4fe1\u53f7\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u805a\u5408\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.06819", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06819", "abs": "https://arxiv.org/abs/2507.06819", "authors": ["Philipp Schlinge", "Steffen Meinert", "Martin Atzmueller"], "title": "Comprehensive Evaluation of Prototype Neural Networks", "comment": null, "summary": "Prototype models are an important method for explainable artificial\nintelligence (XAI) and interpretable machine learning. In this paper, we\nperform an in-depth analysis of a set of prominent prototype models including\nProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive\nset of metrics. In addition to applying standard metrics from literature, we\npropose several new metrics to further complement the analysis of model\ninterpretability. In our experimentation, we apply the set of prototype models\non a diverse set of datasets including fine-grained classification, Non-IID\nsettings and multi-label classification to further contrast the performance.\nFurthermore, we also provide our code as an open-source library, which\nfacilitates simple application of the metrics itself, as well as extensibility\n- providing the option for easily adding new metrics and models.\nhttps://github.com/uos-sis/quanproto", "AI": {"tldr": "\u672c\u6587\u6df1\u5165\u5206\u6790\u4e86\u51e0\u79cd\u539f\u578b\u6a21\u578b\uff08\u5982ProtoPNet\u3001ProtoPool\u548cPIPNet\uff09\uff0c\u63d0\u51fa\u5e76\u5e94\u7528\u4e86\u4e00\u5957\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5305\u62ec\u65b0\u63d0\u51fa\u7684\u6307\u6807\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002\u5b9e\u9a8c\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u5e93\u3002", "motivation": "\u539f\u578b\u6a21\u578b\u662f\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7684\u91cd\u8981\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u548c\u5bf9\u6bd4\u5206\u6790\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5e94\u7528\u6807\u51c6\u548c\u65b0\u63d0\u51fa\u7684\u6307\u6807\uff0c\u5bf9\u591a\u79cd\u539f\u578b\u6a21\u578b\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u5c55\u793a\u4e86\u539f\u578b\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u5bf9\u6bd4\uff0c\u5e76\u9a8c\u8bc1\u4e86\u65b0\u6307\u6807\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u539f\u578b\u6a21\u578b\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6307\u6807\u548c\u5f00\u6e90\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86XAI\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.06821", "categories": ["cs.LG", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.06821", "abs": "https://arxiv.org/abs/2507.06821", "authors": ["Chuhang Zheng", "Chunwei Tian", "Jie Wen", "Daoqiang Zhang", "Qi Zhu"], "title": "HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning", "comment": null, "summary": "Multi-modal emotion recognition has garnered increasing attention as it plays\na significant role in human-computer interaction (HCI) in recent years. Since\ndifferent discrete emotions may exist at the same time, compared with\nsingle-class emotion recognition, emotion distribution learning (EDL) that\nidentifies a mixture of basic emotions has gradually emerged as a trend.\nHowever, existing EDL methods face challenges in mining the heterogeneity among\nmultiple modalities. Besides, rich semantic correlations across arbitrary basic\nemotions are not fully exploited. In this paper, we propose a multi-modal\nemotion distribution learning framework, named HeLo, aimed at fully exploring\nthe heterogeneity and complementary information in multi-modal emotional data\nand label correlation within mixed basic emotions. Specifically, we first adopt\ncross-attention to effectively fuse the physiological data. Then, an optimal\ntransport (OT)-based heterogeneity mining module is devised to mine the\ninteraction and heterogeneity between the physiological and behavioral\nrepresentations. To facilitate label correlation learning, we introduce a\nlearnable label embedding optimized by correlation matrix alignment. Finally,\nthe learnable label embeddings and label correlation matrices are integrated\nwith the multi-modal representations through a novel label correlation-driven\ncross-attention mechanism for accurate emotion distribution learning.\nExperimental results on two publicly available datasets demonstrate the\nsuperiority of our proposed method in emotion distribution learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHeLo\u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u5e03\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u6316\u6398\u591a\u6a21\u6001\u60c5\u611f\u6570\u636e\u7684\u5f02\u8d28\u6027\u548c\u4e92\u8865\u4fe1\u606f\uff0c\u4ee5\u53ca\u6df7\u5408\u57fa\u672c\u60c5\u611f\u4e2d\u7684\u6807\u7b7e\u76f8\u5173\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6316\u6398\u591a\u6a21\u6001\u5f02\u8d28\u6027\u548c\u60c5\u611f\u6807\u7b7e\u76f8\u5173\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u751f\u7406\u6570\u636e\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u5f02\u8d28\u6027\u6316\u6398\u6a21\u5757\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6807\u7b7e\u5d4c\u5165\u548c\u76f8\u5173\u6027\u77e9\u9635\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u6807\u7b7e\u76f8\u5173\u6027\u9a71\u52a8\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u60c5\u611f\u5206\u5e03\u5b66\u4e60\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u60c5\u611f\u5206\u5e03\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "HeLo\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u5e03\u5b66\u4e60\u4e2d\u7684\u5f02\u8d28\u6027\u548c\u6807\u7b7e\u76f8\u5173\u6027\u6316\u6398\u95ee\u9898\u3002"}}
{"id": "2507.06825", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06825", "abs": "https://arxiv.org/abs/2507.06825", "authors": ["Matej Straka", "Martin Schmid"], "title": "Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning", "comment": null, "summary": "We introduce a real-time strategy game environment built on Generals.io, a\ngame that hosts thousands of active players each week across multiple game\nformats. Our environment is fully compatible with Gymnasium and PettingZoo,\ncapable of running thousands of frames per second on commodity hardware. Our\nreference agent -- trained with supervised pre-training and self-play -- hits\nthe top 0.003\\% of the 1v1 human leaderboard after just 36 hours on a single\nH100 GPU. To accelerate learning, we incorporate potential-based reward shaping\nand memory features. Our contributions -- a modular RTS benchmark and a\ncompetitive, state-of-the-art baseline agent -- provide an accessible yet\nchallenging platform for advancing multi-agent reinforcement learning research.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8eGenerals.io\u7684\u5b9e\u65f6\u7b56\u7565\u6e38\u620f\u73af\u5883\uff0c\u517c\u5bb9Gymnasium\u548cPettingZoo\uff0c\u652f\u6301\u9ad8\u6027\u80fd\u8fd0\u884c\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u9876\u7ea7AI\u4ee3\u7406\u3002", "motivation": "\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u6613\u7528\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u5e73\u53f0\u3002", "method": "\u7ed3\u5408\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u81ea\u5bf9\u5f08\u8bad\u7ec3AI\u4ee3\u7406\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u6f5c\u5728\u5956\u52b1\u5851\u9020\u548c\u8bb0\u5fc6\u7279\u5f81\u52a0\u901f\u5b66\u4e60\u3002", "result": "\u8bad\u7ec336\u5c0f\u65f6\u540e\uff0cAI\u4ee3\u7406\u57281v1\u4eba\u7c7b\u6392\u884c\u699c\u4e2d\u8fbe\u5230\u524d0.003%\u3002", "conclusion": "\u8be5\u73af\u5883\u53ca\u57fa\u51c6\u4ee3\u7406\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7ade\u4e89\u6027\u7684\u5de5\u5177\u3002"}}
{"id": "2507.06839", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.06839", "abs": "https://arxiv.org/abs/2507.06839", "authors": ["Jihao Andreas Lin"], "title": "Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise Conditioning", "comment": "PhD Thesis, University of Cambridge", "summary": "Gaussian processes are a powerful framework for uncertainty-aware function\napproximation and sequential decision-making. Unfortunately, their classical\nformulation does not scale gracefully to large amounts of data and modern\nhardware for massively-parallel computation, prompting many researchers to\ndevelop techniques which improve their scalability. This dissertation focuses\non the powerful combination of iterative methods and pathwise conditioning to\ndevelop methodological contributions which facilitate the use of Gaussian\nprocesses in modern large-scale settings. By combining these two techniques\nsynergistically, expensive computations are expressed as solutions to systems\nof linear equations and obtained by leveraging iterative linear system solvers.\nThis drastically reduces memory requirements, facilitating application to\nsignificantly larger amounts of data, and introduces matrix multiplication as\nthe main computational operation, which is ideal for modern hardware.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fed\u4ee3\u65b9\u6cd5\u548c\u8def\u5f84\u6761\u4ef6\u5316\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u9ad8\u65af\u8fc7\u7a0b\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u9ad8\u65af\u8fc7\u7a0b\u5728\u7ecf\u5178\u6846\u67b6\u4e0b\u96be\u4ee5\u9002\u5e94\u5927\u89c4\u6a21\u6570\u636e\u548c\u73b0\u4ee3\u5e76\u884c\u8ba1\u7b97\u786c\u4ef6\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u5176\u53ef\u6269\u5c55\u6027\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u8fed\u4ee3\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u5668\u548c\u8def\u5f84\u6761\u4ef6\u5316\uff0c\u5c06\u6602\u8d35\u8ba1\u7b97\u8f6c\u5316\u4e3a\u7ebf\u6027\u65b9\u7a0b\u7ec4\u6c42\u89e3\uff0c\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002", "result": "\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u66f4\u5927\u89c4\u6a21\u6570\u636e\uff0c\u5e76\u4f18\u5316\u4e86\u73b0\u4ee3\u786c\u4ef6\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u65af\u8fc7\u7a0b\u5728\u73b0\u4ee3\u5927\u89c4\u6a21\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06853", "categories": ["cs.LG", "cs.AI", "cs.CE", "physics.chem-ph", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2507.06853", "abs": "https://arxiv.org/abs/2507.06853", "authors": ["Liang Wang", "Yu Rong", "Tingyang Xu", "Zhenyi Zhong", "Zhiyuan Liu", "Pengju Wang", "Deli Zhao", "Qiang Liu", "Shu Wu", "Liang Wang"], "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models", "comment": null, "summary": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation.", "AI": {"tldr": "DiffSpectra\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u591a\u6a21\u6001\u5149\u8c31\u6570\u636e\u63a8\u65ad2D\u548c3D\u5206\u5b50\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u89e3\u91ca\u4e14\u7f3a\u4e4f\u6269\u5c55\u6027\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u5e93\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u5206\u5b50\u3002DiffSpectra\u65e8\u5728\u901a\u8fc7\u751f\u6210\u6a21\u578b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "DiffSpectra\u91c7\u7528\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7SE(3)-\u7b49\u53d8\u67b6\u6784\uff08Diffusion Molecule Transformer\uff09\u548c\u57fa\u4e8eTransformer\u7684\u5149\u8c31\u7f16\u7801\u5668\uff08SpecFormer\uff09\u6574\u5408\u62d3\u6251\u548c\u51e0\u4f55\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiffSpectra\u5728\u7ed3\u6784\u89e3\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0ctop-1\u51c6\u786e\u7387\u4e3a16.01%\uff0ctop-20\u4e3a96.86%\u30023D\u51e0\u4f55\u5efa\u6a21\u548c\u591a\u6a21\u6001\u6761\u4ef6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "DiffSpectra\u9996\u6b21\u7edf\u4e00\u4e86\u591a\u6a21\u6001\u5149\u8c31\u63a8\u7406\u548c2D/3D\u751f\u6210\u5efa\u6a21\uff0c\u4e3a\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06859", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06859", "abs": "https://arxiv.org/abs/2507.06859", "authors": ["Zitian Li", "Wang Chi Cheung"], "title": "Episodic Contextual Bandits with Knapsacks under Conversion Models", "comment": null, "summary": "We study an online setting, where a decision maker (DM) interacts with\ncontextual bandit-with-knapsack (BwK) instances in repeated episodes. These\nepisodes start with different resource amounts, and the contexts' probability\ndistributions are non-stationary in an episode. All episodes share the same\nlatent conversion model, which governs the random outcome contingent upon a\nrequest's context and an allocation decision. Our model captures applications\nsuch as dynamic pricing on perishable resources with episodic replenishment,\nand first price auctions in repeated episodes with different starting budgets.\nWe design an online algorithm that achieves a regret sub-linear in $T$, the\nnumber of episodes, assuming access to a \\emph{confidence bound oracle} that\nachieves an $o(T)$-regret. Such an oracle is readily available from existing\ncontextual bandit literature. We overcome the technical challenge with\narbitrarily many possible contexts, which leads to a reinforcement learning\nproblem with an unbounded state space. Our framework provides improved regret\nbounds in certain settings when the DM is provided with unlabeled feature data,\nwhich is novel to the contextual BwK literature.", "AI": {"tldr": "\u7814\u7a76\u5728\u7ebf\u51b3\u7b56\u8005\u5728\u8d44\u6e90\u52a8\u6001\u53d8\u5316\u7684\u4e0a\u4e0b\u6587BwK\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u5b9a\u4ef7\u548c\u62cd\u5356\u7b49\u5e94\u7528\u4e2d\u8d44\u6e90\u5206\u914d\u7684\u975e\u5e73\u7a33\u6027\u548c\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5728\u7ebf\u7b97\u6cd5\uff0c\u5229\u7528\u7f6e\u4fe1\u533a\u95f4\u9884\u8a00\u673a\u5904\u7406\u65e0\u754c\u72b6\u6001\u7a7a\u95f4\u3002", "result": "\u7b97\u6cd5\u5728T\u6b21\u8fed\u4ee3\u4e2d\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\uff0c\u5e76\u5728\u7279\u5b9a\u8bbe\u7f6e\u4e0b\u63d0\u4f9b\u6539\u8fdb\u7684\u9057\u61be\u8fb9\u754c\u3002", "conclusion": "\u6846\u67b6\u4e3a\u4e0a\u4e0b\u6587BwK\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u672a\u6807\u8bb0\u7279\u5f81\u6570\u636e\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2507.06888", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06888", "abs": "https://arxiv.org/abs/2507.06888", "authors": ["Wei Chen", "Wanyang Gu", "Linjun Peng", "Ruichu Cai", "Zhifeng Hao", "Kun Zhang"], "title": "Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants", "comment": null, "summary": "Federated causal discovery aims to uncover the causal relationships between\nentities while protecting data privacy, which has significant importance and\nnumerous applications in real-world scenarios. Existing federated causal\nstructure learning methods primarily focus on horizontal federated settings.\nHowever, in practical situations, different clients may not necessarily contain\ndata on the same variables. In a single client, the incomplete set of variables\ncan easily lead to spurious causal relationships, thereby affecting the\ninformation transmitted to other clients. To address this issue, we\ncomprehensively consider causal structure learning methods under both\nhorizontal and vertical federated settings. We provide the identification\ntheories and methods for learning causal structure in the horizontal and\nvertical federal setting via higher-order cumulants. Specifically, we first\naggregate higher-order cumulant information from all participating clients to\nconstruct global cumulant estimates. These global estimates are then used for\nrecursive source identification, ultimately yielding a global causal strength\nmatrix. Our approach not only enables the reconstruction of causal graphs but\nalso facilitates the estimation of causal strength coefficients. Our algorithm\ndemonstrates superior performance in experiments conducted on both synthetic\ndata and real-world data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u9636\u7d2f\u79ef\u91cf\u5728\u6c34\u5e73\u548c\u5782\u76f4\u8054\u90a6\u8bbe\u7f6e\u4e0b\u5b66\u4e60\u56e0\u679c\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u548c\u53d8\u91cf\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u56e0\u679c\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6c34\u5e73\u8054\u90a6\u8bbe\u7f6e\uff0c\u4f46\u5b9e\u9645\u4e2d\u4e0d\u540c\u5ba2\u6237\u7aef\u53ef\u80fd\u5305\u542b\u4e0d\u540c\u53d8\u91cf\uff0c\u5bfc\u81f4\u865a\u5047\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u805a\u5408\u5ba2\u6237\u7aef\u7684\u9ad8\u9636\u7d2f\u79ef\u91cf\u4fe1\u606f\u6784\u5efa\u5168\u5c40\u4f30\u8ba1\uff0c\u7528\u4e8e\u9012\u5f52\u6e90\u8bc6\u522b\uff0c\u751f\u6210\u5168\u5c40\u56e0\u679c\u5f3a\u5ea6\u77e9\u9635\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u91cd\u6784\u56e0\u679c\u56fe\u5e76\u4f30\u8ba1\u56e0\u679c\u5f3a\u5ea6\u7cfb\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6c34\u5e73\u548c\u5782\u76f4\u8054\u90a6\u8bbe\u7f6e\u4e0b\u5747\u6709\u6548\uff0c\u89e3\u51b3\u4e86\u53d8\u91cf\u4e0d\u5b8c\u6574\u548c\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u3002"}}
{"id": "2507.06892", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06892", "abs": "https://arxiv.org/abs/2507.06892", "authors": ["Jing Liang", "Hongyao Tang", "Yi Ma", "Jinyi Liu", "Yan Zheng", "Shuyue Hu", "Lei Bai", "Jianye Hao"], "title": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model", "comment": "Preliminary version. Project page:\n  https://anitaleungxx.github.io/ReMix", "summary": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReMix\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u7b56\u7565\u548c\u7b56\u7565\u91cd\u751f\u6280\u672f\uff0c\u5229\u7528\u79bb\u7b56\u7565\u6570\u636e\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u65b9\u6cd5\u591a\u4e3a\u540c\u7b56\u7565\uff0c\u6570\u636e\u5229\u7528\u7387\u4f4e\uff0c\u8ba1\u7b97\u548c\u65f6\u95f4\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u6269\u5c55\u3002", "method": "ReMix\u5305\u542b\u6df7\u5408\u7b56\u7565\u8fd1\u7aef\u7b56\u7565\u68af\u5ea6\u3001KL\u51f8\u7b56\u7565\u7ea6\u675f\u548c\u7b56\u7565\u91cd\u751f\u4e09\u90e8\u5206\uff0c\u652f\u6301\u540c\u7b56\u7565\u65b9\u6cd5\u5229\u7528\u79bb\u7b56\u7565\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u663e\u793aReMix\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u6210\u672c\u964d\u4f4e30\u81f3450\u500d\uff0c\u6027\u80fd\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "conclusion": "ReMix\u901a\u8fc7\u9ad8\u6548\u5229\u7528\u79bb\u7b56\u7565\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u540c\u7b56\u7565\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.06901", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06901", "abs": "https://arxiv.org/abs/2507.06901", "authors": ["Abolfazl Zarghani", "Sadegh Abedi"], "title": "Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams", "comment": null, "summary": "Multi-dimensional data streams, prevalent in applications like IoT, financial\nmarkets, and real-time analytics, pose significant challenges due to their high\nvelocity, unbounded nature, and complex inter-dimensional dependencies. Sliding\nwindow techniques are critical for processing such streams, but fixed-size\nwindows struggle to adapt to dynamic changes like concept drift or bursty\npatterns. This paper proposes a novel reinforcement learning (RL)-based\napproach to dynamically optimize sliding window sizes for multi-dimensional\ndata streams. By formulating window size selection as an RL problem, we enable\nan agent to learn an adaptive policy based on stream characteristics, such as\nvariance, correlations, and temporal trends. Our method, RL-Window, leverages a\nDueling Deep Q-Network (DQN) with prioritized experience replay to handle\nnon-stationarity and high-dimensionality. Evaluations on benchmark datasets\n(UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms\nstate-of-the-art methods like ADWIN and CNN-Adaptive in classification\naccuracy, drift robustness, and computational efficiency. Additional\nqualitative analyses, extended metrics (e.g., energy efficiency, latency), and\na comprehensive dataset characterization further highlight its adaptability and\nstability, making it suitable for real-time applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u6ed1\u52a8\u7a97\u53e3\u4f18\u5316\u65b9\u6cd5\uff08RL-Window\uff09\uff0c\u7528\u4e8e\u5904\u7406\u591a\u7ef4\u6570\u636e\u6d41\uff0c\u89e3\u51b3\u4e86\u56fa\u5b9a\u7a97\u53e3\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u591a\u7ef4\u6570\u636e\u6d41\uff08\u5982IoT\u3001\u91d1\u878d\u5e02\u573a\u7b49\uff09\u7684\u9ad8\u901f\u5ea6\u3001\u65e0\u754c\u6027\u548c\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u5e26\u6765\u4e86\u5904\u7406\u6311\u6218\uff0c\u56fa\u5b9a\u7a97\u53e3\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\uff08\u5982\u6982\u5ff5\u6f02\u79fb\u6216\u7a81\u53d1\u6a21\u5f0f\uff09\u3002", "method": "\u5c06\u7a97\u53e3\u5927\u5c0f\u9009\u62e9\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u91c7\u7528Dueling DQN\u548c\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u6280\u672f\uff0c\u81ea\u9002\u5e94\u5b66\u4e60\u7a97\u53e3\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cRL-Window\u5728\u5206\u7c7b\u7cbe\u5ea6\u3001\u6f02\u79fb\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8eADWIN\u548cCNN-Adaptive\u7b49\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RL-Window\u5177\u6709\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u6269\u5c55\u6307\u6807\uff08\u5982\u80fd\u6548\u3001\u5ef6\u8fdf\uff09\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002"}}
{"id": "2507.06907", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06907", "abs": "https://arxiv.org/abs/2507.06907", "authors": ["Linyun Gao", "Qiang Wen", "Fumio Machida"], "title": "Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting", "comment": "27 pages including appendix, 1 figure", "summary": "Autonomous driving is rapidly advancing as a key application of machine\nlearning, yet ensuring the safety of these systems remains a critical\nchallenge. Traffic sign recognition, an essential component of autonomous\nvehicles, is particularly vulnerable to adversarial attacks that can compromise\ndriving safety. In this paper, we propose an N-version machine learning (NVML)\nframework that integrates a safety-aware weighted soft voting mechanism. Our\napproach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential\nsafety risks and assign dynamic, safety-aware weights to the ensemble outputs.\nWe evaluate the robustness of three-version NVML systems employing various\nvoting mechanisms against adversarial samples generated using the Fast Gradient\nSign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental\nresults demonstrate that our NVML approach significantly enhances the\nrobustness and safety of traffic sign recognition systems under adversarial\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eN\u7248\u672c\u673a\u5668\u5b66\u4e60\uff08NVML\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b89\u5168\u611f\u77e5\u52a0\u6743\u8f6f\u6295\u7968\u673a\u5236\u63d0\u5347\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u7684\u6297\u5bf9\u6297\u653b\u51fb\u80fd\u529b\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u5f71\u54cd\uff0c\u5b89\u5168\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528NVML\u6846\u67b6\uff0c\u7ed3\u5408FMEA\u8bc4\u4f30\u5b89\u5168\u98ce\u9669\uff0c\u52a8\u6001\u5206\u914d\u6743\u91cd\uff1b\u6d4b\u8bd5\u4e86\u4e09\u79cd\u6295\u7968\u673a\u5236\u5bf9\u6297FGSM\u548cPGD\u653b\u51fb\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNVML\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "NVML\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.06931", "categories": ["cs.LG", "cs.DC", "cs.MA", "cs.SI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.06931", "abs": "https://arxiv.org/abs/2507.06931", "authors": ["Tongtian Zhu", "Wenhao Li", "Can Wang", "Fengxiang He"], "title": "DICE: Data Influence Cascade in Decentralized Learning", "comment": "Published as a poster at ICLR 2025", "summary": "Decentralized learning offers a promising approach to crowdsource data\nconsumptions and computational workloads across geographically distributed\ncompute interconnected through peer-to-peer networks, accommodating the\nexponentially increasing demands. However, proper incentives are still in\nabsence, considerably discouraging participation. Our vision is that a fair\nincentive mechanism relies on fair attribution of contributions to\nparticipating nodes, which faces non-trivial challenges arising from the\nlocalized connections making influence ``cascade'' in a decentralized network.\nTo overcome this, we design the first method to estimate \\textbf{D}ata\n\\textbf{I}nfluence \\textbf{C}ascad\\textbf{E} (DICE) in a decentralized\nenvironment. Theoretically, the framework derives tractable approximations of\ninfluence cascade over arbitrary neighbor hops, suggesting the influence\ncascade is determined by an interplay of data, communication topology, and the\ncurvature of loss landscape. DICE also lays the foundations for applications\nincluding selecting suitable collaborators and identifying malicious behaviors.\nProject page is available at https://raiden-zhu.github.io/blog/2025/DICE/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDICE\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u53bb\u4e2d\u5fc3\u5316\u7f51\u7edc\u4e2d\u4f30\u8ba1\u6570\u636e\u5f71\u54cd\u529b\u4f20\u64ad\uff0c\u4ee5\u89e3\u51b3\u53c2\u4e0e\u8005\u6fc0\u52b1\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u867d\u80fd\u5206\u6563\u6570\u636e\u6d88\u8017\u548c\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u4f46\u7f3a\u4e4f\u516c\u5e73\u7684\u6fc0\u52b1\u673a\u5236\uff0c\u963b\u788d\u4e86\u53c2\u4e0e\u79ef\u6781\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86DICE\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u8fd1\u4f3c\u8ba1\u7b97\u4efb\u610f\u90bb\u5c45\u8df3\u6570\u7684\u5f71\u54cd\u529b\u4f20\u64ad\uff0c\u7ed3\u5408\u6570\u636e\u3001\u901a\u4fe1\u62d3\u6251\u548c\u635f\u5931\u666f\u89c2\u66f2\u7387\u3002", "result": "DICE\u4e3a\u9009\u62e9\u5408\u9002\u5408\u4f5c\u8005\u548c\u8bc6\u522b\u6076\u610f\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "DICE\u662f\u9996\u4e2a\u5728\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u4f30\u8ba1\u6570\u636e\u5f71\u54cd\u529b\u4f20\u64ad\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.06952", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06952", "abs": "https://arxiv.org/abs/2507.06952", "authors": ["Keyon Vafa", "Peter G. Chang", "Ashesh Rambachan", "Sendhil Mullainathan"], "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models", "comment": "To appear in ICML 2025", "summary": "Foundation models are premised on the idea that sequence prediction can\nuncover deeper domain understanding, much like how Kepler's predictions of\nplanetary motion later led to the discovery of Newtonian mechanics. However,\nevaluating whether these models truly capture deeper structure remains a\nchallenge. We develop a technique for evaluating foundation models that\nexamines how they adapt to synthetic datasets generated from some postulated\nworld model. Our technique measures whether the foundation model's inductive\nbias aligns with the world model, and so we refer to it as an inductive bias\nprobe. Across multiple domains, we find that foundation models can excel at\ntheir training tasks yet fail to develop inductive biases towards the\nunderlying world model when adapted to new tasks. We particularly find that\nfoundation models trained on orbital trajectories consistently fail to apply\nNewtonian mechanics when adapted to new physics tasks. Further analysis reveals\nthat these models behave as if they develop task-specific heuristics that fail\nto generalize.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u6df1\u5c42\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u9002\u5e94\u65b0\u4efb\u52a1\u65f6\u53ef\u80fd\u65e0\u6cd5\u5f62\u6210\u4e0e\u5e95\u5c42\u4e16\u754c\u6a21\u578b\u4e00\u81f4\u7684\u5f52\u7eb3\u504f\u5dee\u3002", "motivation": "\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u662f\u5426\u80fd\u591f\u901a\u8fc7\u5e8f\u5217\u9884\u6d4b\u63ed\u793a\u6df1\u5c42\u9886\u57df\u7406\u89e3\uff0c\u7c7b\u4f3c\u4e8e\u5f00\u666e\u52d2\u9884\u6d4b\u884c\u661f\u8fd0\u52a8\u540e\u53d1\u73b0\u7684\u725b\u987f\u529b\u5b66\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6280\u672f\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u5e76\u68c0\u67e5\u6a21\u578b\u5982\u4f55\u9002\u5e94\u8fd9\u4e9b\u6570\u636e\uff0c\u6d4b\u91cf\u5176\u5f52\u7eb3\u504f\u5dee\u662f\u5426\u4e0e\u4e16\u754c\u6a21\u578b\u4e00\u81f4\u3002", "result": "\u57fa\u7840\u6a21\u578b\u5728\u8bad\u7ec3\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9002\u5e94\u65b0\u4efb\u52a1\u65f6\u672a\u80fd\u5f62\u6210\u4e0e\u5e95\u5c42\u4e16\u754c\u6a21\u578b\u4e00\u81f4\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u5c24\u5176\u662f\u5728\u8f68\u9053\u8f68\u8ff9\u4efb\u52a1\u4e2d\u672a\u80fd\u5e94\u7528\u725b\u987f\u529b\u5b66\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u4ec5\u53d1\u5c55\u51fa\u4efb\u52a1\u7279\u5b9a\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u800c\u672a\u80fd\u5b9e\u73b0\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.06967", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06967", "abs": "https://arxiv.org/abs/2507.06967", "authors": ["Sebastien Andre-Sloan", "Anirbit Mukherjee", "Matthew Colbrook"], "title": "Noisy PDE Training Requires Bigger PINNs", "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) are increasingly used to approximate\nsolutions of partial differential equations (PDEs), especially in high\ndimensions. In real-world applications, data samples are noisy, so it is\nimportant to know when a predictor can still achieve low empirical risk.\nHowever, little is known about the conditions under which a PINN can do so\neffectively. We prove a lower bound on the size of neural networks required for\nthe supervised PINN empirical risk to fall below the variance of noisy\nsupervision labels. Specifically, if a predictor achieves an empirical risk\n$O(\\eta)$ below $\\sigma^2$ (variance of supervision data), then necessarily\n$d_N\\log d_N\\gtrsim N_s \\eta^2$, where $N_s$ is the number of samples and $d_N$\nis the number of trainable parameters of the PINN. A similar constraint applies\nto the fully unsupervised PINN setting when boundary labels are sampled\nnoisily. Consequently, increasing the number of noisy supervision labels alone\ndoes not provide a ``free lunch'' in reducing empirical risk. We also show\nempirically that PINNs can indeed achieve empirical risks below $\\sigma^2$\nunder such conditions. As a case study, we investigate PINNs applied to the\nHamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for\nquantitatively understanding the parameter requirements for training PINNs in\nthe presence of noise.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u566a\u58f0\u6570\u636e\u4e0b\uff0c\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u8fbe\u5230\u4f4e\u7ecf\u9a8c\u98ce\u9669\u7684\u6761\u4ef6\uff0c\u5e76\u8bc1\u660e\u4e86\u7f51\u7edc\u89c4\u6a21\u7684\u4e0b\u754c\u3002", "motivation": "\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u6570\u636e\u6837\u672c\u901a\u5e38\u5e26\u6709\u566a\u58f0\uff0c\u56e0\u6b64\u9700\u8981\u660e\u786ePINNs\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u4ecd\u80fd\u6709\u6548\u964d\u4f4e\u7ecf\u9a8c\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5728\u76d1\u7763\u548c\u65e0\u76d1\u7763PINN\u8bbe\u7f6e\u4e0b\uff0c\u7f51\u7edc\u89c4\u6a21\u4e0e\u6837\u672c\u6570\u91cf\u53ca\u566a\u58f0\u65b9\u5dee\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4ec5\u589e\u52a0\u566a\u58f0\u76d1\u7763\u6807\u7b7e\u6570\u91cf\u65e0\u6cd5\u514d\u8d39\u964d\u4f4e\u7ecf\u9a8c\u98ce\u9669\uff0c\u5e76\u9a8c\u8bc1\u4e86PINNs\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u4ee5\u8fbe\u5230\u4f4e\u4e8e\u566a\u58f0\u65b9\u5dee\u7684\u7ecf\u9a8c\u98ce\u9669\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5b9a\u91cf\u7406\u89e3\u566a\u58f0\u73af\u5883\u4e0b\u8bad\u7ec3PINNs\u7684\u53c2\u6570\u9700\u6c42\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.06969", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.06969", "abs": "https://arxiv.org/abs/2507.06969", "authors": ["Bogdan Kulynych", "Juan Felipe Gomez", "Georgios Kaissis", "Jamie Hayes", "Borja Balle", "Flavio du Pin Calmon", "Jean Louis Raisaro"], "title": "Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy", "comment": null, "summary": "Differentially private (DP) mechanisms are difficult to interpret and\ncalibrate because existing methods for mapping standard privacy parameters to\nconcrete privacy risks -- re-identification, attribute inference, and data\nreconstruction -- are both overly pessimistic and inconsistent. In this work,\nwe use the hypothesis-testing interpretation of DP ($f$-DP), and determine that\nbounds on attack success can take the same unified form across\nre-identification, attribute inference, and data reconstruction risks. Our\nunified bounds are (1) consistent across a multitude of attack settings, and\n(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary\n(including worst-case) levels of baseline risk. Empirically, our results are\ntighter than prior methods using $\\varepsilon$-DP, R\\'enyi DP, and concentrated\nDP. As a result, calibrating noise using our bounds can reduce the required\nnoise by 20% at the same risk level, which yields, e.g., more than 15pp\naccuracy increase in a text classification task. Overall, this unifying\nperspective provides a principled framework for interpreting and calibrating\nthe degree of protection in DP against specific levels of re-identification,\nattribute inference, or data reconstruction risk.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5047\u8bbe\u68c0\u9a8c\u89e3\u91ca\u7684\u5dee\u5206\u9690\u79c1\uff08f-DP\uff09\u65b9\u6cd5\uff0c\u7edf\u4e00\u4e86\u91cd\u8bc6\u522b\u3001\u5c5e\u6027\u63a8\u65ad\u548c\u6570\u636e\u91cd\u5efa\u98ce\u9669\u7684\u754c\u9650\uff0c\u63d0\u4f9b\u4e86\u66f4\u7d27\u81f4\u7684\u9690\u79c1\u4fdd\u62a4\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u5dee\u5206\u9690\u79c1\u673a\u5236\u5728\u9690\u79c1\u53c2\u6570\u6620\u5c04\u5230\u5177\u4f53\u98ce\u9669\uff08\u5982\u91cd\u8bc6\u522b\u3001\u5c5e\u6027\u63a8\u65ad\u548c\u6570\u636e\u91cd\u5efa\uff09\u65f6\u8fc7\u4e8e\u60b2\u89c2\u4e14\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u66f4\u7edf\u4e00\u548c\u53ef\u8c03\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528f-DP\u7684\u5047\u8bbe\u68c0\u9a8c\u89e3\u91ca\uff0c\u63a8\u5bfc\u51fa\u7edf\u4e00\u7684\u98ce\u9669\u754c\u9650\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u653b\u51fb\u573a\u666f\uff0c\u5e76\u53ef\u8c03\u6574\u4ee5\u8bc4\u4f30\u4efb\u610f\u57fa\u7ebf\u98ce\u9669\u6c34\u5e73\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u03b5-DP\u3001R\u00e9nyi DP\u548c\u96c6\u4e2dDP\u66f4\u7d27\u81f4\uff0c\u80fd\u5c06\u566a\u58f0\u51cf\u5c1120%\uff0c\u5e76\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u63d0\u9ad815%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5dee\u5206\u9690\u79c1\u7684\u4fdd\u62a4\u7a0b\u5ea6\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u89e3\u91ca\u548c\u6821\u51c6\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5177\u4f53\u98ce\u9669\u6c34\u5e73\u7684\u8bc4\u4f30\u3002"}}
{"id": "2507.06979", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06979", "abs": "https://arxiv.org/abs/2507.06979", "authors": ["Panagiotis Koromilas", "Efthymios Georgiou", "Giorgos Bouritsas", "Theodoros Giannakopoulos", "Mihalis A. Nicolaou", "Yannis Panagakis"], "title": "A Principled Framework for Multi-View Contrastive Learning", "comment": null, "summary": "Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning\n(SSL), typically relies on pairs of data views generated through augmentation.\nWhile multiple augmentations per instance (more than two) improve\ngeneralization in supervised learning, current CL methods handle additional\nviews suboptimally by simply aggregating different pairwise objectives. This\napproach suffers from four critical limitations: (L1) it utilizes multiple\noptimization terms per data point resulting to conflicting objectives, (L2) it\nfails to model all interactions across views and data points, (L3) it inherits\nfundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL\nlosses, and (L4) it prevents fully realizing the benefits of increased view\nmultiplicity observed in supervised settings. We address these limitations\nthrough two novel loss functions: MV-InfoNCE, which extends InfoNCE to\nincorporate all possible view interactions simultaneously in one term per data\npoint, and MV-DHEL, which decouples alignment from uniformity across views\nwhile scaling interaction complexity with view multiplicity. Both approaches\nare theoretically grounded - we prove they asymptotically optimize for\nalignment of all views and uniformity, providing principled extensions to\nmulti-view contrastive learning. Our empirical results on ImageNet1K and three\nother datasets demonstrate that our methods consistently outperform existing\nmulti-view approaches and effectively scale with increasing view multiplicity.\nWe also apply our objectives to multimodal data and show that, in contrast to\nother contrastive objectives, they can scale beyond just two modalities. Most\nsignificantly, ablation studies reveal that MV-DHEL with five or more views\neffectively mitigates dimensionality collapse by fully utilizing the embedding\nspace, thereby delivering multi-view benefits observed in supervised learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff08MV-InfoNCE\u548cMV-DHEL\uff09\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u5b58\u5728\u7684\u56db\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u80fd\u6709\u6548\u5229\u7528\u591a\u89c6\u56fe\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u56db\u4e2a\u5173\u952e\u5c40\u9650\u6027\uff08\u5982\u76ee\u6807\u51b2\u7a81\u3001\u89c6\u56fe\u4ea4\u4e92\u5efa\u6a21\u4e0d\u8db3\u7b49\uff09\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u89c6\u56fe\u5e26\u6765\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u635f\u5931\u51fd\u6570\uff1aMV-InfoNCE\uff08\u540c\u65f6\u5efa\u6a21\u6240\u6709\u89c6\u56fe\u4ea4\u4e92\uff09\u548cMV-DHEL\uff08\u89e3\u8026\u5bf9\u9f50\u4e0e\u5747\u5300\u6027\uff09\u3002", "result": "\u5728ImageNet1K\u7b49\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u80fd\u6269\u5c55\u5230\u591a\u6a21\u6001\u6570\u636e\u3002MV-DHEL\u5c24\u5176\u80fd\u6709\u6548\u7f13\u89e3\u7ef4\u5ea6\u5d29\u6e83\u95ee\u9898\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5145\u5206\u5229\u7528\u591a\u89c6\u56fe\u4f18\u52bf\uff0c\u5e76\u9002\u7528\u4e8e\u591a\u6a21\u6001\u573a\u666f\u3002"}}
{"id": "2507.06996", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06996", "abs": "https://arxiv.org/abs/2507.06996", "authors": ["Eunbyeol Cho", "Jiyoun Kim", "Minjae Lee", "Sungjin Park", "Edward Choi"], "title": "Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing", "comment": null, "summary": "Electronic Health Records (EHR) are time-series relational databases that\nrecord patient interactions and medical events over time, serving as a critical\nresource for healthcare research and applications. However, privacy concerns\nand regulatory restrictions limit the sharing and utilization of such sensitive\ndata, necessitating the generation of synthetic EHR datasets. Unlike previous\nEHR synthesis methods, which typically generate medical records consisting of\nexpert-chosen features (e.g. a few vital signs or structured codes only), we\nintroduce RawMed, the first framework to synthesize multi-table, time-series\nEHR data that closely resembles raw EHRs. Using text-based representation and\ncompression techniques, RawMed captures complex structures and temporal\ndynamics with minimal preprocessing. We also propose a new evaluation framework\nfor multi-table time-series synthetic EHRs, assessing distributional\nsimilarity, inter-table relationships, temporal dynamics, and privacy.\nValidated on two open-source EHR datasets, RawMed outperforms baseline models\nin fidelity and utility. The code is available at\nhttps://github.com/eunbyeol-cho/RawMed.", "AI": {"tldr": "RawMed\u662f\u4e00\u4e2a\u751f\u6210\u591a\u8868\u65f6\u95f4\u5e8f\u5217\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u7684\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u63a5\u8fd1\u539f\u59cbEHR\u7684\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u65b0\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u4e86\u5176\u9ad8\u4fdd\u771f\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u548c\u6cd5\u89c4\u9650\u5236\uff0cEHR\u6570\u636e\u7684\u5171\u4eab\u548c\u5229\u7528\u53d7\u9650\uff0c\u9700\u8981\u751f\u6210\u5408\u6210\u6570\u636e\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u751f\u6210\u4e13\u5bb6\u9009\u62e9\u7684\u7279\u5f81\uff0c\u65e0\u6cd5\u6355\u6349\u539f\u59cbEHR\u7684\u590d\u6742\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\u3002", "method": "RawMed\u91c7\u7528\u57fa\u4e8e\u6587\u672c\u7684\u8868\u793a\u548c\u538b\u7f29\u6280\u672f\uff0c\u4ee5\u6700\u5c0f\u9884\u5904\u7406\u6355\u6349\u590d\u6742\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u8868\u65f6\u95f4\u5e8f\u5217\u5408\u6210EHR\u7684\u65b0\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5728\u5f00\u6e90EHR\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cRawMed\u5728\u4fdd\u771f\u5ea6\u548c\u5b9e\u7528\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "RawMed\u4e3a\u5408\u6210EHR\u6570\u636e\u63d0\u4f9b\u4e86\u66f4\u63a5\u8fd1\u539f\u59cb\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u6f5c\u5728\u7684\u7814\u7a76\u548c\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.07008", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07008", "abs": "https://arxiv.org/abs/2507.07008", "authors": ["Emile Pierret", "Bruno Galerne"], "title": "Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions", "comment": null, "summary": "Used as priors for Bayesian inverse problems, diffusion models have recently\nattracted considerable attention in the literature. Their flexibility and high\nvariance enable them to generate multiple solutions for a given task, such as\ninpainting, super-resolution, and deblurring. However, several unresolved\nquestions remain about how well they perform. In this article, we investigate\nthe accuracy of these models when applied to a Gaussian data distribution for\ndeblurring. Within this constrained context, we are able to precisely analyze\nthe discrepancy between the theoretical resolution of inverse problems and\ntheir resolution obtained using diffusion models by computing the exact\nWasserstein distance between the distribution of the diffusion model sampler\nand the ideal distribution of solutions to the inverse problem. Our findings\nallow for the comparison of different algorithms from the literature.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u5728\u9ad8\u65af\u6570\u636e\u5206\u5e03\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u8ba1\u7b97Wasserstein\u8ddd\u79bb\u6bd4\u8f83\u7406\u8bba\u89e3\u4e0e\u5b9e\u9645\u89e3\u7684\u5dee\u5f02\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u7684\u5148\u9a8c\uff0c\u5177\u6709\u7075\u6d3b\u6027\u548c\u9ad8\u65b9\u5dee\uff0c\u4f46\u5176\u6027\u80fd\u5c1a\u672a\u5b8c\u5168\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30\u5176\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3002", "method": "\u5728\u53d7\u9650\u7684\u9ad8\u65af\u6570\u636e\u5206\u5e03\u80cc\u666f\u4e0b\uff0c\u8ba1\u7b97\u6269\u6563\u6a21\u578b\u91c7\u6837\u5668\u4e0e\u7406\u60f3\u89e3\u5206\u5e03\u4e4b\u95f4\u7684Wasserstein\u8ddd\u79bb\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u7406\u8bba\u89e3\u4e0e\u5b9e\u9645\u89e3\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u5141\u8bb8\u6bd4\u8f83\u4e0d\u540c\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u4e3a\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u7b97\u6cd5\u6bd4\u8f83\u548c\u6539\u8fdb\u3002"}}
{"id": "2507.07016", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07016", "abs": "https://arxiv.org/abs/2507.07016", "authors": ["Jian Huang", "Yongli Zhu", "Linna Xu", "Zhe Zheng", "Wenpeng Cui", "Mingyang Sun"], "title": "On-Device Training of PV Power Forecasting Models in a Smart Meter for Grid Edge Intelligence", "comment": "This paper is currently under reviewing by an IEEE publication; it\n  may be subjected to minor changes due to review comments later", "summary": "In this paper, an edge-side model training study is conducted on a\nresource-limited smart meter. The motivation of grid-edge intelligence and the\nconcept of on-device training are introduced. Then, the technical preparation\nsteps for on-device training are described. A case study on the task of\nphotovoltaic power forecasting is presented, where two representative machine\nlearning models are investigated: a gradient boosting tree model and a\nrecurrent neural network model. To adapt to the resource-limited situation in\nthe smart meter, \"mixed\"- and \"reduced\"-precision training schemes are also\ndevised. Experiment results demonstrate the feasibility of economically\nachieving grid-edge intelligence via the existing advanced metering\ninfrastructures.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u667a\u80fd\u7535\u8868\u4e0a\u8fdb\u884c\u8fb9\u7f18\u4fa7\u6a21\u578b\u8bad\u7ec3\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u6df7\u5408\u548c\u964d\u4f4e\u7cbe\u5ea6\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5149\u4f0f\u529f\u7387\u9884\u6d4b\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u7ecf\u6d4e\u6027\u3002", "motivation": "\u63a8\u52a8\u7535\u7f51\u8fb9\u7f18\u667a\u80fd\u5316\u548c\u8bbe\u5907\u7aef\u8bad\u7ec3\u7684\u6982\u5ff5\uff0c\u4ee5\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6a21\u578b\u8bad\u7ec3\u95ee\u9898\u3002", "method": "\u4ecb\u7ecd\u4e86\u8bbe\u5907\u7aef\u8bad\u7ec3\u7684\u6280\u672f\u51c6\u5907\u6b65\u9aa4\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6df7\u5408\u548c\u964d\u4f4e\u7cbe\u5ea6\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u5e94\u7528\u4e8e\u68af\u5ea6\u63d0\u5347\u6811\u6a21\u578b\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u73b0\u6709\u9ad8\u7ea7\u8ba1\u91cf\u57fa\u7840\u8bbe\u65bd\uff0c\u7ecf\u6d4e\u5730\u5b9e\u73b0\u7535\u7f51\u8fb9\u7f18\u667a\u80fd\u662f\u53ef\u884c\u7684\u3002", "conclusion": "\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u8fb9\u7f18\u4fa7\u6a21\u578b\u8bad\u7ec3\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u7535\u7f51\u8fb9\u7f18\u667a\u80fd\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07032", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.07032", "abs": "https://arxiv.org/abs/2507.07032", "authors": ["Hanqun Cao", "Xinyi Zhou", "Zijun Gao", "Chenyu Wang", "Xin Gao", "Zhi Zhang", "Chunbin Gu", "Ge Liu", "Pheng-Ann Heng"], "title": "PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments", "comment": null, "summary": "Protein structure prediction is essential for drug discovery and\nunderstanding biological functions. While recent advancements like AlphaFold\nhave achieved remarkable accuracy, most folding models rely heavily on multiple\nsequence alignments (MSAs) to boost prediction performance. This dependency\nlimits their effectiveness on low-homology proteins and orphan proteins, where\nMSA information is sparse or unavailable. To address this limitation, we\npropose PLAME, a novel MSA design model that leverages evolutionary embeddings\nfrom pretrained protein language models. Unlike existing methods, PLAME\nintroduces pretrained representations to enhance evolutionary information and\nemploys a conservation-diversity loss to enhance generation quality.\nAdditionally, we propose a novel MSA selection method to effectively screen\nhigh-quality MSAs and improve folding performance. We also propose a sequence\nquality assessment metric that provides an orthogonal perspective to evaluate\nMSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,\nPLAME achieves state-of-the-art performance in folding enhancement and sequence\nquality assessment, with consistent improvements demonstrated on AlphaFold3.\nAblation studies validate the effectiveness of the MSA selection method, while\nextensive case studies on various protein types provide insights into the\nrelationship between AlphaFold's prediction quality and MSA characteristics.\nFurthermore, we demonstrate that PLAME can serve as an adapter achieving\nAlphaFold2-level accuracy with the ESMFold's inference speed.", "AI": {"tldr": "PLAME\u662f\u4e00\u79cd\u65b0\u578b\u7684MSA\u8bbe\u8ba1\u6a21\u578b\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5316\u5d4c\u5165\uff0c\u63d0\u5347\u4f4e\u540c\u6e90\u6027\u548c\u5b64\u513f\u86cb\u767d\u8d28\u7684\u7ed3\u6784\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6298\u53e0\u6a21\u578b\u4f9d\u8d56\u591a\u5e8f\u5217\u6bd4\u5bf9\uff08MSA\uff09\uff0c\u4f46\u5728\u4f4e\u540c\u6e90\u6027\u548c\u5b64\u513f\u86cb\u767d\u8d28\u4e0a\u6548\u679c\u6709\u9650\uff0cPLAME\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "PLAME\u7ed3\u5408\u9884\u8bad\u7ec3\u8868\u793a\u548c\u5b88\u6052-\u591a\u6837\u6027\u635f\u5931\uff0c\u63d0\u51fa\u65b0\u7684MSA\u7b5b\u9009\u65b9\u6cd5\u548c\u5e8f\u5217\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728AlphaFold2\u548cAlphaFold3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPLAME\u5728\u6298\u53e0\u589e\u5f3a\u548c\u5e8f\u5217\u8d28\u91cf\u8bc4\u4f30\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "PLAME\u4e0d\u4ec5\u63d0\u5347\u9884\u6d4b\u8d28\u91cf\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u9002\u914d\u5668\u7ed3\u5408AlphaFold2\u7684\u51c6\u786e\u6027\u548cESMFold\u7684\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2507.07033", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07033", "abs": "https://arxiv.org/abs/2507.07033", "authors": ["Roberto Pereira", "Fernanda Fam\u00e1", "Asal Rangrazi", "Marco Miozzo", "Charalampos Kalalas", "Paolo Dini"], "title": "Self-Supervised Learning at the Edge: The Cost of Labeling", "comment": "Accepted for publication in IEEE MLSP 2025", "summary": "Contrastive learning (CL) has recently emerged as an alternative to\ntraditional supervised machine learning solutions by enabling rich\nrepresentations from unstructured and unlabeled data. However, CL and, more\nbroadly, self-supervised learning (SSL) methods often demand a large amount of\ndata and computational resources, posing challenges for deployment on\nresource-constrained edge devices. In this work, we explore the feasibility and\nefficiency of SSL techniques for edge-based learning, focusing on trade-offs\nbetween model performance and energy efficiency. In particular, we analyze how\ndifferent SSL techniques adapt to limited computational, data, and energy\nbudgets, evaluating their effectiveness in learning robust representations\nunder resource-constrained settings. Moreover, we also consider the energy\ncosts involved in labeling data and assess how semi-supervised learning may\nassist in reducing the overall energy consumed to train CL models. Through\nextensive experiments, we demonstrate that tailored SSL strategies can achieve\ncompetitive performance while reducing resource consumption by up to 4X,\nunderscoring their potential for energy-efficient learning at the edge.", "AI": {"tldr": "\u5bf9\u6bd4\u5b66\u4e60\uff08CL\uff09\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u9762\u4e34\u6570\u636e\u4e0e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u7684\u6311\u6218\u3002\u672c\u6587\u63a2\u8ba8\u4e86SSL\u5728\u8fb9\u7f18\u5b66\u4e60\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6548\u7387\uff0c\u5206\u6790\u4e86\u4e0d\u540cSSL\u6280\u672f\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u534a\u76d1\u7763\u5b66\u4e60\u53ef\u964d\u4f4e\u8bad\u7ec3CL\u6a21\u578b\u7684\u80fd\u8017\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5b9a\u5236\u5316SSL\u7b56\u7565\u80fd\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u8fbe4\u500d\u3002", "motivation": "\u4f20\u7edfCL\u548cSSL\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22SSL\u5728\u8fb9\u7f18\u5b66\u4e60\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4f18\u5316\u8d44\u6e90\u5229\u7528\u3002", "method": "\u5206\u6790\u4e0d\u540cSSL\u6280\u672f\u5728\u6709\u9650\u8ba1\u7b97\u3001\u6570\u636e\u548c\u80fd\u6e90\u9884\u7b97\u4e0b\u7684\u9002\u5e94\u6027\uff0c\u8bc4\u4f30\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b66\u4e60\u9c81\u68d2\u8868\u793a\u7684\u6548\u679c\uff0c\u5e76\u63a2\u8ba8\u534a\u76d1\u7763\u5b66\u4e60\u5bf9\u964d\u4f4e\u80fd\u8017\u7684\u4f5c\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5b9a\u5236\u5316SSL\u7b56\u7565\u80fd\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u8fbe4\u500d\u3002", "conclusion": "SSL\u6280\u672f\u5728\u8fb9\u7f18\u5b66\u4e60\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u53ef\u884c\u6027\uff0c\u5b9a\u5236\u5316\u7b56\u7565\u53ef\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u9700\u6c42\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002"}}
{"id": "2507.07061", "categories": ["cs.LG", "68T50", "I.2.7; H.3.3; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.07061", "abs": "https://arxiv.org/abs/2507.07061", "authors": ["Shervin Ghaffari", "Zohre Bahranifard", "Mohammad Akbari"], "title": "An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems", "comment": "10 pages, 8 figures, 2 table. Submitted to the Journal of Information\n  Science", "summary": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5d4c\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u5d4c\u5165\u6a21\u578b\u548c\u8bad\u7ec3\u5143\u7f16\u7801\u5668\uff0c\u63d0\u5347LLM\u7f13\u5b58\u7cfb\u7edf\u4e2d\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u68c0\u6d4b\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u7b49\u6548\u67e5\u8be2\u4e2d\u5b9e\u73b0\u4e8692%\u7684\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u540c\u65f6\u4fdd\u630185%\u7684\u51c6\u786e\u7387\u62d2\u7edd\u975e\u7b49\u6548\u67e5\u8be2\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u7f13\u5b58\u6846\u67b6\u4f9d\u8d56\u5355\u4e00\u5d4c\u5165\u6a21\u578b\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u771f\u5b9e\u67e5\u8be2\u5206\u5e03\u4e2d\u7684\u591a\u6837\u8bed\u4e49\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u7f13\u5b58\u6548\u7387\u3002", "method": "\u91c7\u7528\u96c6\u6210\u5d4c\u5165\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u4e2a\u5d4c\u5165\u6a21\u578b\u5e76\u901a\u8fc7\u8bad\u7ec3\u5143\u7f16\u7801\u5668\u4f18\u5316\u8bed\u4e49\u76f8\u4f3c\u6027\u68c0\u6d4b\u3002", "result": "\u5728QQP\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u73b0\u4e8692%\u7684\u7f13\u5b58\u547d\u4e2d\u7387\u548c85%\u7684\u975e\u7b49\u6548\u67e5\u8be2\u62d2\u7edd\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u65b9\u6cd5\u3002", "conclusion": "\u96c6\u6210\u5d4c\u5165\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u533a\u5206\u8bed\u4e49\u76f8\u4f3c\u4e0e\u4e0d\u76f8\u4f3c\u67e5\u8be2\uff0c\u63d0\u5347LLM\u7f13\u5b58\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.07100", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07100", "abs": "https://arxiv.org/abs/2507.07100", "authors": ["Lan Li", "Da-Wei Zhou", "Han-Jia Ye", "De-Chuan Zhan"], "title": "Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts", "comment": "Accepted by ICML 2025", "summary": "Domain-Incremental Learning (DIL) focuses on continual learning in\nnon-stationary environments, requiring models to adjust to evolving domains\nwhile preserving historical knowledge. DIL faces two critical challenges in the\ncontext of imbalanced data: intra-domain class imbalance and cross-domain class\ndistribution shifts. These challenges significantly hinder model performance,\nas intra-domain imbalance leads to underfitting of few-shot classes, while\ncross-domain shifts require maintaining well-learned many-shot classes and\ntransferring knowledge to improve few-shot class performance in old domains. To\novercome these challenges, we introduce the Dual-Balance Collaborative Experts\n(DCE) framework. DCE employs a frequency-aware expert group, where each expert\nis guided by specialized loss functions to learn features for specific\nfrequency groups, effectively addressing intra-domain class imbalance.\nSubsequently, a dynamic expert selector is learned by synthesizing\npseudo-features through balanced Gaussian sampling from historical class\nstatistics. This mechanism navigates the trade-off between preserving many-shot\nknowledge of previous domains and leveraging new data to improve few-shot class\nperformance in earlier tasks. Extensive experimental results on four benchmark\ndatasets demonstrate DCE's state-of-the-art performance.", "AI": {"tldr": "DCE\u6846\u67b6\u901a\u8fc7\u9891\u7387\u611f\u77e5\u4e13\u5bb6\u7ec4\u548c\u52a8\u6001\u4e13\u5bb6\u9009\u62e9\u5668\u89e3\u51b3DIL\u4e2d\u7684\u7c7b\u4e0d\u5e73\u8861\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3DIL\u4e2d\u56e0\u7c7b\u4e0d\u5e73\u8861\u548c\u8de8\u57df\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u7684\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9891\u7387\u611f\u77e5\u4e13\u5bb6\u7ec4\u5b66\u4e60\u7279\u5b9a\u9891\u7387\u7c7b\u522b\u7684\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u9009\u62e9\u5668\u5e73\u8861\u65b0\u65e7\u57df\u77e5\u8bc6\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86DCE\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "DCE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86DIL\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.07101", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07101", "abs": "https://arxiv.org/abs/2507.07101", "authors": ["Martin Marek", "Sanae Lotfi", "Aditya Somasundaram", "Andrew Gordon Wilson", "Micah Goldblum"], "title": "Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful", "comment": "Code available at: https://github.com/martin-marek/batch-size", "summary": "Conventional wisdom dictates that small batch sizes make language model\npretraining and fine-tuning unstable, motivating gradient accumulation, which\ntrades off the number of optimizer steps for a proportional increase in batch\nsize. While it is common to decrease the learning rate for smaller batch sizes,\nother hyperparameters are often held fixed. In this work, we revisit small\nbatch sizes all the way down to batch size one, and we propose a rule for\nscaling Adam hyperparameters to small batch sizes. We find that small batch\nsizes (1) train stably, (2) are consistently more robust to hyperparameter\nchoices, (3) achieve equal or better per-FLOP performance than larger batch\nsizes, and (4) notably enable stable language model training with vanilla SGD,\neven without momentum, despite storing no optimizer state. Building on these\nresults, we provide practical recommendations for selecting a batch size and\nsetting optimizer hyperparameters. We further recommend against gradient\naccumulation unless training on multiple devices with multiple model replicas,\nbottlenecked by inter-device bandwidth.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5c0f\u6279\u91cf\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u66f4\u7a33\u5b9a\u4e14\u9ad8\u6548\uff0c\u63d0\u51faAdam\u8d85\u53c2\u6570\u8c03\u6574\u89c4\u5219\uff0c\u5e76\u5efa\u8bae\u907f\u514d\u68af\u5ea6\u7d2f\u79ef\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u5c0f\u6279\u91cf\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u9700\u68af\u5ea6\u7d2f\u79ef\uff0c\u4f46\u672c\u7814\u7a76\u91cd\u65b0\u5ba1\u89c6\u5c0f\u6279\u91cf\u8bad\u7ec3\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faAdam\u8d85\u53c2\u6570\u8c03\u6574\u89c4\u5219\uff0c\u6d4b\u8bd5\u5c0f\u6279\u91cf\uff08\u751a\u81f3\u6279\u91cf\u5927\u5c0f\u4e3a1\uff09\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u4e0e\u6027\u80fd\u3002", "result": "\u5c0f\u6279\u91cf\u8bad\u7ec3\u66f4\u7a33\u5b9a\u3001\u5bf9\u8d85\u53c2\u6570\u66f4\u9c81\u68d2\u3001\u6027\u80fd\u4e0d\u900a\u4e8e\u5927\u6279\u91cf\uff0c\u4e14\u652f\u6301\u666e\u901aSGD\u8bad\u7ec3\u3002", "conclusion": "\u5efa\u8bae\u9009\u62e9\u5c0f\u6279\u91cf\u8bad\u7ec3\u5e76\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u907f\u514d\u68af\u5ea6\u7d2f\u79ef\uff0c\u9664\u975e\u591a\u8bbe\u5907\u8bad\u7ec3\u5e26\u5bbd\u53d7\u9650\u3002"}}
{"id": "2507.07102", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07102", "abs": "https://arxiv.org/abs/2507.07102", "authors": ["Arnas Uselis", "Andrea Dittadi", "Seong Joon Oh"], "title": "Does Data Scaling Lead to Visual Compositional Generalization?", "comment": "ICML 2025", "summary": "Compositional understanding is crucial for human intelligence, yet it remains\nunclear whether contemporary vision models exhibit it. The dominant machine\nlearning paradigm is built on the premise that scaling data and model sizes\nwill improve out-of-distribution performance, including compositional\ngeneralization. We test this premise through controlled experiments that\nsystematically vary data scale, concept diversity, and combination coverage. We\nfind that compositional generalization is driven by data diversity, not mere\ndata scale. Increased combinatorial coverage forces models to discover a\nlinearly factored representational structure, where concepts decompose into\nadditive components. We prove this structure is key to efficiency, enabling\nperfect generalization from few observed combinations. Evaluating pretrained\nmodels (DINO, CLIP), we find above-random yet imperfect performance, suggesting\npartial presence of this structure. Our work motivates stronger emphasis on\nconstructing diverse datasets for compositional generalization, and considering\nthe importance of representational structure that enables efficient\ncompositional learning. Code available at\nhttps://github.com/oshapio/visual-compositional-generalization.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u7531\u6570\u636e\u591a\u6837\u6027\u800c\u975e\u6570\u636e\u89c4\u6a21\u9a71\u52a8\uff0c\u7ebf\u6027\u5206\u89e3\u8868\u793a\u7ed3\u6784\u662f\u5173\u952e\u3002", "motivation": "\u63a2\u8ba8\u5f53\u4ee3\u89c6\u89c9\u6a21\u578b\u662f\u5426\u5177\u5907\u7ec4\u5408\u7406\u89e3\u80fd\u529b\uff0c\u4ee5\u53ca\u6570\u636e\u89c4\u6a21\u548c\u591a\u6837\u6027\u5bf9\u7ec4\u5408\u6cdb\u5316\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u53d8\u5316\u6570\u636e\u89c4\u6a21\u3001\u6982\u5ff5\u591a\u6837\u6027\u548c\u7ec4\u5408\u8986\u76d6\u7387\uff0c\u8bc4\u4f30\u9884\u8bad\u7ec3\u6a21\u578b\uff08DINO\u3001CLIP\uff09\u3002", "result": "\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u4f9d\u8d56\u4e8e\u6570\u636e\u591a\u6837\u6027\uff0c\u7ebf\u6027\u5206\u89e3\u8868\u793a\u7ed3\u6784\u80fd\u5b9e\u73b0\u9ad8\u6548\u6cdb\u5316\u3002\u9884\u8bad\u7ec3\u6a21\u578b\u8868\u73b0\u9ad8\u4e8e\u968f\u673a\u4f46\u4e0d\u5b8c\u7f8e\u3002", "conclusion": "\u5f3a\u8c03\u6784\u5efa\u591a\u6837\u5316\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u9ad8\u6548\u7ec4\u5408\u5b66\u4e60\u6240\u9700\u7684\u8868\u793a\u7ed3\u6784\u3002"}}
