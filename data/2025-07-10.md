<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 21]
- [cs.LG](#cs.LG) [Total: 72]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Solving the Constrained Random Disambiguation Path Problem via Lagrangian Relaxation and Graph Reduction](https://arxiv.org/abs/2507.06346)
*Li Zhou,Elvan Ceyhan*

Main category: cs.RO

TL;DR: 论文研究了资源受限的随机消歧路径（RDP）问题，提出了一种结合拉格朗日松弛和两阶段顶点消除（TPVE）的新算法框架COLOGR，解决了带权约束的最短路径问题（WCSPP），并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决在不确定障碍物环境中导航的资源受限路径规划问题，优化消歧成本与路径风险。

Method: 提出COLOGR框架，结合拉格朗日松弛和TPVE，剪枝不可行和次优路径，利用对偶界指导搜索。

Result: COLOGR在实验中表现优于贪心基线，接近离线最优基准，且计算复杂度更低。

Conclusion: COLOGR适用于随机网络设计、移动规划和不确定性下的受限决策问题。

Abstract: We study a resource-constrained variant of the Random Disambiguation Path
(RDP) problem, a generalization of the Stochastic Obstacle Scene (SOS) problem,
in which a navigating agent must reach a target in a spatial environment
populated with uncertain obstacles. Each ambiguous obstacle may be
disambiguated at a (possibly) heterogeneous resource cost, subject to a global
disambiguation budget. We formulate this constrained planning problem as a
Weight-Constrained Shortest Path Problem (WCSPP) with risk-adjusted edge costs
that incorporate probabilistic blockage and traversal penalties. To solve it,
we propose a novel algorithmic framework-COLOGR-combining Lagrangian relaxation
with a two-phase vertex elimination (TPVE) procedure. The method prunes
infeasible and suboptimal paths while provably preserving the optimal solution,
and leverages dual bounds to guide efficient search. We establish correctness,
feasibility guarantees, and surrogate optimality under mild assumptions. Our
analysis also demonstrates that COLOGR frequently achieves zero duality gap and
offers improved computational complexity over prior constrained path-planning
methods. Extensive simulation experiments validate the algorithm's robustness
across varying obstacle densities, sensor accuracies, and risk models,
consistently outperforming greedy baselines and approaching offline-optimal
benchmarks. The proposed framework is broadly applicable to stochastic network
design, mobility planning, and constrained decision-making under uncertainty.

</details>


### [2] [Mapping the Catacombs: An Underwater Cave Segment of the Devil's Eye System](https://arxiv.org/abs/2507.06397)
*Michalis Chatzispyrou,Luke Horgan,Hyunkil Hwang,Harish Sathishchandra,Monika Roznere,Alberto Quattrini Li,Philippos Mordohai,Ioannis Rekleitis*

Main category: cs.RO

TL;DR: 提出了一种利用低成本运动相机和潜水电脑绘制水下洞穴地图的框架，结合视觉/惯性框架和全局优化技术生成洞穴的一维轮廓和密集3D重建。


<details>
  <summary>Details</summary>
Motivation: 水下洞穴对淡水资源管理、水下考古和水文地质学至关重要，绘制其轮廓和尺寸以及生成逼真的3D地图有助于更好地理解这一领域。

Method: 使用低成本运动相机和潜水电脑估计相机轨迹和稀疏点云，结合SVIn2框架和COLMAP全局优化技术生成一维洞穴轮廓和密集3D重建。

Result: 成功绘制了Devil's Eye洞穴系统的一段，并通过手动测量验证了方法的有效性，生成了洞穴的一维轮廓和部分区域的密集3D重建。

Conclusion: 该方法利用低成本设备实现了水下洞穴地图的主要组成部分，并通过全局优化技术生成了逼真的3D重建，为相关领域提供了实用工具。

Abstract: This paper presents a framework for mapping underwater caves. Underwater
caves are crucial for fresh water resource management, underwater archaeology,
and hydrogeology. Mapping the cave's outline and dimensions, as well as
creating photorealistic 3D maps, is critical for enabling a better
understanding of this underwater domain. In this paper, we present the mapping
of an underwater cave segment (the catacombs) of the Devil's Eye cave system at
Ginnie Springs, FL. We utilized a set of inexpensive action cameras in
conjunction with a dive computer to estimate the trajectories of the cameras
together with a sparse point cloud. The resulting reconstructions are utilized
to produce a one-dimensional retract of the cave passages in the form of the
average trajectory together with the boundaries (top, bottom, left, and right).
The use of the dive computer enables the observability of the z-dimension in
addition to the roll and pitch in a visual/inertial framework (SVIn2). In
addition, the keyframes generated by SVIn2 together with the estimated camera
poses for select areas are used as input to a global optimization (bundle
adjustment) framework -- COLMAP -- in order to produce a dense reconstruction
of those areas. The same cave segment is manually surveyed using the MNemo V2
instrument, providing an additional set of measurements validating the proposed
approach. It is worth noting that with the use of action cameras, the primary
components of a cave map can be constructed. Furthermore, with the utilization
of a global optimization framework guided by the results of VI-SLAM package
SVIn2, photorealistic dense 3D representations of selected areas can be
reconstructed.

</details>


### [3] [Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction](https://arxiv.org/abs/2507.06404)
*Matteo Tiezzi,Tommaso Apicella,Carlos Cardenas-Perez,Giovanni Fregonese,Stefano Dafarra,Pietro Morerio,Daniele Pucci,Alessio Del Bue*

Main category: cs.RO

TL;DR: 提出了一种基于轨迹性能的通用评估框架NeME，用于比较模仿学习方法在复杂人机交互任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 评估人形机器人性能具有挑战性，传统成功率指标难以复现且无法捕捉轨迹复杂性。

Method: 设计了NeME（神经元评估器），通过深度学习模型从关节轨迹中分类动作，作为无人类参与的元评估器。

Result: 在ergoCub机器人上验证，NeME比基线方法更接近实际成功率，提供可复现且系统的评估手段。

Conclusion: NeME为复杂人机交互任务中的模仿学习方法提供了更可靠的性能比较工具。

Abstract: Evaluating and comparing the performance of autonomous Humanoid Robots is
challenging, as success rate metrics are difficult to reproduce and fail to
capture the complexity of robot movement trajectories, critical in Human-Robot
Interaction and Collaboration (HRIC). To address these challenges, we propose a
general evaluation framework that measures the quality of Imitation Learning
(IL) methods by focusing on trajectory performance. We devise the Neural Meta
Evaluator (NeME), a deep learning model trained to classify actions from robot
joint trajectories. NeME serves as a meta-evaluator to compare the performance
of robot control policies, enabling policy evaluation without requiring human
involvement in the loop. We validate our framework on ergoCub, a humanoid
robot, using teleoperation data and comparing IL methods tailored to the
available platform. The experimental results indicate that our method is more
aligned with the success rate obtained on the robot than baselines, offering a
reproducible, systematic, and insightful means for comparing the performance of
multimodal imitation learning approaches in complex HRI tasks.

</details>


### [4] [Evaluating Robots Like Human Infants: A Case Study of Learned Bipedal Locomotion](https://arxiv.org/abs/2507.06426)
*Devin Crowley,Whitney G. Cole,Christina M. Hospodar,Ruiting Shen,Karen E. Adolph,Alan Fern*

Main category: cs.RO

TL;DR: 论文提出了一种将发展心理学方法应用于机器人行为学习的研究，通过系统设计训练方案，揭示了训练对机器人行为发展的影响。


<details>
  <summary>Details</summary>
Motivation: 传统机器人控制器训练方法缺乏系统性，且评估指标粗糙，无法深入理解训练方案对行为的影响。发展心理学对婴儿行为的研究方法精细，但受限于实际约束。

Method: 采用发展心理学方法，系统设计强化学习训练方案，并在模拟环境中测试双足机器人Cassie的行为。

Result: 研究揭示了不同训练方案对机器人行为的影响，并与婴儿学步行为进行了对比。

Conclusion: 这种跨学科方法为未来系统研究复杂机器人行为发展提供了新思路。

Abstract: Typically, learned robot controllers are trained via relatively unsystematic
regimens and evaluated with coarse-grained outcome measures such as average
cumulative reward. The typical approach is useful to compare learning
algorithms but provides limited insight into the effects of different training
regimens and little understanding about the richness and complexity of learned
behaviors. Likewise, human infants and other animals are "trained" via
unsystematic regimens, but in contrast, developmental psychologists evaluate
their performance in highly-controlled experiments with fine-grained measures
such as success, speed of walking, and prospective adjustments. However, the
study of learned behavior in human infants is limited by the practical
constraints of training and testing babies. Here, we present a case study that
applies methods from developmental psychology to study the learned behavior of
the simulated bipedal robot Cassie. Following research on infant walking, we
systematically designed reinforcement learning training regimens and tested the
resulting controllers in simulated environments analogous to those used for
babies--but without the practical constraints. Results reveal new insights into
the behavioral impact of different training regimens and the development of
Cassie's learned behaviors relative to infants who are learning to walk. This
interdisciplinary baby-robot approach provides inspiration for future research
designed to systematically test effects of training on the development of
complex learned robot behaviors.

</details>


### [5] [Failure Forecasting Boosts Robustness of Sim2Real Rhythmic Insertion Policies](https://arxiv.org/abs/2507.06519)
*Yuhan Liu,Xinyu Zhang,Haonan Chang,Abdeslam Boularias*

Main category: cs.RO

TL;DR: 提出了一种结合强化学习插入策略和失败预测模块的sim-to-real框架，用于解决高精度重复插入任务（RIT）的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决RIT任务中毫米级精度和长时间重复性能保持的难题，特别是螺母旋转和摩擦带来的复杂性。

Method: 使用强化学习训练插入策略，结合6D位姿跟踪和失败预测模块，通过螺母坐标系表示扳手位姿提升sim-to-real迁移性。

Result: 在仿真和实际环境中均实现了高一次性成功率和长时间重复任务的稳健性能。

Conclusion: 该方法有效提升了RIT任务的精度和鲁棒性，适用于实际工业应用。

Abstract: This paper addresses the challenges of Rhythmic Insertion Tasks (RIT), where
a robot must repeatedly perform high-precision insertions, such as screwing a
nut into a bolt with a wrench. The inherent difficulty of RIT lies in achieving
millimeter-level accuracy and maintaining consistent performance over multiple
repetitions, particularly when factors like nut rotation and friction introduce
additional complexity. We propose a sim-to-real framework that integrates a
reinforcement learning-based insertion policy with a failure forecasting
module. By representing the wrench's pose in the nut's coordinate frame rather
than the robot's frame, our approach significantly enhances sim-to-real
transferability. The insertion policy, trained in simulation, leverages
real-time 6D pose tracking to execute precise alignment, insertion, and
rotation maneuvers. Simultaneously, a neural network predicts potential
execution failures, triggering a simple recovery mechanism that lifts the
wrench and retries the insertion. Extensive experiments in both simulated and
real-world environments demonstrate that our method not only achieves a high
one-time success rate but also robustly maintains performance over long-horizon
repetitive tasks.

</details>


### [6] [KLEIYN : A Quadruped Robot with an Active Waist for Both Locomotion and Wall Climbing](https://arxiv.org/abs/2507.06562)
*Keita Yoneda,Kento Kawaharazuka,Temma Suzuki,Takahiro Hattori,Kei Okada*

Main category: cs.RO

TL;DR: 研究开发了具有腰部关节的四足机器人KLEIYN，通过强化学习实现垂直运动（如烟囱攀爬），并引入接触引导课程学习（CGCL）方法，显著提升了攀爬速度和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管四足机器人在平坦地形上的运动控制已取得进展，但在具有显著高度变化的崎岖地形中稳定垂直运动的机器人及控制方法尚未成熟。

Method: 开发了具有腰部关节的四足机器人KLEIYN，采用强化学习（RL）和接触引导课程学习（CGCL）方法，训练机器人进行垂直运动（如烟囱攀爬）。

Result: KLEIYN成功攀爬宽度为800至1000毫米的墙壁，平均速度为150毫米/秒，比传统机器人快50倍；腰部关节的引入进一步提升了在狭窄墙壁上的跟踪能力。

Conclusion: 研究表明，腰部关节和CGCL方法显著提升了四足机器人在垂直运动中的性能，为复杂地形下的自动化任务提供了新可能。

Abstract: In recent years, advancements in hardware have enabled quadruped robots to
operate with high power and speed, while robust locomotion control using
reinforcement learning (RL) has also been realized. As a result, expectations
are rising for the automation of tasks such as material transport and
exploration in unknown environments. However, autonomous locomotion in rough
terrains with significant height variations requires vertical movement, and
robots capable of performing such movements stably, along with their control
methods, have not yet been fully established. In this study, we developed the
quadruped robot KLEIYN, which features a waist joint, and aimed to expand
quadruped locomotion by enabling chimney climbing through RL. To facilitate the
learning of vertical motion, we introduced Contact-Guided Curriculum Learning
(CGCL). As a result, KLEIYN successfully climbed walls ranging from 800 mm to
1000 mm in width at an average speed of 150 mm/s, 50 times faster than
conventional robots. Furthermore, we demonstrated that the introduction of a
waist joint improves climbing performance, particularly enhancing tracking
ability on narrow walls.

</details>


### [7] [SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments](https://arxiv.org/abs/2507.06564)
*Tianshun Li,Tianyi Huai,Zhen Li,Yichun Gao,Haoang Li,Xinhu Zheng*

Main category: cs.RO

TL;DR: SkyVLN框架结合视觉语言导航（VLN）和非线性模型预测控制（NMPC），提升无人机在复杂城市环境中的自主导航能力。


<details>
  <summary>Details</summary>
Motivation: 传统导航方法在动态3D环境中表现不足，需要更智能的解决方案。

Method: 结合大型语言模型（LLMs）解析自然语言指令和视觉观察，配备空间语言化器和历史路径记忆机制，并集成NMPC模块进行动态避障。

Result: 在AirSim高保真3D模拟环境中验证，SkyVLN显著提高了导航成功率和效率。

Conclusion: SkyVLN为无人机在复杂环境中的自主导航提供了高效、鲁棒的解决方案。

Abstract: Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across
various sectors, driven by their mobility and adaptability. This paper
introduces SkyVLN, a novel framework integrating vision-and-language navigation
(VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in
complex urban environments. Unlike traditional navigation methods, SkyVLN
leverages Large Language Models (LLMs) to interpret natural language
instructions and visual observations, enabling UAVs to navigate through dynamic
3D spaces with improved accuracy and robustness. We present a multimodal
navigation agent equipped with a fine-grained spatial verbalizer and a history
path memory mechanism. These components allow the UAV to disambiguate spatial
contexts, handle ambiguous instructions, and backtrack when necessary. The
framework also incorporates an NMPC module for dynamic obstacle avoidance,
ensuring precise trajectory tracking and collision prevention. To validate our
approach, we developed a high-fidelity 3D urban simulation environment using
AirSim, featuring realistic imagery and dynamic urban elements. Extensive
experiments demonstrate that SkyVLN significantly improves navigation success
rates and efficiency, particularly in new and unseen environments.

</details>


### [8] [AI Space Cortex: An Experimental System for Future Era Space Exploration](https://arxiv.org/abs/2507.06574)
*Thomas Touma,Ersin Daş,Erica Tevere,Martin Feather,Ksenia Kolcio,Maurice Prather,Alberto Candela,Ashish Goel,Erik Kramer,Hari Nayar,Lorraine Fesq,Joel W. Burdick*

Main category: cs.RO

TL;DR: REASIMO项目为NASA的COLDTech计划开发AI辅助自主系统，用于解决海洋世界任务中的通信延迟、能源限制和辐射问题，通过预训练行为和异常检测提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 海洋世界任务（如欧罗巴和恩塞拉达斯）面临通信延迟、能源限制和恶劣环境等挑战，传统安全模式无法满足需求，需要更智能的自主系统。

Method: 结合AI技术和预训练行为，开发了一个智能框架，用于异常检测和恢复，并在NASA喷气推进实验室的测试平台上验证自主采样操作。

Result: 成功展示了AI辅助自主系统在模拟海洋世界表面条件下的操作能力，验证了其异常处理和行为执行的可行性。

Conclusion: REASIMO框架为未来海洋世界任务提供了高效、自主的解决方案，显著提升了任务完成率和科学目标达成能力。

Abstract: Our Robust, Explainable Autonomy for Scientific Icy Moon Operations (REASIMO)
effort contributes to NASA's Concepts for Ocean worlds Life Detection
Technology (COLDTech) program, which explores science platform technologies for
ocean worlds such as Europa and Enceladus. Ocean world missions pose
significant operational challenges. These include long communication lags,
limited power, and lifetime limitations caused by radiation damage and hostile
conditions. Given these operational limitations, onboard autonomy will be vital
for future Ocean world missions. Besides the management of nominal lander
operations, onboard autonomy must react appropriately in the event of
anomalies. Traditional spacecraft rely on a transition into 'safe-mode' in
which non-essential components and subsystems are powered off to preserve
safety and maintain communication with Earth. For a severely time-limited Ocean
world mission, resolutions to these anomalies that can be executed without
Earth-in-the-loop communication and associated delays are paramount for
completion of the mission objectives and science goals. To address these
challenges, the REASIMO effort aims to demonstrate a robust level of
AI-assisted autonomy for such missions, including the ability to detect and
recover from anomalies, and to perform missions based on pre-trained behaviors
rather than hard-coded, predetermined logic like all prior space missions. We
developed an AI-assisted, personality-driven, intelligent framework for control
of an Ocean world mission by combining a mix of advanced technologies. To
demonstrate the capabilities of the framework, we perform tests of autonomous
sampling operations on a lander-manipulator testbed at the NASA Jet Propulsion
Laboratory, approximating possible surface conditions such a mission might
encounter.

</details>


### [9] [Growing Trees with an Agent: Accelerating RRTs with Learned, Multi-Step Episodic Exploration](https://arxiv.org/abs/2507.06605)
*Xinyu Wu*

Main category: cs.RO

TL;DR: Episodic RRT (ERRT) 是一种基于深度强化学习的混合规划框架，通过替换随机采样为多步探索性片段，显著提升了运动规划的效率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于采样的运动规划器（如RRT）在高维或复杂环境中效率低下，主要依赖随机采样导致探索方向不明确。

Method: ERRT使用深度强化学习（DRL）生成多步探索性片段，将搜索过程从随机扩展转变为定向分支式增长。

Result: 在2D、3D和6D环境中，ERRT及其变体表现显著优于传统方法，6D机械臂场景中成功率从19%提升至98%，速度提升107倍，碰撞检查减少99.6%，路径长度缩短近50%。

Conclusion: ERRT通过定向探索和高效路径生成，显著提升了运动规划的性能，其最优变体ERRT*在优化速度上比RRT*快29倍。

Abstract: Classical sampling-based motion planners like the RRTs suffer from
inefficiencies, particularly in cluttered or high-dimensional spaces, due to
their reliance on undirected, random sampling. This paper introduces the
Episodic RRT, a novel hybrid planning framework that replaces the primitive of
a random point with a learned, multi-step "exploratory episode" generated by a
Deep Reinforcement Learning agent. By making the DRL agent the engine of
exploration, ERRT transforms the search process from a diffuse, volumetric
expansion into a directed, branch-like growth. This paradigm shift yields key
advantages: it counters the curse of dimensionality with focused exploration,
minimizes expensive collision checks by proactively proposing locally valid
paths, and improves connectivity by generating inherently connected path
segments. We demonstrate through extensive empirical evaluation across 2D, 3D,
and 6D environments that ERRT and its variants consistently and significantly
outperform their classical counterparts. In a challenging 6D robotic arm
scenario, ERRT achieves a 98% success rate compared to 19% for RRT, is up to
107x faster, reduces collision checks by over 99.6%, and finds initial paths
that are nearly 50% shorter. Furthermore, its asymptotically optimal variant,
ERRT*, demonstrates vastly superior anytime performance, refining solutions to
near-optimality up to 29x faster than standard RRT* in 3D environments. Code:
https://xinyuwuu.github.io/Episodic_RRT/.

</details>


### [10] [Q-STAC: Q-Guided Stein Variational Model Predictive Actor-Critic](https://arxiv.org/abs/2507.06625)
*Shizhe Cai,Jayadeep Jacob,Zeya Yin,Fabio Ramos*

Main category: cs.RO

TL;DR: Q-STAC框架结合贝叶斯MPC与演员-评论家强化学习，通过约束Stein变分梯度下降优化控制序列，提升样本效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习在连续控制任务中数据需求大、长时规划复杂及安全性不足的问题，同时弥补MPC仅局部最优且需精心设计成本函数的局限。

Method: 提出Q-STAC框架，集成贝叶斯MPC与演员-评论家强化学习，利用约束SVGD优化控制序列，以学习到的Q值替代显式成本函数设计。

Result: 在2D导航与机器人操作任务中，Q-STAC表现出更高的样本效率、鲁棒性和最优性，同时保持策略分布的高表达能力。

Conclusion: Q-STAC成功结合了强化学习与MPC的优势，为连续控制任务提供了一种高效、安全且无需显式成本设计的解决方案。

Abstract: Deep reinforcement learning has shown remarkable success in continuous
control tasks, yet often requires extensive training data, struggles with
complex, long-horizon planning, and fails to maintain safety constraints during
operation. Meanwhile, Model Predictive Control (MPC) offers explainability and
constraint satisfaction, but typically yields only locally optimal solutions
and demands careful cost function design. This paper introduces the Q-guided
STein variational model predictive Actor-Critic (Q-STAC), a novel framework
that bridges these approaches by integrating Bayesian MPC with actor-critic
reinforcement learning through constrained Stein Variational Gradient Descent
(SVGD). Our method optimizes control sequences directly using learned Q-values
as objectives, eliminating the need for explicit cost function design while
leveraging known system dynamics to enhance sample efficiency and ensure
control signals remain within safe boundaries. Extensive experiments on 2D
navigation and robotic manipulation tasks demonstrate that Q-STAC achieves
superior sample efficiency, robustness, and optimality compared to
state-of-the-art algorithms, while maintaining the high expressiveness of
policy distributions. Experiment videos are available on our website:
https://sites.google.com/view/q-stac

</details>


### [11] [Multi-Task Multi-Agent Reinforcement Learning via Skill Graphs](https://arxiv.org/abs/2507.06690)
*Guobin Zhu,Rui Zhou,Wenkang Ji,Hongyin Zhang,Donglin Wang,Shiyu Zhao*

Main category: cs.RO

TL;DR: 提出了一种分层方法，通过技能图和高层模块解决多任务多智能体强化学习中的无关任务和知识转移问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理无关任务且知识转移能力有限，需要更高效的方法。

Method: 采用分层方法，高层模块使用技能图，低层模块采用标准MARL算法。

Result: 实验证明该方法优于最新的分层MAPPO算法。

Conclusion: 该方法扩展了多任务强化学习的范围，提升了无关任务处理能力和知识转移效果。

Abstract: Multi-task multi-agent reinforcement learning (MT-MARL) has recently gained
attention for its potential to enhance MARL's adaptability across multiple
tasks. However, it is challenging for existing multi-task learning methods to
handle complex problems, as they are unable to handle unrelated tasks and
possess limited knowledge transfer capabilities. In this paper, we propose a
hierarchical approach that efficiently addresses these challenges. The
high-level module utilizes a skill graph, while the low-level module employs a
standard MARL algorithm. Our approach offers two contributions. First, we
consider the MT-MARL problem in the context of unrelated tasks, expanding the
scope of MTRL. Second, the skill graph is used as the upper layer of the
standard hierarchical approach, with training independent of the lower layer,
effectively handling unrelated tasks and enhancing knowledge transfer
capabilities. Extensive experiments are conducted to validate these advantages
and demonstrate that the proposed method outperforms the latest hierarchical
MAPPO algorithms. Videos and code are available at
https://github.com/WindyLab/MT-MARL-SG

</details>


### [12] [Distributed Fault-Tolerant Multi-Robot Cooperative Localization in Adversarial Environments](https://arxiv.org/abs/2507.06750)
*Tohid Kargar Tasooji,Ramviyas Parasuraman*

Main category: cs.RO

TL;DR: 提出了一种分布式容错协作定位框架，通过自适应事件触发通信策略提升多机器人系统在对抗环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在GPS缺失或通信受限环境中，传统定位方法易受对抗攻击（如传感器操纵和通信干扰）影响，需提升系统鲁棒性。

Method: 采用自适应事件触发通信策略，动态调整通信阈值，并结合实时感知和通信质量分析，确保算法收敛性和稳定性。

Result: 实验表明，该算法在定位精度和通信效率上显著优于传统方法，尤其在对抗环境中表现突出。

Conclusion: 该框架提升了多机器人系统的可扩展性、可靠性和容错性，适用于大规模实际部署。

Abstract: In multi-robot systems (MRS), cooperative localization is a crucial task for
enhancing system robustness and scalability, especially in GPS-denied or
communication-limited environments. However, adversarial attacks, such as
sensor manipulation, and communication jamming, pose significant challenges to
the performance of traditional localization methods. In this paper, we propose
a novel distributed fault-tolerant cooperative localization framework to
enhance resilience against sensor and communication disruptions in adversarial
environments. We introduce an adaptive event-triggered communication strategy
that dynamically adjusts communication thresholds based on real-time sensing
and communication quality. This strategy ensures optimal performance even in
the presence of sensor degradation or communication failure. Furthermore, we
conduct a rigorous analysis of the convergence and stability properties of the
proposed algorithm, demonstrating its resilience against bounded adversarial
zones and maintaining accurate state estimation. Robotarium-based experiment
results show that our proposed algorithm significantly outperforms traditional
methods in terms of localization accuracy and communication efficiency,
particularly in adversarial settings. Our approach offers improved scalability,
reliability, and fault tolerance for MRS, making it suitable for large-scale
deployments in real-world, challenging environments.

</details>


### [13] [Integrating Perceptions: A Human-Centered Physical Safety Model for Human-Robot Interaction](https://arxiv.org/abs/2507.06700)
*Pranav Pandey,Ramviyas Parasuraman,Prashant Doshi*

Main category: cs.RO

TL;DR: 论文提出了一种参数化通用安全模型，通过个性化参数ρ结合物理与感知安全，研究发现机器人行为的可预测性、情感状态和用户类型对感知安全有显著影响。


<details>
  <summary>Details</summary>
Motivation: 传统安全模型仅依赖传感器数据，未能捕捉主观安全感知，需结合个体差异和情境因素。

Method: 通过模拟救援场景的人体实验，研究情感状态、信任和机器人行为对感知安全的影响，引入参数ρ量化个体差异。

Result: ρ能有效捕捉个体差异，机器人行为的可预测性和积极情感状态显著提升感知安全，用户响应可分为少数类型。

Conclusion: 研究强调需开发结合心理与行为维度的自适应安全模型，以提升人机交互的可信度和效果。

Abstract: Ensuring safety in human-robot interaction (HRI) is essential to foster user
trust and enable the broader adoption of robotic systems. Traditional safety
models primarily rely on sensor-based measures, such as relative distance and
velocity, to assess physical safety. However, these models often fail to
capture subjective safety perceptions, which are shaped by individual traits
and contextual factors. In this paper, we introduce and analyze a parameterized
general safety model that bridges the gap between physical and perceived safety
by incorporating a personalization parameter, $\rho$, into the safety
measurement framework to account for individual differences in safety
perception. Through a series of hypothesis-driven human-subject studies in a
simulated rescue scenario, we investigate how emotional state, trust, and robot
behavior influence perceived safety. Our results show that $\rho$ effectively
captures meaningful individual differences, driven by affective responses,
trust in task consistency, and clustering into distinct user types.
Specifically, our findings confirm that predictable and consistent robot
behavior as well as the elicitation of positive emotional states, significantly
enhance perceived safety. Moreover, responses cluster into a small number of
user types, supporting adaptive personalization based on shared safety models.
Notably, participant role significantly shapes safety perception, and repeated
exposure reduces perceived safety for participants in the casualty role,
emphasizing the impact of physical interaction and experiential change. These
findings highlight the importance of adaptive, human-centered safety models
that integrate both psychological and behavioral dimensions, offering a pathway
toward more trustworthy and effective HRI in safety-critical domains.

</details>


### [14] [Stream Function-Based Navigation for Complex Quadcopter Obstacle Avoidance](https://arxiv.org/abs/2507.06787)
*Sean Smith,Emmanuel Witrant,Ya-Jun Pan*

Main category: cs.RO

TL;DR: 提出了一种基于流函数的导航控制系统，结合涡流面板法和模型预测控制，实现复杂环境中的实时避障。


<details>
  <summary>Details</summary>
Motivation: 解决传统涡流面板法在近距离避障和快速移动障碍物处理中的局限性。

Method: 结合涡流面板法（VPM）和基于高阶控制屏障函数（HOCBF）的模型预测控制器（MPC），利用最小包围椭圆（MBE）和自适应卡尔曼滤波器（AKF）处理障碍物动态。

Result: 在PX4驱动的Clover无人机Gazebo模拟器和实际实验中验证了系统的有效性。

Conclusion: 该系统能够高效处理复杂环境中的实时避障问题。

Abstract: This article presents a novel stream function-based navigational control
system for obstacle avoidance, where obstacles are represented as
two-dimensional (2D) rigid surfaces in inviscid, incompressible flows. The
approach leverages the vortex panel method (VPM) and incorporates safety
margins to control the stream function and flow properties around virtual
surfaces, enabling navigation in complex, partially observed environments using
real-time sensing. To address the limitations of the VPM in managing relative
distance and avoiding rapidly accelerating obstacles at close proximity, the
system integrates a model predictive controller (MPC) based on higher-order
control barrier functions (HOCBF). This integration incorporates VPM trajectory
generation, state estimation, and constraint handling into a receding-horizon
optimization problem. The 2D rigid surfaces are enclosed using minimum bounding
ellipses (MBEs), while an adaptive Kalman filter (AKF) captures and predicts
obstacle dynamics, propagating these estimates into the MPC-HOCBF for rapid
avoidance maneuvers. Evaluation is conducted using a PX4-powered Clover drone
Gazebo simulator and real-time experiments involving a COEX Clover quadcopter
equipped with a 360 degree LiDAR sensor.

</details>


### [15] [Spatial-Temporal Aware Visuomotor Diffusion Policy Learning](https://arxiv.org/abs/2507.06710)
*Zhenyang Liu,Yikai Wang,Kuanning Wang,Longfei Liang,Xiangyang Xue,Yanwei Fu*

Main category: cs.RO

TL;DR: 论文提出了一种名为4D Diffusion Policy (DP4)的新型视觉模仿学习方法，通过引入时空感知能力，显著提升了机器人在3D空间和4D时空关系中的学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模仿学习方法主要依赖行为克隆和历史轨迹监督，缺乏对3D结构和4D时空关系的捕捉能力，限制了其在实际应用中的表现。

Method: DP4采用动态高斯世界模型，从交互环境中学习3D空间和4D时空感知，通过单视角RGB-D观测构建当前3D场景并预测未来3D场景，优化轨迹生成。

Result: 在17个仿真任务和3个真实机器人任务中，DP4表现优于基线方法，仿真任务平均成功率提升16.4%（Adroit）、14%（DexArt）和6.45%（RLBench），真实任务提升8.6%。

Conclusion: DP4通过引入时空感知能力，显著提升了视觉模仿学习的效果，为机器人任务学习提供了更强大的工具。

Abstract: Visual imitation learning is effective for robots to learn versatile tasks.
However, many existing methods rely on behavior cloning with supervised
historical trajectories, limiting their 3D spatial and 4D spatiotemporal
awareness. Consequently, these methods struggle to capture the 3D structures
and 4D spatiotemporal relationships necessary for real-world deployment. In
this work, we propose 4D Diffusion Policy (DP4), a novel visual imitation
learning method that incorporates spatiotemporal awareness into diffusion-based
policies. Unlike traditional approaches that rely on trajectory cloning, DP4
leverages a dynamic Gaussian world model to guide the learning of 3D spatial
and 4D spatiotemporal perceptions from interactive environments. Our method
constructs the current 3D scene from a single-view RGB-D observation and
predicts the future 3D scene, optimizing trajectory generation by explicitly
modeling both spatial and temporal dependencies. Extensive experiments across
17 simulation tasks with 173 variants and 3 real-world robotic tasks
demonstrate that the 4D Diffusion Policy (DP4) outperforms baseline methods,
improving the average simulation task success rate by 16.4% (Adroit), 14%
(DexArt), and 6.45% (RLBench), and the average real-world robotic task success
rate by 8.6%.

</details>


### [16] [LOVON: Legged Open-Vocabulary Object Navigator](https://arxiv.org/abs/2507.06747)
*Daojie Peng,Jiahang Cao,Qiang Zhang,Jun Ma*

Main category: cs.RO

TL;DR: LOVON是一个集成大型语言模型（LLMs）和开放词汇视觉检测模型的新框架，用于动态非结构化环境中的长距离物体导航。


<details>
  <summary>Details</summary>
Motivation: 解决开放世界环境中机器人系统在长时程任务中整合物体检测和高级任务规划的挑战。

Method: 结合LLMs进行分层任务规划，采用开放词汇视觉检测模型，并设计视觉稳定化和功能执行逻辑。

Result: 成功完成涉及实时检测、搜索和导航的长序列任务，并在不同机器人上验证了兼容性。

Conclusion: LOVON在动态环境中表现出色，具有广泛的适用性和鲁棒性。

Abstract: Object navigation in open-world environments remains a formidable and
pervasive challenge for robotic systems, particularly when it comes to
executing long-horizon tasks that require both open-world object detection and
high-level task planning. Traditional methods often struggle to integrate these
components effectively, and this limits their capability to deal with complex,
long-range navigation missions. In this paper, we propose LOVON, a novel
framework that integrates large language models (LLMs) for hierarchical task
planning with open-vocabulary visual detection models, tailored for effective
long-range object navigation in dynamic, unstructured environments. To tackle
real-world challenges including visual jittering, blind zones, and temporary
target loss, we design dedicated solutions such as Laplacian Variance Filtering
for visual stabilization. We also develop a functional execution logic for the
robot that guarantees LOVON's capabilities in autonomous navigation, task
adaptation, and robust task completion. Extensive evaluations demonstrate the
successful completion of long-sequence tasks involving real-time detection,
search, and navigation toward open-vocabulary dynamic targets. Furthermore,
real-world experiments across different legged robots (Unitree Go2, B2, and
H1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.

</details>


### [17] [Hierarchical Reinforcement Learning for Articulated Tool Manipulation with Multifingered Hand](https://arxiv.org/abs/2507.06822)
*Wei Xu,Yanchao Zhao,Weichao Guo,Xinjun Sheng*

Main category: cs.RO

TL;DR: 提出了一种分层目标条件强化学习框架，用于提升拟人化机械手操作铰接工具的能力，实验成功率为70.8%。


<details>
  <summary>Details</summary>
Motivation: 铰接工具（如镊子或剪刀）的动态形状变化为机械手操作带来独特挑战，此前研究较少涉及。

Method: 采用分层策略：低层策略控制工具配置，高层策略定义目标状态并控制机械臂；使用编码器估计工具的可操作状态，并通过启发式策略生成回放缓冲区。

Result: 实验表明，机器人能有效操作镊子状工具抓取不同形状和大小的物体，成功率为70.8%。

Conclusion: 强化学习在提升铰接工具操作能力方面具有潜力。

Abstract: Manipulating articulated tools, such as tweezers or scissors, has rarely been
explored in previous research. Unlike rigid tools, articulated tools change
their shape dynamically, creating unique challenges for dexterous robotic
hands. In this work, we present a hierarchical, goal-conditioned reinforcement
learning (GCRL) framework to improve the manipulation capabilities of
anthropomorphic robotic hands using articulated tools. Our framework comprises
two policy layers: (1) a low-level policy that enables the dexterous hand to
manipulate the tool into various configurations for objects of different sizes,
and (2) a high-level policy that defines the tool's goal state and controls the
robotic arm for object-picking tasks. We employ an encoder, trained on
synthetic pointclouds, to estimate the tool's affordance states--specifically,
how different tool configurations (e.g., tweezer opening angles) enable
grasping of objects of varying sizes--from input point clouds, thereby enabling
precise tool manipulation. We also utilize a privilege-informed heuristic
policy to generate replay buffer, improving the training efficiency of the
high-level policy. We validate our approach through real-world experiments,
showing that the robot can effectively manipulate a tweezer-like tool to grasp
objects of diverse shapes and sizes with a 70.8 % success rate. This study
highlights the potential of RL to advance dexterous robotic manipulation of
articulated tools.

</details>


### [18] [Friction Estimation for In-Hand Planar Motion](https://arxiv.org/abs/2507.06824)
*Gabriel Arslan Waltersson,Yiannis Karayiannidis*

Main category: cs.RO

TL;DR: 提出了一种在线估计平行夹持器滑动操作中接触特性的方法，包括静摩擦、库仑摩擦和接触半径。


<details>
  <summary>Details</summary>
Motivation: 为了在滑动操作中实时估计接触特性，以提高夹持器的控制精度和适应性。

Method: 通过触觉测量接触力和滑动速度，估计静摩擦、库仑摩擦和接触半径，并提出了处理快速滑移-粘附动力学的启发式方法。

Result: 在仿真和实际实验中验证了方法的有效性。

Conclusion: 该方法能够有效估计接触特性，并处理滑移-粘附动力学问题。

Abstract: This paper presents a method for online estimation of contact properties
during in-hand sliding manipulation with a parallel gripper. We estimate the
static and Coulomb friction as well as the contact radius from tactile
measurements of contact forces and sliding velocities. The method is validated
in both simulation and real-world experiments. Furthermore, we propose a
heuristic to deal with fast slip-stick dynamics which can adversely affect the
estimation.

</details>


### [19] [Toward a Full-Stack Co-Simulation Platform for Testing of Automated Driving Systems](https://arxiv.org/abs/2507.06884)
*Dong Bi,Yongqi Zhao,Zhengguo Gu,Tomislav Mihalj,Jia Hu,Arno Eichberger*

Main category: cs.RO

TL;DR: 提出了一种全栈工具链，用于从真实数据自动生成场景并通过基于CarMaker、ROS和Apollo的协同仿真平台高效验证自动驾驶系统。


<details>
  <summary>Details</summary>
Motivation: 现有仿真工具链难以整合快速、自动化的场景生成与支持高级自动驾驶能力的仿真环境。

Method: 开发了一种全栈工具链，结合真实数据集自动生成场景，并通过CarMaker、ROS和Apollo的协同仿真平台进行验证。

Result: 仿真结果证明了该工具链的有效性。

Conclusion: 该工具链解决了现有仿真工具链的局限性，加速了自动驾驶系统的部署。

Abstract: Virtual testing has emerged as an effective approach to accelerate the
deployment of automated driving systems. Nevertheless, existing simulation
toolchains encounter difficulties in integrating rapid, automated scenario
generation with simulation environments supporting advanced automated driving
capabilities. To address this limitation, a full-stack toolchain is presented,
enabling automatic scenario generation from real-world datasets and efficient
validation through a co-simulation platform based on CarMaker, ROS, and Apollo.
The simulation results demonstrate the effectiveness of the proposed toolchain.
A demonstration video showcasing the toolchain is available at the provided
link: https://youtu.be/taJw_-CmSiY.

</details>


### [20] [ULC: A Unified and Fine-Grained Controller for Humanoid Loco-Manipulation](https://arxiv.org/abs/2507.06905)
*Wandong Sun,Luying Feng,Baoshi Cao,Yang Liu,Yaochu Jin,Zongwu Xie*

Main category: cs.RO

TL;DR: 论文提出了一种统一的人形机器人运动与操作控制框架（ULC），通过单一策略实现全身协调控制，优于传统分层方法。


<details>
  <summary>Details</summary>
Motivation: 现有分层控制方法限制了子系统间的协调，无法实现类似人类的全身统一控制。

Method: 采用序列技能获取、残差动作建模、命令多项式插值等技术，实现端到端的统一控制。

Result: ULC在跟踪精度、工作空间范围和鲁棒性上优于基线方法，支持复杂任务下的精确操作。

Conclusion: 统一控制框架可行且高效，为复杂人形机器人任务提供了新思路。

Abstract: Loco-Manipulation for humanoid robots aims to enable robots to integrate
mobility with upper-body tracking capabilities. Most existing approaches adopt
hierarchical architectures that decompose control into isolated upper-body
(manipulation) and lower-body (locomotion) policies. While this decomposition
reduces training complexity, it inherently limits coordination between
subsystems and contradicts the unified whole-body control exhibited by humans.
We demonstrate that a single unified policy can achieve a combination of
tracking accuracy, large workspace, and robustness for humanoid
loco-manipulation. We propose the Unified Loco-Manipulation Controller (ULC), a
single-policy framework that simultaneously tracks root velocity, root height,
torso rotation, and dual-arm joint positions in an end-to-end manner, proving
the feasibility of unified control without sacrificing performance. We achieve
this unified control through key technologies: sequence skill acquisition for
progressive learning complexity, residual action modeling for fine-grained
control adjustments, command polynomial interpolation for smooth motion
transitions, random delay release for robustness to deploy variations, load
randomization for generalization to external disturbances, and
center-of-gravity tracking for providing explicit policy gradients to maintain
stability. We validate our method on the Unitree G1 humanoid robot with 3-DOF
(degrees-of-freedom) waist. Compared with strong baselines, ULC shows better
tracking performance to disentangled methods and demonstrating larger workspace
coverage. The unified dual-arm tracking enables precise manipulation under
external loads while maintaining coordinated whole-body control for complex
loco-manipulation tasks.

</details>


### [21] [Bounomodes: the grazing ox algorithm for exploration of clustered anomalies](https://arxiv.org/abs/2507.06960)
*Samuel Matloob,Ayan Dutta,O. Patrick Kreidl,Swapnonel Roy,Ladislau Bölöni*

Main category: cs.RO

TL;DR: 论文提出了一种名为“bounom=odes”的算法，结合均匀采样和异常区域探索，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统IPP算法采用均匀覆盖策略，但在异常集群场景下效果不佳，需优先探索异常区域。

Method: 结合均匀采样（boustrophedon）和基于深度强化学习的异常区域探索。

Result: 实验表明，该方法优于多个基线算法。

Conclusion: bounom=odes算法在异常集群场景下更高效。

Abstract: A common class of algorithms for informative path planning (IPP) follows
boustrophedon ("as the ox turns") patterns, which aim to achieve uniform area
coverage. However, IPP is often applied in scenarios where anomalies, such as
plant diseases, pollution, or hurricane damage, appear in clusters. In such
cases, prioritizing the exploration of anomalous regions over uniform coverage
is beneficial. This work introduces a class of algorithms referred to as
bounom\=odes ("as the ox grazes"), which alternates between uniform
boustrophedon sampling and targeted exploration of detected anomaly clusters.
While uniform sampling can be designed using geometric principles, close
exploration of clusters depends on the spatial distribution of anomalies and
must be learned. In our implementation, the close exploration behavior is
learned using deep reinforcement learning algorithms. Experimental evaluations
demonstrate that the proposed approach outperforms several established
baselines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease](https://arxiv.org/abs/2507.06326)
*Harsh Ravivarapu,Gaurav Bagwe,Xiaoyong Yuan,Chunxiu Yu,Lan Zhang*

Main category: cs.LG

TL;DR: SEA-DBS是一种基于强化学习的自适应脑深部刺激框架，通过预测奖励模型和Gumbel Softmax探索，提高了样本效率和硬件兼容性。


<details>
  <summary>Details</summary>
Motivation: 传统开环DBS缺乏适应性和个性化，而现有强化学习方法存在样本复杂度高和硬件部署限制的问题。

Method: SEA-DBS结合预测奖励模型和Gumbel Softmax探索，优化了样本效率和二进制动作空间的稳定性。

Result: 在帕金森病基底节活动模拟中，SEA-DBS表现出更快的收敛速度、更强的病理β波段抑制能力，并支持FP16量化。

Conclusion: SEA-DBS为资源受限的实时神经调控提供了一种实用且有效的强化学习解决方案。

Abstract: Deep brain stimulation (DBS) is an established intervention for Parkinson's
disease (PD), but conventional open-loop systems lack adaptability, are
energy-inefficient due to continuous stimulation, and provide limited
personalization to individual neural dynamics. Adaptive DBS (aDBS) offers a
closed-loop alternative, using biomarkers such as beta-band oscillations to
dynamically modulate stimulation. While reinforcement learning (RL) holds
promise for personalized aDBS control, existing methods suffer from high sample
complexity, unstable exploration in binary action spaces, and limited
deployability on resource-constrained hardware.
  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses
the core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a
predictive reward model to reduce reliance on real-time feedback and employs
Gumbel Softmax-based exploration for stable, differentiable policy updates in
binary action spaces. Together, these components improve sample efficiency,
exploration robustness, and compatibility with resource-constrained
neuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic
simulation of Parkinsonian basal ganglia activity, demonstrating faster
convergence, stronger suppression of pathological beta-band power, and
resilience to post-training FP16 quantization. Our results show that SEA-DBS
offers a practical and effective RL-based aDBS framework for real-time,
resource-constrained neuromodulation.

</details>


### [23] [Neural Network-Based Parameter Estimation for Non-Autonomous Differential Equations with Discontinuous Signals](https://arxiv.org/abs/2507.06267)
*Hyeontae Jo,Krešimir Josić,Jae Kyoung Kim*

Main category: cs.LG

TL;DR: 提出了一种名为HADES-NN的新方法，通过神经网络平滑近似非连续外部信号，用于非自治微分方程的参数估计。


<details>
  <summary>Details</summary>
Motivation: 非自治微分方程在建模受外部信号影响的系统时很重要，但信号突变时参数估计变得困难。

Method: HADES-NN分两阶段：先用神经网络平滑近似非连续信号，再用平滑信号估计模型参数。

Result: HADES-NN在多种应用中提供了高精度参数估计，如昼夜节律系统和酵母交配响应。

Conclusion: HADES-NN显著扩展了可拟合实际测量数据的模型系统范围。

Abstract: Non-autonomous differential equations are crucial for modeling systems
influenced by external signals, yet fitting these models to data becomes
particularly challenging when the signals change abruptly. To address this
problem, we propose a novel parameter estimation method utilizing functional
approximations with artificial neural networks. Our approach, termed Harmonic
Approximation of Discontinuous External Signals using Neural Networks
(HADES-NN), operates in two iterated stages. In the first stage, the algorithm
employs a neural network to approximate the discontinuous signal with a smooth
function. In the second stage, it uses this smooth approximate signal to
estimate model parameters. HADES-NN gives highly accurate and precise parameter
estimates across various applications, including circadian clock systems
regulated by external light inputs measured via wearable devices and the mating
response of yeast to external pheromone signals. HADES-NN greatly extends the
range of model systems that can be fit to real-world measurements.

</details>


### [24] [Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits](https://arxiv.org/abs/2507.06535)
*Shan Shen,Shenglu Hua,Jiajun Zou,Jiawei Liu,Jianwang Zhai,Chuan Shi,Wenjian Yu*

Main category: cs.LG

TL;DR: 论文提出CircuitGCL，一种图对比学习框架，用于解决AMS电路图表示学习中的数据稀缺、标签不平衡和电路多样性问题，显著提升了寄生估计任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决AMS电路图表示学习中数据稀缺、标签不平衡和电路多样性带来的挑战，以提升电路表示的鲁棒性和可迁移性。

Method: 提出CircuitGCL框架，结合超球表示散射和标签重平衡技术，采用自监督学习策略和平衡MSE与bsmCE损失函数。

Result: 在TSMC 28nm AMS设计上，CircuitGCL在寄生电容估计和接地电容分类任务中表现优于现有方法，R²提升33.64%~44.20%，F1分数提升0.9~2.1倍。

Conclusion: CircuitGCL通过创新的表示学习和标签平衡方法，显著提升了AMS电路图的表示学习效果，适用于多种下游任务。

Abstract: Graph representation learning on Analog-Mixed Signal (AMS) circuits is
crucial for various downstream tasks, e.g., parasitic estimation. However, the
scarcity of design data, the unbalanced distribution of labels, and the
inherent diversity of circuit implementations pose significant challenges to
learning robust and transferable circuit representations. To address these
limitations, we propose CircuitGCL, a novel graph contrastive learning
framework that integrates representation scattering and label rebalancing to
enhance transferability across heterogeneous circuit graphs. CircuitGCL employs
a self-supervised strategy to learn topology-invariant node embeddings through
hyperspherical representation scattering, eliminating dependency on large-scale
data. Simultaneously, balanced mean squared error (MSE) and softmax
cross-entropy (bsmCE) losses are introduced to mitigate label distribution
disparities between circuits, enabling robust and transferable parasitic
estimation. Evaluated on parasitic capacitance estimation (edge-level task) and
ground capacitance classification (node-level task) across TSMC 28nm AMS
designs, CircuitGCL outperforms all state-of-the-art (SOTA) methods, with the
$R^2$ improvement of $33.64\% \sim 44.20\%$ for edge regression and F1-score
gain of $0.9\times \sim 2.1\times$ for node classification. Our code is
available at
\href{https://anonymous.4open.science/r/CircuitGCL-099B/README.md}{here}.

</details>


### [25] [SymFlux: deep symbolic regression of Hamiltonian vector fields](https://arxiv.org/abs/2507.06342)
*M. A. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: SymFlux是一个深度学习框架，通过符号回归从标准辛平面上的向量场识别哈密顿函数。


<details>
  <summary>Details</summary>
Motivation: 推动哈密顿力学中的自动化发现，解决从向量场中恢复哈密顿函数的挑战。

Method: 采用混合CNN-LSTM架构，从新开发的哈密顿向量场数据集中学习和输出符号表达式。

Result: 模型能准确恢复符号表达式，验证了其有效性。

Conclusion: SymFlux在哈密顿力学中实现了自动化发现的进步。

Abstract: We present SymFlux, a novel deep learning framework that performs symbolic
regression to identify Hamiltonian functions from their corresponding vector
fields on the standard symplectic plane. SymFlux models utilize hybrid CNN-LSTM
architectures to learn and output the symbolic mathematical expression of the
underlying Hamiltonian. Training and validation are conducted on newly
developed datasets of Hamiltonian vector fields, a key contribution of this
work. Our results demonstrate the model's effectiveness in accurately
recovering these symbolic expressions, advancing automated discovery in
Hamiltonian mechanics.

</details>


### [26] [Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction](https://arxiv.org/abs/2507.06538)
*Shan Shen,Yibin Zhang,Hector Rodriguez Rodriguez,Wenjian Yu*

Main category: cs.LG

TL;DR: CircuitGPS是一种少样本学习方法，用于预测AMS电路中的寄生效应，通过小跳采样和混合图Transformer提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 由于集成电路设计数据稀缺，训练深度学习模型用于AMS设计受到限制，因此需要一种高效的方法来预测寄生效应。

Method: 将电路网表表示为异构图，通过小跳采样生成子图，使用混合图Transformer学习子图嵌入，并结合低成本的位置编码。

Result: CircuitGPS将耦合存在预测的准确性提高了至少20%，电容估计的MAE降低了至少0.067。

Conclusion: CircuitGPS展示了强大的可扩展性，并通过零样本学习直接应用于多样化的AMS电路设计。

Abstract: Graph representation learning is a powerful method to extract features from
graph-structured data, such as analog/mixed-signal (AMS) circuits. However,
training deep learning models for AMS designs is severely limited by the
scarcity of integrated circuit design data. In this work, we present
CircuitGPS, a few-shot learning method for parasitic effect prediction in AMS
circuits. The circuit netlist is represented as a heterogeneous graph, with the
coupling capacitance modeled as a link. CircuitGPS is pre-trained on link
prediction and fine-tuned on edge regression. The proposed method starts with a
small-hop sampling technique that converts a link or a node into a subgraph.
Then, the subgraph embeddings are learned with a hybrid graph Transformer.
Additionally, CircuitGPS integrates a low-cost positional encoding that
summarizes the positional and structural information of the sampled subgraph.
CircuitGPS improves the accuracy of coupling existence by at least 20\% and
reduces the MAE of capacitance estimation by at least 0.067 compared to
existing methods. Our method demonstrates strong inherent scalability, enabling
direct application to diverse AMS circuit designs through zero-shot learning.
Furthermore, the ablation studies provide valuable insights into graph models
for representation learning.

</details>


### [27] [DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction](https://arxiv.org/abs/2507.06366)
*Yupu Zhang,Zelin Xu,Tingsong Xiao,Gustavo Seabra,Yanjun Li,Chenglong Li,Zhe Jiang*

Main category: cs.LG

TL;DR: 论文提出DecoyDB数据集和定制化GCL框架，用于蛋白质-配体复合物的自监督学习，显著提升结合亲和力预测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 蛋白质-配体结合亲和力预测在药物发现中至关重要，但现有数据集规模小且标签不足，限制了进展。自监督学习（尤其是图对比学习）为解决这一问题提供了可能。

Method: 提出DecoyDB数据集，包含高分辨率真实复合物和多样化诱饵结构，并设计定制化GCL框架进行预训练和微调。

Result: 实验表明，基于DecoyDB预训练的模型在准确性、标签效率和泛化性方面表现优异。

Conclusion: DecoyDB和定制化GCL框架填补了领域空白，为蛋白质-配体结合亲和力预测提供了高效解决方案。

Abstract: Predicting the binding affinity of protein-ligand complexes plays a vital
role in drug discovery. Unfortunately, progress has been hindered by the lack
of large-scale and high-quality binding affinity labels. The widely used
PDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,
especially graph contrastive learning (GCL), provides a unique opportunity to
break the barrier by pre-training graph neural network models based on vast
unlabeled complexes and fine-tuning the models on much fewer labeled complexes.
However, the problem faces unique challenges, including a lack of a
comprehensive unlabeled dataset with well-defined positive/negative complex
pairs and the need to design GCL algorithms that incorporate the unique
characteristics of such data. To fill the gap, we propose DecoyDB, a
large-scale, structure-aware dataset specifically designed for self-supervised
GCL on protein-ligand complexes. DecoyDB consists of high-resolution ground
truth complexes (less than 2.5 Angstrom) and diverse decoy structures with
computationally generated binding poses that range from realistic to suboptimal
(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation
(RMSD) from the native pose. We further design a customized GCL framework to
pre-train graph neural networks based on DecoyDB and fine-tune the models with
labels from PDBbind. Extensive experiments confirm that models pre-trained with
DecoyDB achieve superior accuracy, label efficiency, and generalizability.

</details>


### [28] [Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs](https://arxiv.org/abs/2507.06549)
*Shan Shen,Dingcheng Yang,Yuyang Xie,Chunyan Pei,Wenjian Yu,Bei Yu*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的2阶段模型，用于在预布局阶段准确预测SRAM电路中的寄生效应，显著提高了模拟效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决预布局和后布局电路模拟之间的显著差异，减少设计迭代次数，提高系统能效。

Method: 结合图神经网络（GNN）分类器和多层感知机（MLP）回归器，使用Focal Loss处理类别不平衡，并整合子电路信息以抽象层次结构。

Result: 在4个实际SRAM设计中，模型将寄生预测误差最大降低19倍，模拟速度提升高达598倍。

Conclusion: 该方法显著提高了寄生预测的准确性和模拟效率，为SRAM设计提供了有力工具。

Abstract: To achieve higher system energy efficiency, SRAM in SoCs is often customized.
The parasitic effects cause notable discrepancies between pre-layout and
post-layout circuit simulations, leading to difficulty in converging design
parameters and excessive design iterations. Is it possible to well predict the
parasitics based on the pre-layout circuit, so as to perform parasitic-aware
pre-layout simulation? In this work, we propose a deep-learning-based 2-stage
model to accurately predict these parasitics in pre-layout stages. The model
combines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron
(MLP) regressors, effectively managing class imbalance of the net parasitics in
SRAM circuits. We also employ Focal Loss to mitigate the impact of abundant
internal net samples and integrate subcircuit information into the graph to
abstract the hierarchical structure of schematics. Experiments on 4 real SRAM
designs show that our approach not only surpasses the state-of-the-art model in
parasitic prediction by a maximum of 19X reduction of error but also
significantly boosts the simulation process by up to 598X speedup.

</details>


### [29] [The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks](https://arxiv.org/abs/2507.06367)
*El Mehdi Achour,Kathlén Kohn,Holger Rauhut*

Main category: cs.LG

TL;DR: 研究了深度线性卷积网络梯度流的几何性质，发现参数空间的梯度流可以表示为函数空间的黎曼梯度流，且不受初始化条件限制。


<details>
  <summary>Details</summary>
Motivation: 探索线性卷积网络梯度流的几何特性，特别是与初始化条件的关系。

Method: 通过分析参数空间和函数空间的梯度流关系，研究不同维度卷积和步长的影响。

Result: 发现梯度流可以表示为黎曼梯度流，且对初始化无要求，适用于D≥2维卷积或步长大于1的一维卷积。

Conclusion: 线性卷积网络的梯度流具有几何不变性，黎曼度量依赖于初始化。

Abstract: We study geometric properties of the gradient flow for learning deep linear
convolutional networks. For linear fully connected networks, it has been shown
recently that the corresponding gradient flow on parameter space can be written
as a Riemannian gradient flow on function space (i.e., on the product of weight
matrices) if the initialization satisfies a so-called balancedness condition.
We establish that the gradient flow on parameter space for learning linear
convolutional networks can be written as a Riemannian gradient flow on function
space regardless of the initialization. This result holds for $D$-dimensional
convolutions with $D \geq 2$, and for $D =1$ it holds if all so-called strides
of the convolutions are greater than one. The corresponding Riemannian metric
depends on the initialization.

</details>


### [30] [Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation](https://arxiv.org/abs/2507.06380)
*Habibur Rahaman,Atri Chatterjee,Swarup Bhunia*

Main category: cs.LG

TL;DR: WINGs框架通过动态生成全连接层权重和压缩卷积层权重，显著减少内存需求，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂神经网络因存储大量权重而占用过多内存的问题，适用于资源受限的边缘应用。

Method: 使用PCA降维和轻量级SVR模型预测权重，结合敏感性分析优先压缩CNN低敏感性层。

Result: 实现全连接层53倍压缩，AlexNet在MNIST上28倍压缩，CIFAR-10上18倍压缩，精度损失1-2%。

Conclusion: WINGs显著降低内存需求，提高推理吞吐量并减少能耗，适用于边缘计算。

Abstract: Complex neural networks require substantial memory to store a large number of
synaptic weights. This work introduces WINGs (Automatic Weight Generator for
Secure and Storage-Efficient Deep Learning Models), a novel framework that
dynamically generates layer weights in a fully connected neural network (FC)
and compresses the weights in convolutional neural networks (CNNs) during
inference, significantly reducing memory requirements without sacrificing
accuracy. WINGs framework uses principal component analysis (PCA) for
dimensionality reduction and lightweight support vector regression (SVR) models
to predict layer weights in the FC networks, removing the need for storing
full-weight matrices and achieving substantial memory savings. It also
preferentially compresses the weights in low-sensitivity layers of CNNs using
PCA and SVR with sensitivity analysis. The sensitivity-aware design also offers
an added level of security, as any bit-flip attack with weights in compressed
layers has an amplified and readily detectable effect on accuracy. WINGs
achieves 53x compression for the FC layers and 28x for AlexNet with MNIST
dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.
This significant reduction in memory results in higher throughput and lower
energy for DNN inference, making it attractive for resource-constrained edge
applications.

</details>


### [31] [Heterogeneous Graph Neural Networks for Short-term State Forecasting in Power Systems across Domains and Time Scales: A Hydroelectric Power Plant Case Study](https://arxiv.org/abs/2507.06694)
*Raffael Theiler,Olga Fink*

Main category: cs.LG

TL;DR: 论文提出了一种基于异构图注意力网络的方法，用于多领域电力系统状态预测，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统因可再生能源和分布式能源的引入而变得复杂，需要可靠的短期状态预测以确保稳定运行。现有图神经网络方法通常假设传感器关系同质且局限于单一领域，无法处理真实系统中的异构数据。

Method: 采用异构图注意力网络（Heterogeneous Graph Attention Networks），建模同质领域内和异质领域间的传感器关系，特别针对液压和电气两个领域的不同时间动态。

Result: 实验表明，该方法在归一化均方根误差上平均比传统基线方法提高了35.5%。

Conclusion: 该方法在多领域、多速率电力系统状态预测中表现出色，验证了其有效性。

Abstract: Accurate short-term state forecasting is essential for efficient and stable
operation of modern power systems, especially in the context of increasing
variability introduced by renewable and distributed energy resources. As these
systems evolve rapidly, it becomes increasingly important to reliably predict
their states in the short term to ensure operational stability, support control
decisions, and enable interpretable monitoring of sensor and machine behavior.
Modern power systems often span multiple physical domains - including
electrical, mechanical, hydraulic, and thermal - posing significant challenges
for modeling and prediction. Graph Neural Networks (GNNs) have emerged as a
promising data-driven framework for system state estimation and state
forecasting in such settings. By leveraging the topological structure of sensor
networks, GNNs can implicitly learn inter-sensor relationships and propagate
information across the network. However, most existing GNN-based methods are
designed under the assumption of homogeneous sensor relationships and are
typically constrained to a single physical domain. This limitation restricts
their ability to integrate and reason over heterogeneous sensor data commonly
encountered in real-world energy systems, such as those used in energy
conversion infrastructure. In this work, we propose the use of Heterogeneous
Graph Attention Networks to address these limitations. Our approach models both
homogeneous intra-domain and heterogeneous inter-domain relationships among
sensor data from two distinct physical domains - hydraulic and electrical -
which exhibit fundamentally different temporal dynamics. Experimental results
demonstrate that our method significantly outperforms conventional baselines on
average by 35.5% in terms of normalized root mean square error, confirming its
effectiveness in multi-domain, multi-rate power system state forecasting.

</details>


### [32] [KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks](https://arxiv.org/abs/2507.06381)
*James Hazelden,Laura Driscoll,Eli Shlizerman,Eric Shea-Brown*

Main category: cs.LG

TL;DR: 论文提出了一种梯度流分解方法，用于分析循环动态系统中的学习机制，揭示了低维潜在动态和多任务对齐的机制。


<details>
  <summary>Details</summary>
Motivation: 理解梯度下降在非线性循环模型（如RNNs、Neural ODEs、GRUs）中如何塑造学习表示，尤其是神经崩溃和潜在动态的机制。

Method: 将梯度流分解为两个算子（参数算子K和线性化流传播子P），分析其相互作用及其对学习动态的影响。

Result: 揭示了低维潜在动态的形成机制，并提出了多任务训练中目标对齐的度量方法。实验和理论验证了这些发现。

Conclusion: 该工作为理解非线性循环模型中的梯度下降学习提供了新的理论工具，并开发了相应的分析工具包KPFlow。

Abstract: Gradient Descent (GD) and its variants are the primary tool for enabling
efficient training of recurrent dynamical systems such as Recurrent Neural
Networks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics
that are formed in these models exhibit features such as neural collapse and
emergence of latent representations that may support the remarkable
generalization properties of networks. In neuroscience, qualitative features of
these representations are used to compare learning in biological and artificial
systems. Despite recent progress, there remains a need for theoretical tools to
rigorously understand the mechanisms shaping learned representations,
especially in finite, non-linear models. Here, we show that the gradient flow,
which describes how the model's dynamics evolve over GD, can be decomposed into
a product that involves two operators: a Parameter Operator, K, and a
Linearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in
feed-forward neural networks, while P appears in Lyapunov stability and optimal
control theory. We demonstrate two applications of our decomposition. First, we
show how their interplay gives rise to low-dimensional latent dynamics under
GD, and, specifically, how the collapse is a result of the network structure,
over and above the nature of the underlying task. Second, for multi-task
training, we show that the operators can be used to measure how objectives
relevant to individual sub-tasks align. We experimentally and theoretically
validate these findings, providing an efficient Pytorch package, \emph{KPFlow},
implementing robust analysis tools for general recurrent architectures. Taken
together, our work moves towards building a next stage of understanding of GD
learning in non-linear recurrent models.

</details>


### [33] [Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning](https://arxiv.org/abs/2507.06402)
*Siddhant Deshpande,Yalemzerf Getnet,Waltenegus Dargie*

Main category: cs.LG

TL;DR: 论文研究了无线心电图（ECG）信号的防篡改检测和身份验证，比较了CNN、ResNet和Transformer-CNN混合模型的性能，并在多种篡改策略下进行了测试。


<details>
  <summary>Details</summary>
Motivation: 随着无线ECG系统的普及，保护信号完整性免受篡改变得至关重要。

Method: 使用CNN、ResNet和Transformer-CNN模型进行篡改检测，Siamese网络进行身份验证。ECG信号通过连续小波变换（CWT）转换为时频域二维表示。

Result: 在高度碎片化篡改场景下，模型准确率超过99.5%；对于细微篡改，FeatCNN-TranCNN模型平均准确率为98%。身份验证中，CNN-Transformer混合Siamese模型达到100%准确率。

Conclusion: 混合模型在ECG防篡改和身份验证中表现出色，尤其是CNN-Transformer Siamese模型实现了完美验证。

Abstract: With the proliferation of wireless electrocardiogram (ECG) systems for health
monitoring and authentication, protecting signal integrity against tampering is
becoming increasingly important. This paper analyzes the performance of CNN,
ResNet, and hybrid Transformer-CNN models for tamper detection. It also
evaluates the performance of a Siamese network for ECG based identity
verification. Six tampering strategies, including structured segment
substitutions and random insertions, are emulated to mimic real world attacks.
The one-dimensional ECG signals are transformed into a two dimensional
representation in the time frequency domain using the continuous wavelet
transform (CWT). The models are trained and evaluated using ECG data from 54
subjects recorded in four sessions 2019 to 2025 outside of clinical settings
while the subjects performed seven different daily activities. Experimental
results show that in highly fragmented manipulation scenarios, CNN,
FeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding
99.5 percent . Similarly, for subtle manipulations (for example, 50 percent
from A and 50 percent from B and, 75 percent from A and 25 percent from B
substitutions) our FeatCNN-TranCNN model demonstrated consistently reliable
performance, achieving an average accuracy of 98 percent . For identity
verification, the pure Transformer-Siamese network achieved an average accuracy
of 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model
delivered perfect verification performance with 100 percent accuracy.

</details>


### [34] [Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction](https://arxiv.org/abs/2507.06432)
*Mingcheng Zhu,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: KnowRare是一个基于领域适应的深度学习框架，用于预测ICU中罕见疾病的临床结果，通过自监督预训练和条件知识图谱解决数据稀缺和异质性问题，表现优于现有模型和ICU评分系统。


<details>
  <summary>Details</summary>
Motivation: ICU中罕见疾病因数据稀缺和异质性未得到充分服务，需要开发新方法以改善临床决策。

Method: KnowRare通过自监督预训练学习条件无关表示，并利用条件知识图谱选择性适应临床相似条件的知识。

Result: 在五个临床预测任务中，KnowRare表现优于现有模型和ICU评分系统，并展示了灵活性和泛化能力。

Conclusion: KnowRare是支持ICU罕见疾病临床决策的实用解决方案，具有潜力改善护理质量。

Abstract: Artificial Intelligence has revolutionised critical care for common
conditions. Yet, rare conditions in the intensive care unit (ICU), including
recognised rare diseases and low-prevalence conditions in the ICU, remain
underserved due to data scarcity and intra-condition heterogeneity. To bridge
such gaps, we developed KnowRare, a domain adaptation-based deep learning
framework for predicting clinical outcomes for rare conditions in the ICU.
KnowRare mitigates data scarcity by initially learning condition-agnostic
representations from diverse electronic health records through self-supervised
pre-training. It addresses intra-condition heterogeneity by selectively
adapting knowledge from clinically similar conditions with a developed
condition knowledge graph. Evaluated on two ICU datasets across five clinical
prediction tasks (90-day mortality, 30-day readmission, ICU mortality,
remaining length of stay, and phenotyping), KnowRare consistently outperformed
existing state-of-the-art models. Additionally, KnowRare demonstrated superior
predictive performance compared to established ICU scoring systems, including
APACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in
adapting its parameters to accommodate dataset-specific and task-specific
characteristics, its generalisation to common conditions under limited data
scenarios, and its rationality in selecting source conditions. These findings
highlight KnowRare's potential as a robust and practical solution for
supporting clinical decision-making and improving care for rare conditions in
the ICU.

</details>


### [35] [eegFloss: A Python package for refining sleep EEG recordings using machine learning models](https://arxiv.org/abs/2507.06433)
*Niloy Sikder,Paul Zerr,Mahdad Jafarzadeh Esfahani,Martin Dresler,Matthias Krauledat*

Main category: cs.LG

TL;DR: eegFloss是一个开源Python包，用于检测睡眠EEG记录中的伪影，提高睡眠研究的准确性。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受伪影干扰，影响自动睡眠分期的准确性，eegFloss旨在解决这一问题。

Method: 使用eegUsability机器学习模型检测伪影，并结合eegMobility模型自动检测卧床时间。

Result: eegUsability表现出高分类性能（F1-score约0.85），能准确识别可用EEG数据（召回率约94%）。

Conclusion: eegFloss通过减少伪影干扰，提升了睡眠研究的分析精度和结果可靠性。

Abstract: Electroencephalography (EEG) allows monitoring of brain activity, providing
insights into the functional dynamics of various brain regions and their roles
in cognitive processes. EEG is a cornerstone in sleep research, serving as the
primary modality of polysomnography, the gold standard in the field. However,
EEG signals are prone to artifacts caused by both internal (device-specific)
factors and external (environmental) interferences. As sleep studies are
becoming larger, most rely on automatic sleep staging, a process highly
susceptible to artifacts, leading to erroneous sleep scores. This paper
addresses this challenge by introducing eegFloss, an open-source Python package
to utilize eegUsability, a novel machine learning (ML) model designed to detect
segments with artifacts in sleep EEG recordings. eegUsability has been trained
and evaluated on manually artifact-labeled EEG data collected from 15
participants over 127 nights using the Zmax headband. It demonstrates solid
overall classification performance (F1-score is approximately 0.85, Cohens
kappa is 0.78), achieving a high recall rate of approximately 94% in
identifying channel-wise usable EEG data, and extends beyond Zmax.
Additionally, eegFloss offers features such as automatic time-in-bed detection
using another ML model named eegMobility, filtering out certain artifacts, and
generating hypnograms and sleep statistics. By addressing a fundamental
challenge faced by most sleep studies, eegFloss can enhance the precision and
rigor of their analysis as well as the accuracy and reliability of their
outcomes.

</details>


### [36] [Can Interpretation Predict Behavior on Unseen Data?](https://arxiv.org/abs/2507.06445)
*Victoria R. Li,Jenny Kaufmann,Martin Wattenberg,David Alvarez-Melis,Naomi Saphra*

Main category: cs.LG

TL;DR: 论文探讨了可解释性研究在预测模型在未见数据（OOD）上行为的潜力与挑战，通过分析Transformer模型的注意力模式与OOD泛化的关系，发现简单的可解释性工具可以预测OOD性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索可解释性是否能预测模型在未见数据上的行为，填补现有研究在预测OOD行为上的空白。

Method: 在合成分类任务上独立训练数百个Transformer模型，分析其注意力模式与OOD泛化的相关性。

Result: 发现当模型在分布内数据上表现出层次化注意力模式时，其在OOD数据上也可能层次化泛化。

Conclusion: 研究为可解释性工具预测未见模型行为提供了概念验证，鼓励进一步研究。

Abstract: Interpretability research often aims to predict how a model will respond to
targeted interventions on specific mechanisms. However, it rarely predicts how
a model will respond to unseen input data. This paper explores the promises and
challenges of interpretability as a tool for predicting out-of-distribution
(OOD) model behavior. Specifically, we investigate the correspondence between
attention patterns and OOD generalization in hundreds of Transformer models
independently trained on a synthetic classification task. These models exhibit
several distinct systematic generalization rules OOD, forming a diverse
population for correlational analysis. In this setting, we find that simple
observational tools from interpretability can predict OOD performance. In
particular, when in-distribution attention exhibits hierarchical patterns, the
model is likely to generalize hierarchically on OOD data -- even when the
rule's implementation does not rely on these hierarchical patterns, according
to ablation tests. Our findings offer a proof-of-concept to motivate further
interpretability work on predicting unseen model behavior.

</details>


### [37] [FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models](https://arxiv.org/abs/2507.06449)
*Qianyu Long,Qiyuan Wang,Christos Anagnostopoulos,Daning Bi*

Main category: cs.LG

TL;DR: FedPhD是一种新颖的联邦学习方法，专注于高效训练扩散模型（DMs），通过分层联邦学习和同质性感知模型聚合解决数据异质性和高通信成本问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）在联邦学习（FL）环境中训练时面临数据异质性和高通信成本的挑战，现有研究对此关注不足。

Method: FedPhD采用分层联邦学习，结合同质性感知模型聚合和选择策略，并通过分布式结构化剪枝提升计算效率和减少客户端存储需求。

Result: 实验表明，FedPhD在多个数据集上显著降低通信成本（高达88%），同时FID分数提升至少34%，仅使用56%的计算和通信资源。

Conclusion: FedPhD为FL环境中高效训练DMs提供了一种有效解决方案，显著提升了性能并降低了资源消耗。

Abstract: Federated Learning (FL), as a distributed learning paradigm, trains models
over distributed clients' data. FL is particularly beneficial for distributed
training of Diffusion Models (DMs), which are high-quality image generators
that require diverse data. However, challenges such as high communication costs
and data heterogeneity persist in training DMs similar to training Transformers
and Convolutional Neural Networks. Limited research has addressed these issues
in FL environments. To address this gap and challenges, we introduce a novel
approach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD
leverages Hierarchical FL with homogeneity-aware model aggregation and
selection policy to tackle data heterogeneity while reducing communication
costs. The distributed structured pruning of FedPhD enhances computational
efficiency and reduces model storage requirements in clients. Our experiments
across multiple datasets demonstrate that FedPhD achieves high model
performance regarding Fr\'echet Inception Distance (FID) scores while reducing
communication costs by up to $88\%$. FedPhD outperforms baseline methods
achieving at least a $34\%$ improvement in FID, while utilizing only $56\%$ of
the total computation and communication resources.

</details>


### [38] [Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models](https://arxiv.org/abs/2507.06458)
*Arjun Banerjee,David Martinez,Camille Dang,Ethan Tam*

Main category: cs.LG

TL;DR: 本文提出了一种自动化框架，用于为蛋白质语言模型（PLM）中的每个神经元提供生物学基础的自然语言描述，并开发了一种基于神经元激活的引导方法，以生成具有目标特性的蛋白质。


<details>
  <summary>Details</summary>
Motivation: 理解PLM内部神经元的生物学意义，并利用这些信息指导蛋白质设计。

Method: 引入自动化框架标注神经元，开发神经元激活引导方法生成目标蛋白质。

Result: 揭示了神经元对多样生化特性的选择性敏感，实现了目标生化特性和结构基序的生成。

Conclusion: 该方法不仅揭示了PLM的神经元分布规律，还为蛋白质设计提供了新工具。

Abstract: Protein language models (PLMs) encode rich biological information, yet their
internal neuron representations are poorly understood. We introduce the first
automated framework for labeling every neuron in a PLM with biologically
grounded natural language descriptions. Unlike prior approaches relying on
sparse autoencoders or manual annotation, our method scales to hundreds of
thousands of neurons, revealing individual neurons are selectively sensitive to
diverse biochemical and structural properties. We then develop a novel neuron
activation-guided steering method to generate proteins with desired traits,
enabling convergence to target biochemical properties like molecular weight and
instability index as well as secondary and tertiary structural motifs,
including alpha helices and canonical Zinc Fingers. We finally show that
analysis of labeled neurons in different model sizes reveals PLM scaling laws
and a structured neuron space distribution.

</details>


### [39] [Energy-Efficient Supervised Learning with a Binary Stochastic Forward-Forward Algorithm](https://arxiv.org/abs/2507.06461)
*Risi Jaiswal,Supriyo Datta,Joseph G. Makin*

Main category: cs.LG

TL;DR: 论文提出了一种基于前向-前向算法的二进制随机单元训练方法，显著降低了能量消耗，同时保持了接近实数算法的性能。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习因大规模神经网络的高能耗问题亟需解决方案，传统反向传播算法在硬件加速上存在挑战。

Method: 采用二进制随机单元的前向-前向算法，通过激活二值化和随机性优化矩阵乘法，利用p-bit硬件高效实现。

Result: 在MNIST、Fashion-MNIST和CIFAR-10数据集上表现接近实数算法，能耗降低约一个数量级。

Conclusion: 二进制随机单元的前向-前向算法是一种高效节能的替代方案，适合硬件加速。

Abstract: Reducing energy consumption has become a pressing need for modern machine
learning, which has achieved many of its most impressive results by scaling to
larger and more energy-consumptive neural networks. Unfortunately, the main
algorithm for training such networks, backpropagation, poses significant
challenges for custom hardware accelerators, due to both its serial
dependencies and the memory footprint needed to store forward activations for
the backward pass. Alternatives to backprop, although less effective, do exist;
here the main computational bottleneck becomes matrix multiplication. In this
study, we derive forward-forward algorithms for binary, stochastic units.
Binarization of the activations transforms matrix multiplications into indexing
operations, which can be executed efficiently in hardware. Stochasticity,
combined with tied weights across units with different biases, bypasses the
information bottleneck imposed by binary units. Furthermore, although slow and
expensive in traditional hardware, binary sampling that is very fast can be
implemented cheaply with p-bits (probabilistic bits), novel devices made up of
unstable magnets. We evaluate our proposed algorithms on the MNIST,
Fashion-MNIST, and CIFAR-10 datasets, showing that its performance is close to
real-valued forward-forward, but with an estimated energy savings of about one
order of magnitude.

</details>


### [40] [SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam](https://arxiv.org/abs/2507.06464)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Wen Gao*

Main category: cs.LG

TL;DR: 论文提出了一种名为SignSoftSGD（S3）的新型优化器，通过改进Adam的机制，解决了其在大梯度波动下的不稳定性问题，并提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: Adam在训练深度神经网络中表现出色，但其成功机制和局限性尚未充分探索。本文旨在通过分析Adam的机制，提出改进方案以增强其优势并减少局限性。

Method: 提出SignSoftSGD（S3）优化器，采用三方面创新：1）使用灵活的p阶动量；2）统一指数移动平均系数以减少损失峰值；3）集成Nesterov加速梯度模块。

Result: S3在非凸随机优化中达到最优收敛率，实验表明其收敛更快、性能更优，且在高学习率下仍保持稳定。

Conclusion: S3在效率和最终任务性能上均优于AdamW，证明了其作为优化器的有效性。

Abstract: Adam has proven remarkable successful in training deep neural networks, but
the mechanisms underlying its empirical successes and limitations remain
underexplored. In this study, we demonstrate that the effectiveness of Adam
stems largely from its similarity to SignSGD in robustly handling large
gradient fluctuations, yet it is also vulnerable to destabilizing loss spikes
due to its uncontrolled update scaling. To enhance the advantage of Adam and
mitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with
three key innovations. \emph{First}, S3 generalizes the sign-like update by
employing a flexible $p$-th order momentum ($p \geq 1$) in the denominator,
departing from the conventional second-order momentum (variance)
preconditioning. This design enables enhanced performance while achieving
stable training even with aggressive learning rates. \emph{Second}, S3
minimizes the occurrences of loss spikes through unified exponential moving
average coefficients for numerator and denominator momenta, which inherently
bound updates to $[-1, 1]$ and simplify hyperparameter tuning. \emph{Third}, S3
incorporates an equivalent Nesterov's accelerated gradient(NAG) module,
accelerating convergence without memory overhead. Theoretically, we prove that
S3 achieves the optimal convergence rate of
$O\left(\frac{1}{T^{\sfrac{1}{4}}}\right)$ for general nonconvex stochastic
optimization under weak assumptions. Extensive experiments across a range of
vision and language tasks show that \textsf{\small S3} not only converges more
rapidly and improves performance but also rarely experiences loss spikes, even
with a \textbf{$\bm{10 \times}$} larger learning rate. In fact, S3 delivers
performance comparable to or better than AdamW with \textbf{$2 \times$} the
training steps, establishing its efficacy in both efficiency and final task
performance.

</details>


### [41] [Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models](https://arxiv.org/abs/2507.06466)
*Aaron Dharna,Cong Lu,Jeff Clune*

Main category: cs.LG

TL;DR: FMSP利用基础模型的能力改进自博弈算法，克服局部最优问题，提出三种方法（vFMSP、NSSP、QDSP），在Car Tag和Gandalf任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自博弈算法（SP）容易陷入局部最优且缺乏多样性，基础模型（FMs）的代码生成能力和广泛知识为解决这些问题提供了新思路。

Method: 提出三种FMSP方法：vFMSP通过竞争自博弈优化策略；NSSP专注于策略多样性；QDSP结合多样性和高质量策略。

Result: 在Car Tag中，FMSP探索了多种方法并超越人工策略；在Gandalf中，FMSP成功攻破LLM防御并自动修复漏洞。

Conclusion: FMSP为基础模型改进自博弈开辟了新方向，为策略发现提供了更开放和创新的途径。

Abstract: Multi-agent interactions have long fueled innovation, from natural
predator-prey dynamics to the space race. Self-play (SP) algorithms try to
harness these dynamics by pitting agents against ever-improving opponents,
thereby creating an implicit curriculum toward learning high-quality solutions.
However, SP often fails to produce diverse solutions and can get stuck in
locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a
new direction that leverages the code-generation capabilities and vast
knowledge of foundation models (FMs) to overcome these challenges by leaping
across local optima in policy space. We propose a family of approaches: (1)
\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent
policies via competitive self-play; (2) \textbf{Novelty-Search Self-Play
(NSSP)} builds a diverse population of strategies, ignoring performance; and
(3) the most promising variant, \textbf{Quality-Diveristy Self-Play (QDSP)},
creates a diverse set of high-quality policies by combining the diversity of
NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a
continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety
simulation in which an attacker tries to jailbreak an LLM's defenses. In Car
Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and
heuristic-based methods, to name just a few. In terms of discovered policy
quality, \ouralgo and vFMSP surpass strong human-designed strategies. In
Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through
and jailbreaking six different, progressively stronger levels of defense.
Furthermore, FMSPs can automatically proceed to patch the discovered
vulnerabilities. Overall, FMSPs represent a promising new research frontier of
improving self-play with foundation models, opening fresh paths toward more
creative and open-ended strategy discovery

</details>


### [42] [Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning](https://arxiv.org/abs/2507.06469)
*Yudan Song,Yuecen Wei,Yuhang Lu,Qingyun Sun,Minglai Shao,Li-e Wang,Chunming Hu,Xianxian Li,Xingcheng Fu*

Main category: cs.LG

TL;DR: 论文提出了一种双视图图表示学习方法（MimbFD），用于解决欺诈检测中因拓扑和类别不平衡导致的信息传播问题，并通过实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有图表示学习方法在欺诈检测中因局部交互和类别不平衡导致全局拓扑信息传播不均，节点信息易被淹没。

Method: 设计了拓扑消息可达性模块和局部混淆去偏模块，分别用于穿透欺诈者伪装和平衡类别影响。

Result: 在三个公开欺诈数据集上的实验表明，MimbFD在欺诈检测中表现优异。

Conclusion: MimbFD通过双视图方法有效缓解了信息不平衡问题，提升了欺诈检测性能。

Abstract: Graph representation learning has become a mainstream method for fraud
detection due to its strong expressive power, which focuses on enhancing node
representations through improved neighborhood knowledge capture. However, the
focus on local interactions leads to imbalanced transmission of global
topological information and increased risk of node-specific information being
overwhelmed during aggregation due to the imbalance between fraud and benign
nodes. In this paper, we first summarize the impact of topology and class
imbalance on downstream tasks in GNN-based fraud detection, as the problem of
imbalanced supervisory messages is caused by fraudsters' topological behavior
obfuscation and identity feature concealment. Based on statistical validation,
we propose a novel dual-view graph representation learning method to mitigate
Message imbalance in Fraud Detection(MimbFD). Specifically, we design a
topological message reachability module for high-quality node representation
learning to penetrate fraudsters' camouflage and alleviate insufficient
propagation. Then, we introduce a local confounding debiasing module to adjust
node representations, enhancing the stable association between node
representations and labels to balance the influence of different classes.
Finally, we conducted experiments on three public fraud datasets, and the
results demonstrate that MimbFD exhibits outstanding performance in fraud
detection.

</details>


### [43] [FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning](https://arxiv.org/abs/2507.06482)
*Huan Wang,Haoran Li,Huaming Chen,Jun Yan,Jiahua Shi,Jun Shen*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedDifRC的新方法，通过引入扩散模型来解决联邦学习中的数据异质性问题，利用文本驱动的对比学习和噪声驱动的一致性正则化提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异质性问题导致模型收敛和性能下降，需要一种新方法来缓解这一问题。

Method: 提出FedDifRC方法，结合扩散模型的文本驱动对比学习和噪声驱动一致性正则化，以提供丰富的语义信息和一致的收敛信号。

Result: 实验验证了FedDifRC的有效性，并展示了关键组件的高效性。

Conclusion: FedDifRC通过扩散模型的指导成功缓解了数据异质性问题，并在理论和实验上均表现出色。

Abstract: Federated learning aims at training models collaboratively across
participants while protecting privacy. However, one major challenge for this
paradigm is the data heterogeneity issue, where biased data preferences across
multiple clients, harming the model's convergence and performance. In this
paper, we first introduce powerful diffusion models into the federated learning
paradigm and show that diffusion representations are effective steers during
federated training. To explore the possibility of using diffusion
representations in handling data heterogeneity, we propose a novel
diffusion-inspired Federated paradigm with Diffusion Representation
Collaboration, termed FedDifRC, leveraging meaningful guidance of diffusion
models to mitigate data heterogeneity. The key idea is to construct text-driven
diffusion contrasting and noise-driven diffusion regularization, aiming to
provide abundant class-related semantic information and consistent convergence
signals. On the one hand, we exploit the conditional feedback from the
diffusion model for different text prompts to build a text-driven contrastive
learning strategy. On the other hand, we introduce a noise-driven consistency
regularization to align local instances with diffusion denoising
representations, constraining the optimization region in the feature space. In
addition, FedDifRC can be extended to a self-supervised scheme without relying
on any labeled data. We also provide a theoretical analysis for FedDifRC to
ensure convergence under non-convex objectives. The experiments on different
scenarios validate the effectiveness of FedDifRC and the efficiency of crucial
components.

</details>


### [44] [MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models](https://arxiv.org/abs/2507.06502)
*Yiwen Liu,Chenyu Zhang,Junjie Song,Siqi Chen,Sun Yin,Zihan Wang,Lingming Zeng,Yuji Cao,Junming Jiao*

Main category: cs.LG

TL;DR: MoFE-Time是一种创新的时间序列预测模型，通过结合时间和频率域特征，在MoE网络中实现高性能预测。


<details>
  <summary>Details</summary>
Motivation: 现有模型在预训练-微调范式中未能同时建模时间和频率特征，导致复杂时间序列预测性能不佳。

Method: 提出MoFE-Time模型，集成时间和频率域特征，利用MoE路由机制构建多维稀疏表示。

Result: 在六个公共基准测试中达到最新性能，MSE和MAE分别降低6.95%和6.02%。

Conclusion: MoFE-Time在理论和实际应用中均表现出色，验证了其有效性。

Abstract: As a prominent data modality task, time series forecasting plays a pivotal
role in diverse applications. With the remarkable advancements in Large
Language Models (LLMs), the adoption of LLMs as the foundational architecture
for time series modeling has gained significant attention. Although existing
models achieve some success, they rarely both model time and frequency
characteristics in a pretraining-finetuning paradigm leading to suboptimal
performance in predictions of complex time series, which requires both modeling
periodicity and prior pattern knowledge of signals. We propose MoFE-Time, an
innovative time series forecasting model that integrates time and frequency
domain features within a Mixture of Experts (MoE) network. Moreover, we use the
pretraining-finetuning paradigm as our training framework to effectively
transfer prior pattern knowledge across pretraining and finetuning datasets
with different periodicity distributions. Our method introduces both frequency
and time cells as experts after attention modules and leverages the MoE routing
mechanism to construct multidimensional sparse representations of input
signals. In experiments on six public benchmarks, MoFE-Time has achieved new
state-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared
to the representative methods Time-MoE. Beyond the existing evaluation
benchmarks, we have developed a proprietary dataset, NEV-sales, derived from
real-world business scenarios. Our method achieves outstanding results on this
dataset, underscoring the effectiveness of the MoFE-Time model in practical
commercial applications.

</details>


### [45] [Instance-Wise Monotonic Calibration by Constrained Transformation](https://arxiv.org/abs/2507.06516)
*Yunrui Zhang,Gustavo Batista,Salil S. Kanhere*

Main category: cs.LG

TL;DR: 论文提出了一种新的单调后校准方法，通过线性参数化的约束校准映射，确保表达性、鲁棒性和可解释性，同时保持概率输出的相对顺序。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络常产生校准错误的概率估计，导致预测过于自信。现有后校准方法大多无法保证单调性，或牺牲表达能力与可解释性。

Method: 提出一种线性参数化的约束校准映射，将其建模为约束优化问题，确保单调性、表达性和鲁棒性。

Result: 在多个数据集和深度神经网络模型上，该方法优于现有校准方法，且数据与计算效率高。

Conclusion: 该方法在保持概率输出顺序的同时，实现了高性能、鲁棒性和可解释性，为后校准提供了新思路。

Abstract: Deep neural networks often produce miscalibrated probability estimates,
leading to overconfident predictions. A common approach for calibration is
fitting a post-hoc calibration map on unseen validation data that transforms
predicted probabilities. A key desirable property of the calibration map is
instance-wise monotonicity (i.e., preserving the ranking of probability
outputs). However, most existing post-hoc calibration methods do not guarantee
monotonicity. Previous monotonic approaches either use an under-parameterized
calibration map with limited expressive ability or rely on black-box neural
networks, which lack interpretability and robustness. In this paper, we propose
a family of novel monotonic post-hoc calibration methods, which employs a
constrained calibration map parameterized linearly with respect to the number
of classes. Our proposed approach ensures expressiveness, robustness, and
interpretability while preserving the relative ordering of the probability
output by formulating the proposed calibration map as a constrained
optimization problem. Our proposed methods achieve state-of-the-art performance
across datasets with different deep neural network models, outperforming
existing calibration methods while being data and computation-efficient. Our
code is available at
https://github.com/YunruiZhang/Calibration-by-Constrained-Transformation

</details>


### [46] [AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks](https://arxiv.org/abs/2507.06525)
*Huiqi Zhang,Fang Xie*

Main category: cs.LG

TL;DR: AdaDPIGU是一种针对深度神经网络的新型差分隐私SGD框架，通过重要性梯度更新和高维稀疏化，显著提升了隐私保护下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私方法在高维设置下性能下降，噪声随维度增加而增大，亟需一种更高效的隐私保护机制。

Method: 提出AdaDPIGU框架，包括预训练阶段的参数重要性估计和梯度更新阶段的稀疏化与自适应裁剪机制。

Result: 在MNIST和CIFAR-10数据集上，AdaDPIGU在隐私预算下表现优异，甚至超越非隐私基线。

Conclusion: AdaDPIGU通过自适应稀疏化同时提升隐私保护和模型性能，为高维数据提供了高效解决方案。

Abstract: Differential privacy has been proven effective for stochastic gradient
descent; however, existing methods often suffer from performance degradation in
high-dimensional settings, as the scale of injected noise increases with
dimensionality. To tackle this challenge, we propose AdaDPIGU--a new
differentially private SGD framework with importance-based gradient updates
tailored for deep neural networks. In the pretraining stage, we apply a
differentially private Gaussian mechanism to estimate the importance of each
parameter while preserving privacy. During the gradient update phase, we prune
low-importance coordinates and introduce a coordinate-wise adaptive clipping
mechanism, enabling sparse and noise-efficient gradient updates. Theoretically,
we prove that AdaDPIGU satisfies $(\varepsilon, \delta)$-differential privacy
and retains convergence guarantees. Extensive experiments on standard
benchmarks validate the effectiveness of AdaDPIGU. All results are reported
under a fixed retention ratio of 60%. On MNIST, our method achieves a test
accuracy of 99.12% under a privacy budget of $\epsilon = 8$, nearly matching
the non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at
$\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating
that adaptive sparsification can enhance both privacy and utility.

</details>


### [47] [Direct Regret Optimization in Bayesian Optimization](https://arxiv.org/abs/2507.06529)
*Fengxue Zhang,Yuxin Chen*

Main category: cs.LG

TL;DR: 提出了一种新的贝叶斯优化方法，通过联合学习最优模型和非近视采集策略，直接优化多步遗憾。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化方法依赖手工设计的采集函数和代理模型，且通常是近视的。本文旨在解决这些问题。

Method: 使用高斯过程（GPs）集合生成模拟轨迹，训练端到端决策变换器，采用离线密集训练和在线稀疏学习策略。

Result: 在合成和真实基准测试中，该方法表现优于基线，实现了更低的简单遗憾和更鲁棒的探索。

Conclusion: 该方法通过联合学习和非近视策略，显著提升了贝叶斯优化的性能。

Abstract: Bayesian optimization (BO) is a powerful paradigm for optimizing expensive
black-box functions. Traditional BO methods typically rely on separate
hand-crafted acquisition functions and surrogate models for the underlying
function, and often operate in a myopic manner. In this paper, we propose a
novel direct regret optimization approach that jointly learns the optimal model
and non-myopic acquisition by distilling from a set of candidate models and
acquisitions, and explicitly targets minimizing the multi-step regret. Our
framework leverages an ensemble of Gaussian Processes (GPs) with varying
hyperparameters to generate simulated BO trajectories, each guided by an
acquisition function chosen from a pool of conventional choices, until a
Bayesian early stop criterion is met. These simulated trajectories, capturing
multi-step exploration strategies, are used to train an end-to-end decision
transformer that directly learns to select next query points aimed at improving
the ultimate objective. We further adopt a dense training--sparse learning
paradigm: The decision transformer is trained offline with abundant simulated
data sampled from ensemble GPs and acquisitions, while a limited number of real
evaluations refine the GPs online. Experimental results on synthetic and
real-world benchmarks suggest that our method consistently outperforms BO
baselines, achieving lower simple regret and demonstrating more robust
exploration in high-dimensional or noisy settings.

</details>


### [48] [A Single Merging Suffices: Recovering Server-based Learning Performance in Decentralized Learning](https://arxiv.org/abs/2507.06542)
*Tongtian Zhu,Tianyu Zhang,Mingze Wang,Zhanpeng Zhou,Can Wang*

Main category: cs.LG

TL;DR: 研究去中心化学习中通信调度对性能的影响，发现后期集中通信和最终全局合并能显著提升泛化性能，并挑战了去中心化学习在数据异构和有限通信下泛化能力差的常见观点。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习因通信受限性能不佳，研究如何优化通信调度以提升性能。

Method: 通过实验和理论分析，研究通信调度策略，包括通信时机和频率，以及最终全局合并的效果。

Result: 后期集中通信和最终全局合并能匹配服务器训练性能，低通信下本地模型仍可合并。理论证明去中心化SGD的全局合并模型收敛速度可能快于集中式SGD。

Conclusion: 去中心化学习在优化通信调度下表现优异，挑战了传统观点，为模型合并和神经网络损失景观提供了新见解。

Abstract: Decentralized learning provides a scalable alternative to traditional
parameter-server-based training, yet its performance is often hindered by
limited peer-to-peer communication. In this paper, we study how communication
should be scheduled over time, including determining when and how frequently
devices synchronize. Our empirical results show that concentrating
communication budgets in the later stages of decentralized training markedly
improves global generalization. Surprisingly, we uncover that fully connected
communication at the final step, implemented by a single global merging, is
sufficient to match the performance of server-based training. We further show
that low communication in decentralized learning preserves the
\textit{mergeability} of local models throughout training. Our theoretical
contributions, which explains these phenomena, are first to establish that the
globally merged model of decentralized SGD can converge faster than centralized
mini-batch SGD. Technically, we novelly reinterpret part of the discrepancy
among local models, which were previously considered as detrimental noise, as
constructive components that accelerate convergence. This work challenges the
common belief that decentralized learning generalizes poorly under data
heterogeneity and limited communication, while offering new insights into model
merging and neural network loss landscapes.

</details>


### [49] [The Primacy of Magnitude in Low-Rank Adaptation](https://arxiv.org/abs/2507.06558)
*Zicheng Zhang,Haoran Li,Yifeng Zhang,Guoqiang Gong,Jiaxing Wang,Pengzhang Liu,Qixia Jiang,Junxing Hu*

Main category: cs.LG

TL;DR: LoRAM是一种基于更新幅度的初始化方案，通过模拟频谱增益提升性能，同时保持LoRA的高效性。


<details>
  <summary>Details</summary>
Motivation: 现有频谱初始化方法虽提升性能，但计算和存储开销大，LoRAM旨在解决这一问题。

Method: 提出LoRAM，利用预训练权重幅度缩放确定性正交基，模拟频谱增益。

Result: LoRAM在保持高效的同时，性能匹配或优于频谱初始化。

Conclusion: 更新幅度是LoRA性能的关键驱动因素，LoRAM提供了一种高效且性能优越的初始化方案。

Abstract: Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning
large models. While recent spectral initialization methods improve convergence
and performance over the naive "Noise & Zeros" scheme, their extra
computational and storage overhead undermines efficiency. In this paper, we
establish update magnitude as the fundamental driver of LoRA performance and
propose LoRAM, a magnitude-driven "Basis & Basis" initialization scheme that
matches spectral methods without their inefficiencies. Our key contributions
are threefold: (i) Magnitude of weight updates determines convergence. We prove
low-rank structures intrinsically bound update magnitudes, unifying
hyperparameter tuning in learning rate, scaling factor, and initialization as
mechanisms to optimize magnitude regulation. (ii) Spectral initialization
succeeds via magnitude amplification. We demystify that the presumed
knowledge-driven benefit of the spectral component essentially arises from the
boost in the weight update magnitude. (iii) A novel and compact initialization
strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight
magnitudes to simulate spectral gains. Extensive experiments show that LoRAM
serves as a strong baseline, retaining the full efficiency of LoRA while
matching or outperforming spectral initialization across benchmarks.

</details>


### [50] [SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference](https://arxiv.org/abs/2507.06567)
*Qian Chen,Xianhao Chen,Kaibin Huang*

Main category: cs.LG

TL;DR: 该论文提出了一种优化边缘网络中专家缓存的方法，以减少混合专家模型（MoE）的推理延迟。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型中专家网络数量庞大导致的存储负担和推理延迟问题。

Method: 通过贪婪算法和动态规划方法优化专家缓存，并设计加速算法。

Result: 仿真结果表明，该方法显著降低了推理延迟。

Conclusion: 提出的方法在边缘网络中有效优化了MoE模型的推理性能。

Abstract: Mixture-of-Experts (MoE) models improve the scalability of large language
models (LLMs) by activating only a small subset of relevant experts per input.
However, the sheer number of expert networks in an MoE model introduces a
significant storage burden for an edge device. To address this challenge, we
consider a scenario where experts are dispersed within an edge network for
distributed inference. Based on the popular Top-$K$ expert selection strategy,
we formulate a latency minimization problem by optimizing expert caching on
edge servers under storage constraints. When $K=1$, the problem reduces to a
monotone submodular maximization problem with knapsack constraints, for which
we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.
For the general case where $K\geq1$, expert co-activation within the same MoE
layer introduces non-submodularity, causing greedy methods to be ineffective.
To tackle this issue, we propose a successive greedy decomposition method to
decompose the original problem into a series of subproblems, with each being
solved by a dynamic programming approach. Furthermore, we design an accelerated
algorithm based on the max-convolution technique to obtain the approximate
solution with a provable guarantee in polynomial time. Simulation results on
various MoE models demonstrate that our method significantly reduces inference
latency compared to existing baselines.

</details>


### [51] [From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization](https://arxiv.org/abs/2507.06573)
*Xinjie Chen,Minpeng Liao,Guoxin Chen,Chengxi Li,Biao Fu,Kai Fan,Xinggao Liu*

Main category: cs.LG

TL;DR: 论文提出了一种名为LPPO的框架，通过前缀引导采样和学习进度加权技术，优化强化学习中的样本利用效率，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何高效利用少量高质量示范数据，而非简单增加数据量，以提升模型的推理能力。

Method: 提出前缀引导采样和学习进度加权两种技术，分别通过专家示范的部分解前缀引导策略，以及动态调整样本权重以促进学习。

Result: 在数学推理基准测试中，方法表现优于基线，实现了更快的收敛速度和更高的性能上限。

Conclusion: LPPO框架通过样本优化技术，显著提升了强化学习模型的性能和效率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently advanced
the reasoning capabilities of large language models (LLMs). While prior work
has emphasized algorithmic design, data curation, and reward shaping, we
investigate RLVR from a sample-centric perspective and introduce LPPO
(Learning-Progress and Prefix-guided Optimization), a framework of progressive
optimization techniques. Our work addresses a critical question: how to best
leverage a small set of trusted, high-quality demonstrations, rather than
simply scaling up data volume. First, motivated by how hints aid human
problem-solving, we propose prefix-guided sampling, an online data augmentation
method that incorporates partial solution prefixes from expert demonstrations
to guide the policy, particularly for challenging instances. Second, inspired
by how humans focus on important questions aligned with their current
capabilities, we introduce learning-progress weighting, a dynamic strategy that
adjusts each training sample's influence based on model progression. We
estimate sample-level learning progress via an exponential moving average of
per-sample pass rates, promoting samples that foster learning and
de-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks
demonstrate that our methods outperform strong baselines, yielding faster
convergence and a higher performance ceiling.

</details>


### [52] [Learning controllable dynamics through informative exploration](https://arxiv.org/abs/2507.06582)
*Peter N. Loxley,Friedrich T. Sommer*

Main category: cs.LG

TL;DR: 研究提出了一种基于“预测信息增益”的方法，用于确定环境中最具信息量的探索区域，并通过强化学习找到高效的探索策略。


<details>
  <summary>Details</summary>
Motivation: 在缺乏明确动态模型的环境中，探索是理解可控动态的关键。

Method: 使用“预测信息增益”作为信息度量，结合强化学习方法，寻找高效的探索策略。

Result: 与几种短视探索方法相比，该方法能更可靠地估计环境的可控动态。

Conclusion: 基于信息增益的探索策略在缺乏明确模型的环境中表现优异。

Abstract: Environments with controllable dynamics are usually understood in terms of
explicit models. However, such models are not always available, but may
sometimes be learned by exploring an environment. In this work, we investigate
using an information measure called "predicted information gain" to determine
the most informative regions of an environment to explore next. Applying
methods from reinforcement learning allows good suboptimal exploring policies
to be found, and leads to reliable estimates of the underlying controllable
dynamics. This approach is demonstrated by comparing with several myopic
exploration approaches.

</details>


### [53] [Generalization in Reinforcement Learning for Radio Access Networks](https://arxiv.org/abs/2507.06602)
*Burak Demirel,Yu Wang,Cristian Tatino,Pablo Soldati*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习（RL）的通用框架，用于解决现代无线接入网（RAN）中的资源管理问题，通过图注意力网络和域随机化提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代RAN环境动态且异构，传统基于规则的资源管理算法性能不足，而现有RL方法在泛化性上存在挑战。

Method: 采用注意力图表示网络拓扑和节点属性，结合域随机化扩展训练分布，并通过分布式数据生成和集中训练架构实现高效学习。

Result: 在5G基准测试中，该策略在吞吐量和频谱效率上显著优于基线方法，并在多场景下表现优异。

Conclusion: 该框架为AI原生的6G RAN提供了一种通用且可扩展的解决方案。

Abstract: Modern RAN operate in highly dynamic and heterogeneous environments, where
hand-tuned, rule-based RRM algorithms often underperform. While RL can surpass
such heuristics in constrained settings, the diversity of deployments and
unpredictable radio conditions introduce major generalization challenges.
Data-driven policies frequently overfit to training conditions, degrading
performance in unseen scenarios. To address this, we propose a
generalization-centered RL framework for RAN control that: (i) encodes cell
topology and node attributes via attention-based graph representations; (ii)
applies domain randomization to broaden the training distribution; and (iii)
distributes data generation across multiple actors while centralizing training
in a cloud-compatible architecture aligned with O-RAN principles. Although
generalization increases computational and data-management complexity, our
distributed design mitigates this by scaling data collection and training
across diverse network conditions. Applied to downlink link adaptation in five
5G benchmarks, our policy improves average throughput and spectral efficiency
by ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and
by >20% under high mobility. It matches specialized RL in full-buffer traffic
and achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,
respectively. In nine-cell deployments, GAT models offer 30% higher throughput
over MLP baselines. These results, combined with our scalable architecture,
offer a path toward AI-native 6G RAN using a single, generalizable RL agent.

</details>


### [54] [Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation](https://arxiv.org/abs/2507.06613)
*Anshuk Uppal,Yuhta Takida,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 论文提出了一种新的生成模型框架，通过结合不同β值的VAE和非线性扩散模型，平衡解耦和生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型中解耦表示与高质量重建之间的权衡问题。

Method: 使用单一VAE训练多个β值的潜在表示，并引入非线性扩散模型平滑过渡不同β值的表示。

Result: 实现了高质量重建和解耦表示，支持无输入图像的样本生成，并在潜在空间中观察到平滑过渡。

Conclusion: 该框架在解耦和生成质量上表现优异，为生成模型提供了灵活性和一致性。

Abstract: Disentangled and interpretable latent representations in generative models
typically come at the cost of generation quality. The $\beta$-VAE framework
introduces a hyperparameter $\beta$ to balance disentanglement and
reconstruction quality, where setting $\beta > 1$ introduces an information
bottleneck that favors disentanglement over sharp, accurate reconstructions. To
address this trade-off, we propose a novel generative modeling framework that
leverages a range of $\beta$ values to learn multiple corresponding latent
representations. First, we obtain a slew of representations by training a
single variational autoencoder (VAE), with a new loss function that controls
the information retained in each latent representation such that the higher
$\beta$ value prioritize disentanglement over reconstruction fidelity. We then,
introduce a non-linear diffusion model that smoothly transitions latent
representations corresponding to different $\beta$ values. This model denoises
towards less disentangled and more informative representations, ultimately
leading to (almost) lossless representations, enabling sharp reconstructions.
Furthermore, our model supports sample generation without input images,
functioning as a standalone generative model. We evaluate our framework in
terms of both disentanglement and generation quality. Additionally, we observe
smooth transitions in the latent spaces with respect to changes in $\beta$,
facilitating consistent manipulation of generated outputs.

</details>


### [55] [Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance](https://arxiv.org/abs/2507.06615)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: 论文提出了一种名为CTPG的新框架，通过跨任务策略指导加速技能获取，并引入了两种门控机制以提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注参数共享，但忽略了利用已掌握任务的策略直接指导未掌握任务的可能性。

Method: CTPG框架为每个任务训练一个指导策略，从所有任务的控制策略中选择行为策略，并引入两种门控机制优化学习效率。

Result: 实验表明，CTPG与现有参数共享方法结合后，在操作和运动基准测试中性能显著提升。

Conclusion: CTPG是一种通用框架，能有效利用跨任务相似性，提升多任务强化学习的效率。

Abstract: Multi-task reinforcement learning endeavors to efficiently leverage shared
information across various tasks, facilitating the simultaneous learning of
multiple tasks. Existing approaches primarily focus on parameter sharing with
carefully designed network structures or tailored optimization procedures.
However, they overlook a direct and complementary way to exploit cross-task
similarities: the control policies of tasks already proficient in some skills
can provide explicit guidance for unmastered tasks to accelerate skills
acquisition. To this end, we present a novel framework called Cross-Task Policy
Guidance (CTPG), which trains a guide policy for each task to select the
behavior policy interacting with the environment from all tasks' control
policies, generating better training trajectories. In addition, we propose two
gating mechanisms to improve the learning efficiency of CTPG: one gate filters
out control policies that are not beneficial for guidance, while the other gate
blocks tasks that do not necessitate guidance. CTPG is a general framework
adaptable to existing parameter sharing approaches. Empirical evaluations
demonstrate that incorporating CTPG with these approaches significantly
enhances performance in manipulation and locomotion benchmarks.

</details>


### [56] [Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000](https://arxiv.org/abs/2507.06619)
*Xiaobo Huang,Fang Xie*

Main category: cs.LG

TL;DR: 论文提出SAD-DPSGD方法，通过动态调整噪声和裁剪阈值，解决医疗图像分类中数据泄露问题，在HAM10000数据集上表现优于Auto-DPSGD。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分类中数据泄露问题严重，现有方法在小规模不平衡数据集（如HAM10000）上效果不佳，导致模型陷入次优解。

Method: 提出SAD-DPSGD，采用线性衰减机制动态调整噪声和裁剪阈值，初期分配更多隐私预算和更高裁剪阈值。

Result: 在HAM10000数据集上，SAD-DPSGD在ε=3.0、δ=10^-3条件下，准确率比Auto-DPSGD提高2.15%。

Conclusion: SAD-DPSGD有效解决了不平衡医疗数据集上的隐私保护问题，提升了模型性能。

Abstract: When applying machine learning to medical image classification, data leakage
is a critical issue. Previous methods, such as adding noise to gradients for
differential privacy, work well on large datasets like MNIST and CIFAR-100, but
fail on small, imbalanced medical datasets like HAM10000. This is because the
imbalanced distribution causes gradients from minority classes to be clipped
and lose crucial information, while majority classes dominate. This leads the
model to fall into suboptimal solutions early. To address this, we propose
SAD-DPSGD, which uses a linear decaying mechanism for noise and clipping
thresholds. By allocating more privacy budget and using higher clipping
thresholds in the initial training phases, the model avoids suboptimal
solutions and enhances performance. Experiments show that SAD-DPSGD outperforms
Auto-DPSGD on HAM10000, improving accuracy by 2.15% under $\epsilon = 3.0$ ,
$\delta = 10^{-3}$.

</details>


### [57] [UniOD: A Universal Model for Outlier Detection across Diverse Domains](https://arxiv.org/abs/2507.06624)
*Dazhi Fu,Jicong Fan*

Main category: cs.LG

TL;DR: UniOD是一个通用的异常检测框架，通过利用标记数据集训练单一模型，能够检测来自不同领域的异常数据，避免了模型选择和超参数调优的麻烦。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法需要针对特定数据集进行繁琐的超参数调优和模型训练，限制了其实际应用的便利性和效率。

Method: UniOD将每个数据集转换为多个图，生成一致的节点特征，并将异常检测任务转化为节点分类问题，从而能够泛化到未见过的领域。

Result: 在15个基准异常检测数据集上与15种最先进的基线方法相比，UniOD表现出色。

Conclusion: UniOD减少了计算成本，有效利用了历史数据集的知识，提高了实际应用中的便利性和准确性。

Abstract: Outlier detection (OD) seeks to distinguish inliers and outliers in
completely unlabeled datasets and plays a vital role in science and
engineering. Most existing OD methods require troublesome dataset-specific
hyperparameter tuning and costly model training before they can be deployed to
identify outliers. In this work, we propose UniOD, a universal OD framework
that leverages labeled datasets to train a single model capable of detecting
outliers of datasets from diverse domains. Specifically, UniOD converts each
dataset into multiple graphs, produces consistent node features, and frames
outlier detection as a node-classification task, and is able to generalize to
unseen domains. As a result, UniOD avoids effort on model selection and
hyperparameter tuning, reduces computational cost, and effectively utilizes the
knowledge from historical datasets, which improves the convenience and accuracy
in real applications. We evaluate UniOD on 15 benchmark OD datasets against 15
state-of-the-art baselines, demonstrating its effectiveness.

</details>


### [58] [Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2507.06628)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: GO-Skill是一种离线多任务强化学习方法，通过目标导向的技能提取和分层策略学习，提升知识共享和任务性能。


<details>
  <summary>Details</summary>
Motivation: 离线多任务强化学习面临跨任务知识共享的挑战，受人类学习启发，提出GO-Skill以提取和利用可重用技能。

Method: 通过目标导向技能提取和向量量化构建离散技能库，引入技能增强阶段优化技能，并通过分层策略学习动态协调技能。

Result: 在MetaWorld基准测试的多样化机器人操作任务中验证了GO-Skill的有效性和通用性。

Conclusion: GO-Skill通过技能抽象和分层策略学习，显著提升了离线多任务强化学习的性能。

Abstract: Offline multi-task reinforcement learning aims to learn a unified policy
capable of solving multiple tasks using only pre-collected task-mixed datasets,
without requiring any online interaction with the environment. However, it
faces significant challenges in effectively sharing knowledge across tasks.
Inspired by the efficient knowledge abstraction observed in human learning, we
propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed
to extract and utilize reusable skills to enhance knowledge transfer and task
performance. Our approach uncovers reusable skills through a goal-oriented
skill extraction process and leverages vector quantization to construct a
discrete skill library. To mitigate class imbalances between broadly applicable
and task-specific skills, we introduce a skill enhancement phase to refine the
extracted skills. Furthermore, we integrate these skills using hierarchical
policy learning, enabling the construction of a high-level policy that
dynamically orchestrates discrete skills to accomplish specific tasks.
Extensive experiments on diverse robotic manipulation tasks within the
MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.

</details>


### [59] [Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator](https://arxiv.org/abs/2507.06631)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 提出了一种基于网格数据的过拟合检测与预防方法，利用拉普拉斯算子二阶导数优化模型熵。


<details>
  <summary>Details</summary>
Motivation: 解决回归任务中数据过拟合问题，特别是在网格结构数据中，通过优化模型熵减少不必要的振荡。

Method: 在原始训练网格上计算导数作为真实标签，在交错网格上计算训练数据的导数以识别振荡，利用拉普拉斯算子导数损失进行超参数优化。

Result: 实现了通过最小化模型熵减少振荡的目标，无需从训练数据中拆分测试点。

Conclusion: 该方法有效减少了过拟合，并通过拉普拉斯算子在交错网格上的应用提供了一种基于扩散特性的测试指标。

Abstract: This document reports on a method for detecting and preventing overfitting on
data regressions, herein applied to mesh-like data structures. The mesh
structure allows for the straightforward computation of the Laplace-operator
second-order derivatives in a finite-difference fashion for noiseless data.
Derivatives of the training data are computed on the original training mesh to
serve as a true label of the entropy of the training data. Derivatives of the
trained data are computed on a staggered mesh to identify oscillations in the
interior of the original training mesh cells. The loss of the Laplace-operator
derivatives is used for hyperparameter optimisation, achieving a reduction of
unwanted oscillation through the minimisation of the entropy of the trained
model. In this setup, testing does not require the splitting of points from the
training data, and training is thus directly performed on all available
training points. The Laplace operator applied to the trained data on a
staggered mesh serves as a surrogate testing metric based on diffusion
properties.

</details>


### [60] [Deep Disentangled Representation Network for Treatment Effect Estimation](https://arxiv.org/abs/2507.06650)
*Hui Meng,Keping Yang,Xuyu Peng,Bo Zheng*

Main category: cs.LG

TL;DR: 提出了一种新的个体治疗效果估计算法，结合多头注意力机制和线性正交正则化器，通过软分解预处理变量并消除选择偏差。


<details>
  <summary>Details</summary>
Motivation: 解决观测数据中个体治疗效果估计问题，现有方法难以精确分解变量。

Method: 采用多头注意力机制和线性正交正则化器软分解变量，结合重要性采样重加权技术消除选择偏差。

Result: 在公开半合成和真实数据集上实验，算法优于现有方法。

Conclusion: 新算法在个体治疗效果估计中表现优异。

Abstract: Estimating individual-level treatment effect from observational data is a
fundamental problem in causal inference and has attracted increasing attention
in the fields of education, healthcare, and public policy.In this work, we
concentrate on the study of disentangled representation methods that have shown
promising outcomes by decomposing observed covariates into instrumental,
confounding, and adjustment factors. However, most of the previous work has
primarily revolved around generative models or hard decomposition methods for
covariates, which often struggle to guarantee the attainment of precisely
disentangled factors. In order to effectively model different causal
relationships, we propose a novel treatment effect estimation algorithm that
incorporates a mixture of experts with multi-head attention and a linear
orthogonal regularizer to softly decompose the pre-treatment variables, and
simultaneously eliminates selection bias via importance sampling re-weighting
techniques. We conduct extensive experiments on both public semi-synthetic and
real-world production datasets. The experimental results clearly demonstrate
that our algorithm outperforms the state-of-the-art methods focused on
individual treatment effects.

</details>


### [61] [Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making](https://arxiv.org/abs/2507.06652)
*Arthur Alexander Lim,Zhen Bin It,Jovan Bowen Heng,Tee Hui Teo*

Main category: cs.LG

TL;DR: 本文探讨如何通过机器学习和联邦学习改进模糊系统，以处理不确定性，并分析其潜在改进与局限性。


<details>
  <summary>Details</summary>
Motivation: 模糊系统能处理不确定性，但仍有改进空间，机器学习和联邦学习的新技术可为其提供灵感。

Method: 结合机器学习和联邦学习的理念，如更新模糊规则，以逐步优化模糊系统。

Result: 潜在改进显著，但需进一步研究验证其实际效果。

Conclusion: 改进模糊系统的潜力存在，但需更多研究以确定其范围和效果。

Abstract: Fuzzy systems are a way to allow machines, systems and frameworks to deal
with uncertainty, which is not possible in binary systems that most computers
use. These systems have already been deployed for certain use cases, and fuzzy
systems could be further improved as proposed in this paper. Such technologies
to draw inspiration from include machine learning and federated learning.
Machine learning is one of the recent breakthroughs of technology and could be
applied to fuzzy systems to further improve the results it produces. Federated
learning is also one of the recent technologies that have huge potential, which
allows machine learning training to improve by reducing privacy risk, reducing
burden on networking infrastructure, and reducing latency of the latest model.
Aspects from federated learning could be used to improve federated learning,
such as applying the idea of updating the fuzzy rules that make up a key part
of fuzzy systems, to further improve it over time. This paper discusses how
these improvements would be implemented in fuzzy systems, and how it would
improve fuzzy systems. It also discusses certain limitations on the potential
improvements. It concludes that these proposed ideas and improvements require
further investigation to see how far the improvements are, but the potential is
there to improve fuzzy systems.

</details>


### [62] [Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement](https://arxiv.org/abs/2507.06701)
*Michael Bloesch,Markus Wulfmeier,Philemon Brakel,Todor Davchev,Martina Zambelli,Jost Tobias Springenberg,Abbas Abdolmaleki,William F Whitney,Nicolas Heess,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: 该论文提出了一种改进的模仿学习观察方法（IfO），通过利用无动作演示数据，避免了传统方法对动作标记或奖励函数的需求，并研究了更复杂的数据分布。


<details>
  <summary>Details</summary>
Motivation: 传统IfO研究局限于理想化的双峰数据分布，限制了结果的实用性。本文旨在探索更复杂的数据分布，并提出一种方法，使模仿学习能够通过自我改进迭代进行。

Method: 该方法将基于强化学习的模仿学习适应于无动作演示，利用价值函数在专家和非专家数据之间传递信息。

Result: 通过全面评估，论文揭示了不同数据分布与算法适用性之间的关系，并指出了现有方法的局限性。

Conclusion: 研究结果为开发更鲁棒和实用的IfO技术提供了宝贵见解，为实现可扩展的行为学习铺平了道路。

Abstract: Imitation Learning from Observation (IfO) offers a powerful way to learn
behaviors at large-scale: Unlike behavior cloning or offline reinforcement
learning, IfO can leverage action-free demonstrations and thus circumvents the
need for costly action-labeled demonstrations or reward functions. However,
current IfO research focuses on idealized scenarios with mostly bimodal-quality
data distributions, restricting the meaningfulness of the results. In contrast,
this paper investigates more nuanced distributions and introduces a method to
learn from such data, moving closer to a paradigm in which imitation learning
can be performed iteratively via self-improvement. Our method adapts RL-based
imitation learning to action-free demonstrations, using a value function to
transfer information between expert and non-expert data. Through comprehensive
evaluation, we delineate the relation between different data distributions and
the applicability of algorithms and highlight the limitations of established
methods. Our findings provide valuable insights for developing more robust and
practical IfO techniques on a path to scalable behaviour learning.

</details>


### [63] [PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems](https://arxiv.org/abs/2507.06712)
*Ayoub Farkane,Mohamed Boutayeb,Mustapha Oudani,Mounir Ghogho*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息神经网络的非线性系统状态估计方法（PINN-Obs），优于传统模型观测器。


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统的状态估计在部分和噪声测量下具有挑战性，需要更准确的方法。

Method: 结合系统动力学和传感器数据，通过物理信息学习过程自适应学习最优增益矩阵。

Result: 理论分析证明误差最小化，数值模拟验证了其在多种非线性系统中的优越性。

Conclusion: PINN-Obs在准确性、鲁棒性和适应性上优于现有观测器设计。

Abstract: State estimation for nonlinear dynamical systems is a critical challenge in
control and engineering applications, particularly when only partial and noisy
measurements are available. This paper introduces a novel Adaptive
Physics-Informed Neural Network-based Observer (PINN-Obs) for accurate state
estimation in nonlinear systems. Unlike traditional model-based observers,
which require explicit system transformations or linearization, the proposed
framework directly integrates system dynamics and sensor data into a
physics-informed learning process. The observer adaptively learns an optimal
gain matrix, ensuring convergence of the estimated states to the true system
states. A rigorous theoretical analysis establishes formal convergence
guarantees, demonstrating that the proposed approach achieves uniform error
minimization under mild observability conditions. The effectiveness of PINN-Obs
is validated through extensive numerical simulations on diverse nonlinear
systems, including an induction motor model, a satellite motion system, and
benchmark academic examples. Comparative experimental studies against existing
observer designs highlight its superior accuracy, robustness, and adaptability.

</details>


### [64] [Mathematical artificial data for operator learning](https://arxiv.org/abs/2507.06752)
*Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: MAD框架结合物理定律与数据驱动学习，通过生成物理嵌入的解析解和合成数据，解决了传统方法对标记数据或效率-精度权衡的依赖，实现了高效且精确的算子学习。


<details>
  <summary>Details</summary>
Motivation: 传统方法在解决微分方程时面临数据需求高或效率与精度难以兼顾的问题，MAD旨在通过物理嵌入的数据驱动方法克服这些限制。

Method: MAD利用微分方程的数学结构生成物理嵌入的解析解和合成数据，无需实验或模拟训练数据，支持多参数系统的高效算子学习。

Result: 在2D参数化问题中，MAD展示了其通用性和在效率与精度上的优势，适用于多种微分方程场景。

Conclusion: MAD框架有望成为科学计算中物理信息机器智能的通用范式，尤其在复杂参数空间处理方面具有潜力。

Abstract: Machine learning has emerged as a transformative tool for solving
differential equations (DEs), yet prevailing methodologies remain constrained
by dual limitations: data-driven methods demand costly labeled datasets while
model-driven techniques face efficiency-accuracy trade-offs. We present the
Mathematical Artificial Data (MAD) framework, a new paradigm that integrates
physical laws with data-driven learning to facilitate large-scale operator
discovery. By exploiting DEs' intrinsic mathematical structure to generate
physics-embedded analytical solutions and associated synthetic data, MAD
fundamentally eliminates dependence on experimental or simulated training data.
This enables computationally efficient operator learning across multi-parameter
systems while maintaining mathematical rigor. Through numerical demonstrations
spanning 2D parametric problems where both the boundary values and source term
are functions, we showcase MAD's generalizability and superior
efficiency/accuracy across various DE scenarios. This
physics-embedded-data-driven framework and its capacity to handle complex
parameter spaces gives it the potential to become a universal paradigm for
physics-informed machine intelligence in scientific computing.

</details>


### [65] [Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric](https://arxiv.org/abs/2507.06765)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 提出了一种参数化激活函数（Leaky Exponential Linear Unit），用于改进多维非线性数据回归，并通过平滑性和梯度特性提升大型神经网络的性能。


<details>
  <summary>Details</summary>
Motivation: 非线性激活函数对学习非线性数据集至关重要，但现有函数如ELU、SiLU、RELU等在平滑性和梯度特性上存在局限性，影响模型性能。

Method: 提出平滑且梯度非零的Leaky Exponential Linear Unit，并引入一种新的扩散损失指标评估模型过拟合。

Result: 新激活函数在性能上优于现有函数，减少了过拟合和对模型参数的敏感性。

Conclusion: 平滑且梯度非零的激活函数能显著提升模型性能，扩散损失指标为评估过拟合提供了新方法。

Abstract: This document proposes a parametric activation function (ac.f.) aimed at
improving multidimensional nonlinear data regression. It is a established
knowledge that nonlinear ac.f.'s are required for learning nonlinear datasets.
This work shows that smoothness and gradient properties of the ac.f. further
impact the performance of large neural networks in terms of overfitting and
sensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as
ELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and
Leaky-RELU further impart discontinuity in the trained model. Improved
performance is demonstrated with a smooth "Leaky Exponential Linear Unit", with
non-zero gradient that can be trained. A novel diffusion-loss metric is also
proposed to gauge the performance of the trained models in terms of
overfitting.

</details>


### [66] [Mutual Information Free Topological Generalization Bounds via Stability](https://arxiv.org/abs/2507.06775)
*Mario Tuci,Lennart Bastian,Benjamin Dupuis,Nassir Navab,Tolga Birdal,Umut Şimşekli*

Main category: cs.LG

TL;DR: 论文提出了一种新的拓扑泛化界，避免了复杂的信息论项，通过轨迹稳定性框架将泛化误差与拓扑数据分析和算法稳定性联系起来。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑泛化界依赖复杂的信息论项，难以应用于实际算法（如ADAM），因此需要更直观且可解释的泛化界。

Method: 引入轨迹稳定性框架，扩展假设集稳定性概念，通过拓扑数据分析和算法稳定性参数上界泛化误差。

Result: 实验证明拓扑数据分析项对泛化误差有重要影响，尤其在样本量增加时。

Conclusion: 新框架提供了直观且可解释的拓扑泛化界，解释了其实际成功的原因。

Abstract: Providing generalization guarantees for stochastic optimization algorithms is
a major challenge in modern learning theory. Recently, several studies
highlighted the impact of the geometry of training trajectories on the
generalization error, both theoretically and empirically. Among these works, a
series of topological generalization bounds have been proposed, relating the
generalization error to notions of topological complexity that stem from
topological data analysis (TDA). Despite their empirical success, these bounds
rely on intricate information-theoretic (IT) terms that can be bounded in
specific cases but remain intractable for practical algorithms (such as ADAM),
potentially reducing the relevance of the derived bounds. In this paper, we
seek to formulate comprehensive and interpretable topological generalization
bounds free of intractable mutual information terms. To this end, we introduce
a novel learning theoretic framework that departs from the existing strategies
via proof techniques rooted in algorithmic stability. By extending an existing
notion of \textit{hypothesis set stability}, to \textit{trajectory stability},
we prove that the generalization error of trajectory-stable algorithms can be
upper bounded in terms of (i) TDA quantities describing the complexity of the
trajectory of the optimizer in the parameter space, and (ii) the trajectory
stability parameter of the algorithm. Through a series of experimental
evaluations, we demonstrate that the TDA terms in the bound are of great
importance, especially as the number of training samples grows. This ultimately
forms an explanation of the empirical success of the topological generalization
bounds.

</details>


### [67] [Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm](https://arxiv.org/abs/2507.06780)
*George Papadopoulos,George A. Vouros*

Main category: cs.LG

TL;DR: 提出了一种模仿学习方法，学习符合专家轨迹约束的最大熵策略，通过KL散度连接性能与策略差异，优化目标结合强化学习与约束遵循。


<details>
  <summary>Details</summary>
Motivation: 研究如何在模仿学习中结合最大熵策略和约束遵循，以生成更稳健且符合专家行为的策略。

Method: 利用KL散度连接专家与学习策略，结合强化学习目标与约束遵循，采用对偶梯度下降优化目标。

Result: 实验表明，该方法能有效学习符合约束的策略，适应多种约束类型和行为模态，并具备泛化能力。

Conclusion: 该方法在模仿学习中成功结合了最大熵策略与约束遵循，表现出稳定性和有效性。

Abstract: This article introduces an imitation learning method for learning maximum
entropy policies that comply with constraints demonstrated by expert
trajectories executing a task. The formulation of the method takes advantage of
results connecting performance to bounds for the KL-divergence between
demonstrated and learned policies, and its objective is rigorously justified
through a connection to a probabilistic inference framework for reinforcement
learning, incorporating the reinforcement learning objective and the objective
to abide by constraints in an entropy maximization setting. The proposed
algorithm optimizes the learning objective with dual gradient descent,
supporting effective and stable training. Experiments show that the proposed
method can learn effective policy models for constraints-abiding behaviour, in
settings with multiple constraints of different types, accommodating different
modalities of demonstrated behaviour, and with abilities to generalize.

</details>


### [68] [Speech Tokenizer is Key to Consistent Representation](https://arxiv.org/abs/2507.06802)
*Wonjin Jung,Sungil Kang,Dong-Yeon Cho*

Main category: cs.LG

TL;DR: 本文提出了一种新型语音分词器，能够同时编码语言和声学信息，显著提升了语音表征的保真度，适用于多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有的残差向量量化（RVQ）方法虽然引入了语义元素，但往往忽略了关键的声学特征，影响了语音处理的全面性。

Method: 提出了一种先进的方法，同时编码语言和声学信息，保留韵律和情感内容。

Result: 实证评估表明，该方法在语音编码、语音转换、情感识别和多模态语言建模中表现优异，且无需额外训练。

Conclusion: 该方法具有广泛适用性，有望成为推动AI语音处理的关键工具。

Abstract: Speech tokenization is crucial in digital speech processing, converting
continuous speech signals into discrete units for various computational tasks.
This paper introduces a novel speech tokenizer with broad applicability across
downstream tasks. While recent advances in residual vector quantization (RVQ)
have incorporated semantic elements, they often neglect critical acoustic
features. We propose an advanced approach that simultaneously encodes both
linguistic and acoustic information, preserving prosodic and emotional content.
Our method significantly enhances speech representation fidelity across diverse
applications. Empirical evaluations demonstrate its effectiveness in speech
coding, voice conversion, emotion recognition, and multimodal language
modeling, without requiring additional training. This versatility underscores
its potential as a key tool for advancing AI-driven speech processing.

</details>


### [69] [Intrinsic Training Signals for Federated Learning Aggregation](https://arxiv.org/abs/2507.06813)
*Cosimo Fiorini,Matteo Mosconi,Pietro Buzzega,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: LIVAR是一种无需修改架构或损失函数的联邦学习方法，通过利用训练信号实现高效模型聚合。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法需要修改架构或损失函数，LIVAR旨在利用现有训练信号实现高效聚合。

Method: LIVAR采用方差加权的分类器聚合和基于SHAP分析的LoRA合并技术。

Result: LIVAR在多个基准测试中达到最优性能，且与现有方法无缝集成。

Conclusion: LIVAR证明仅通过现有训练信号即可实现高效模型聚合，为联邦学习提供了新范式。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. While existing approaches
for aggregating client-specific classification heads and adapted backbone
parameters require architectural modifications or loss function changes, our
method uniquely leverages intrinsic training signals already available during
standard optimization. We present LIVAR (Layer Importance and VARiance-based
merging), which introduces: i) a variance-weighted classifier aggregation
scheme using naturally emergent feature statistics, and ii) an
explainability-driven LoRA merging technique based on SHAP analysis of existing
update parameter patterns. Without any architectural overhead, LIVAR achieves
state-of-the-art performance on multiple benchmarks while maintaining seamless
integration with existing FL methods. This work demonstrates that effective
model merging can be achieved solely through existing training signals,
establishing a new paradigm for efficient federated model aggregation. The code
will be made publicly available upon acceptance.

</details>


### [70] [Comprehensive Evaluation of Prototype Neural Networks](https://arxiv.org/abs/2507.06819)
*Philipp Schlinge,Steffen Meinert,Martin Atzmueller*

Main category: cs.LG

TL;DR: 本文深入分析了几种原型模型（如ProtoPNet、ProtoPool和PIPNet），提出并应用了一套全面的评估指标，包括新提出的指标，以增强模型可解释性分析。实验在多样化数据集上进行，并开源了代码库。


<details>
  <summary>Details</summary>
Motivation: 原型模型是可解释人工智能（XAI）的重要方法，但缺乏全面的评估指标和对比分析。本文旨在填补这一空白。

Method: 应用标准和新提出的指标，对多种原型模型在多样化数据集上进行评估。

Result: 实验展示了原型模型在不同场景下的性能对比，并验证了新指标的有效性。

Conclusion: 本文为原型模型的评估提供了全面的指标和开源工具，促进了XAI领域的发展。

Abstract: Prototype models are an important method for explainable artificial
intelligence (XAI) and interpretable machine learning. In this paper, we
perform an in-depth analysis of a set of prominent prototype models including
ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive
set of metrics. In addition to applying standard metrics from literature, we
propose several new metrics to further complement the analysis of model
interpretability. In our experimentation, we apply the set of prototype models
on a diverse set of datasets including fine-grained classification, Non-IID
settings and multi-label classification to further contrast the performance.
Furthermore, we also provide our code as an open-source library, which
facilitates simple application of the metrics itself, as well as extensibility
- providing the option for easily adding new metrics and models.
https://github.com/uos-sis/quanproto

</details>


### [71] [HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning](https://arxiv.org/abs/2507.06821)
*Chuhang Zheng,Chunwei Tian,Jie Wen,Daoqiang Zhang,Qi Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种名为HeLo的多模态情感分布学习框架，旨在挖掘多模态情感数据的异质性和互补信息，以及混合基本情感中的标签相关性。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别在人机交互中具有重要意义，但现有方法在挖掘多模态异质性和情感标签相关性方面存在不足。

Method: 采用交叉注意力融合生理数据，设计基于最优传输的异质性挖掘模块，引入可学习的标签嵌入和相关性矩阵对齐，并通过标签相关性驱动的交叉注意力机制进行情感分布学习。

Result: 在两个公开数据集上的实验结果表明，该方法在情感分布学习中表现优越。

Conclusion: HeLo框架有效解决了多模态情感分布学习中的异质性和标签相关性挖掘问题。

Abstract: Multi-modal emotion recognition has garnered increasing attention as it plays
a significant role in human-computer interaction (HCI) in recent years. Since
different discrete emotions may exist at the same time, compared with
single-class emotion recognition, emotion distribution learning (EDL) that
identifies a mixture of basic emotions has gradually emerged as a trend.
However, existing EDL methods face challenges in mining the heterogeneity among
multiple modalities. Besides, rich semantic correlations across arbitrary basic
emotions are not fully exploited. In this paper, we propose a multi-modal
emotion distribution learning framework, named HeLo, aimed at fully exploring
the heterogeneity and complementary information in multi-modal emotional data
and label correlation within mixed basic emotions. Specifically, we first adopt
cross-attention to effectively fuse the physiological data. Then, an optimal
transport (OT)-based heterogeneity mining module is devised to mine the
interaction and heterogeneity between the physiological and behavioral
representations. To facilitate label correlation learning, we introduce a
learnable label embedding optimized by correlation matrix alignment. Finally,
the learnable label embeddings and label correlation matrices are integrated
with the multi-modal representations through a novel label correlation-driven
cross-attention mechanism for accurate emotion distribution learning.
Experimental results on two publicly available datasets demonstrate the
superiority of our proposed method in emotion distribution learning.

</details>


### [72] [Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning](https://arxiv.org/abs/2507.06825)
*Matej Straka,Martin Schmid*

Main category: cs.LG

TL;DR: 介绍了一个基于Generals.io的实时策略游戏环境，兼容Gymnasium和PettingZoo，支持高性能运行，并训练了一个顶级AI代理。


<details>
  <summary>Details</summary>
Motivation: 为多智能体强化学习研究提供一个易用且具有挑战性的平台。

Method: 结合监督预训练和自对弈训练AI代理，并采用基于潜在奖励塑造和记忆特征加速学习。

Result: 训练36小时后，AI代理在1v1人类排行榜中达到前0.003%。

Conclusion: 该环境及基准代理为多智能体强化学习研究提供了高效且竞争性的工具。

Abstract: We introduce a real-time strategy game environment built on Generals.io, a
game that hosts thousands of active players each week across multiple game
formats. Our environment is fully compatible with Gymnasium and PettingZoo,
capable of running thousands of frames per second on commodity hardware. Our
reference agent -- trained with supervised pre-training and self-play -- hits
the top 0.003\% of the 1v1 human leaderboard after just 36 hours on a single
H100 GPU. To accelerate learning, we incorporate potential-based reward shaping
and memory features. Our contributions -- a modular RTS benchmark and a
competitive, state-of-the-art baseline agent -- provide an accessible yet
challenging platform for advancing multi-agent reinforcement learning research.

</details>


### [73] [Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise Conditioning](https://arxiv.org/abs/2507.06839)
*Jihao Andreas Lin*

Main category: cs.LG

TL;DR: 该论文提出了一种结合迭代方法和路径条件化的方法，以提高高斯过程在大规模数据中的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在经典框架下难以适应大规模数据和现代并行计算硬件，因此需要改进其可扩展性。

Method: 通过结合迭代线性系统求解器和路径条件化，将昂贵计算转化为线性方程组求解，降低内存需求。

Result: 显著减少了内存需求，适用于更大规模数据，并优化了现代硬件的计算效率。

Conclusion: 该方法为高斯过程在现代大规模计算环境中的应用提供了有效解决方案。

Abstract: Gaussian processes are a powerful framework for uncertainty-aware function
approximation and sequential decision-making. Unfortunately, their classical
formulation does not scale gracefully to large amounts of data and modern
hardware for massively-parallel computation, prompting many researchers to
develop techniques which improve their scalability. This dissertation focuses
on the powerful combination of iterative methods and pathwise conditioning to
develop methodological contributions which facilitate the use of Gaussian
processes in modern large-scale settings. By combining these two techniques
synergistically, expensive computations are expressed as solutions to systems
of linear equations and obtained by leveraging iterative linear system solvers.
This drastically reduces memory requirements, facilitating application to
significantly larger amounts of data, and introduces matrix multiplication as
the main computational operation, which is ideal for modern hardware.

</details>


### [74] [DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models](https://arxiv.org/abs/2507.06853)
*Liang Wang,Yu Rong,Tingyang Xu,Zhenyi Zhong,Zhiyuan Liu,Pengju Wang,Deli Zhao,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.LG

TL;DR: DiffSpectra是一种基于扩散模型的生成框架，直接从多模态光谱数据推断2D和3D分子结构，解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统分子结构解析方法依赖专家解释且缺乏扩展性，现有机器学习方法依赖有限库，无法泛化到新分子。DiffSpectra旨在通过生成模型解决这些问题。

Method: DiffSpectra采用扩散模型，通过SE(3)-等变架构（Diffusion Molecule Transformer）和基于Transformer的光谱编码器（SpecFormer）整合拓扑和几何信息。

Result: 实验表明，DiffSpectra在结构解析中表现出色，top-1准确率为16.01%，top-20为96.86%。3D几何建模和多模态条件显著提升性能。

Conclusion: DiffSpectra首次统一了多模态光谱推理和2D/3D生成建模，为分子结构解析提供了有效解决方案。

Abstract: Molecular structure elucidation from spectra is a foundational problem in
chemistry, with profound implications for compound identification, synthesis,
and drug development. Traditional methods rely heavily on expert interpretation
and lack scalability. Pioneering machine learning methods have introduced
retrieval-based strategies, but their reliance on finite libraries limits
generalization to novel molecules. Generative models offer a promising
alternative, yet most adopt autoregressive SMILES-based architectures that
overlook 3D geometry and struggle to integrate diverse spectral modalities. In
this work, we present DiffSpectra, a generative framework that directly infers
both 2D and 3D molecular structures from multi-modal spectral data using
diffusion models. DiffSpectra formulates structure elucidation as a conditional
generation process. Its denoising network is parameterized by Diffusion
Molecule Transformer, an SE(3)-equivariant architecture that integrates
topological and geometric information. Conditioning is provided by SpecFormer,
a transformer-based spectral encoder that captures intra- and inter-spectral
dependencies from multi-modal spectra. Extensive experiments demonstrate that
DiffSpectra achieves high accuracy in structure elucidation, recovering exact
structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through
sampling. The model benefits significantly from 3D geometric modeling,
SpecFormer pre-training, and multi-modal conditioning. These results highlight
the effectiveness of spectrum-conditioned diffusion modeling in addressing the
challenge of molecular structure elucidation. To our knowledge, DiffSpectra is
the first framework to unify multi-modal spectral reasoning and joint 2D/3D
generative modeling for de novo molecular structure elucidation.

</details>


### [75] [Episodic Contextual Bandits with Knapsacks under Conversion Models](https://arxiv.org/abs/2507.06859)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 研究在线决策者在资源动态变化的上下文BwK问题中的表现，提出一种算法实现次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决动态定价和拍卖等应用中资源分配的非平稳性和上下文多样性问题。

Method: 设计在线算法，利用置信区间预言机处理无界状态空间。

Result: 算法在T次迭代中实现次线性遗憾，并在特定设置下提供改进的遗憾边界。

Conclusion: 框架为上下文BwK问题提供了新的解决方案，尤其在未标记特征数据下表现优越。

Abstract: We study an online setting, where a decision maker (DM) interacts with
contextual bandit-with-knapsack (BwK) instances in repeated episodes. These
episodes start with different resource amounts, and the contexts' probability
distributions are non-stationary in an episode. All episodes share the same
latent conversion model, which governs the random outcome contingent upon a
request's context and an allocation decision. Our model captures applications
such as dynamic pricing on perishable resources with episodic replenishment,
and first price auctions in repeated episodes with different starting budgets.
We design an online algorithm that achieves a regret sub-linear in $T$, the
number of episodes, assuming access to a \emph{confidence bound oracle} that
achieves an $o(T)$-regret. Such an oracle is readily available from existing
contextual bandit literature. We overcome the technical challenge with
arbitrarily many possible contexts, which leads to a reinforcement learning
problem with an unbounded state space. Our framework provides improved regret
bounds in certain settings when the DM is provided with unlabeled feature data,
which is novel to the contextual BwK literature.

</details>


### [76] [Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants](https://arxiv.org/abs/2507.06888)
*Wei Chen,Wanyang Gu,Linjun Peng,Ruichu Cai,Zhifeng Hao,Kun Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种联邦因果发现方法，通过高阶累积量在水平和垂直联邦设置下学习因果结构，解决了数据隐私和变量不完整的问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦因果学习方法主要关注水平联邦设置，但实际中不同客户端可能包含不同变量，导致虚假因果关系。

Method: 通过聚合客户端的高阶累积量信息构建全局估计，用于递归源识别，生成全局因果强度矩阵。

Result: 在合成和真实数据实验中表现优异，能重构因果图并估计因果强度系数。

Conclusion: 该方法在水平和垂直联邦设置下均有效，解决了变量不完整和隐私保护问题。

Abstract: Federated causal discovery aims to uncover the causal relationships between
entities while protecting data privacy, which has significant importance and
numerous applications in real-world scenarios. Existing federated causal
structure learning methods primarily focus on horizontal federated settings.
However, in practical situations, different clients may not necessarily contain
data on the same variables. In a single client, the incomplete set of variables
can easily lead to spurious causal relationships, thereby affecting the
information transmitted to other clients. To address this issue, we
comprehensively consider causal structure learning methods under both
horizontal and vertical federated settings. We provide the identification
theories and methods for learning causal structure in the horizontal and
vertical federal setting via higher-order cumulants. Specifically, we first
aggregate higher-order cumulant information from all participating clients to
construct global cumulant estimates. These global estimates are then used for
recursive source identification, ultimately yielding a global causal strength
matrix. Our approach not only enables the reconstruction of causal graphs but
also facilitates the estimation of causal strength coefficients. Our algorithm
demonstrates superior performance in experiments conducted on both synthetic
data and real-world data.

</details>


### [77] [Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model](https://arxiv.org/abs/2507.06892)
*Jing Liang,Hongyao Tang,Yi Ma,Jinyi Liu,Yan Zheng,Shuyue Hu,Lei Bai,Jianye Hao*

Main category: cs.LG

TL;DR: 论文提出ReMix方法，通过混合策略和策略重生技术，利用离策略数据优化强化学习微调，显著降低训练成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习微调方法多为同策略，数据利用率低，计算和时间成本高，限制了经济高效的扩展。

Method: ReMix包含混合策略近端策略梯度、KL凸策略约束和策略重生三部分，支持同策略方法利用离策略数据。

Result: 实验显示ReMix在多个数学推理基准上表现优异，训练成本降低30至450倍，性能达到SOTA水平。

Conclusion: ReMix通过高效利用离策略数据，解决了同策略方法的局限性，为强化学习微调提供了新思路。

Abstract: Reinforcement Learning (RL) has demonstrated its potential to improve the
reasoning ability of Large Language Models (LLMs). One major limitation of most
existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL
in nature, i.e., data generated during the past learning process is not fully
utilized. This inevitably comes at a significant cost of compute and time,
posing a stringent bottleneck on continuing economic and efficient scaling. To
this end, we launch the renaissance of off-policy RL and propose Reincarnating
Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable
on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix
consists of three major components: (1) Mix-policy proximal policy gradient
with an increased Update-To-Data (UTD) ratio for efficient training; (2)
KL-Convex policy constraint to balance the trade-off between stability and
flexibility; (3) Policy reincarnation to achieve a seamless transition from
efficient early-stage learning to steady asymptotic improvement. In our
experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base
models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with
0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B
model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math
reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and
MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level
performance with an over 30x to 450x reduction in training cost in terms of
rollout data volume. In addition, we reveal insightful findings via
multifaceted analysis, including the implicit preference for shorter responses
due to the Whipping Effect of off-policy discrepancy, the collapse mode of
self-reflection behavior under the presence of severe off-policyness, etc.

</details>


### [78] [Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams](https://arxiv.org/abs/2507.06901)
*Abolfazl Zarghani,Sadegh Abedi*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的动态滑动窗口优化方法（RL-Window），用于处理多维数据流，解决了固定窗口无法适应动态变化的问题。


<details>
  <summary>Details</summary>
Motivation: 多维数据流（如IoT、金融市场等）的高速度、无界性和复杂依赖关系带来了处理挑战，固定窗口难以适应动态变化（如概念漂移或突发模式）。

Method: 将窗口大小选择建模为强化学习问题，采用Dueling DQN和优先经验回放技术，自适应学习窗口策略。

Result: 在多个基准数据集上，RL-Window在分类精度、漂移鲁棒性和计算效率上优于ADWIN和CNN-Adaptive等现有方法。

Conclusion: RL-Window具有适应性和稳定性，适用于实时应用，并通过扩展指标（如能效、延迟）验证了其优势。

Abstract: Multi-dimensional data streams, prevalent in applications like IoT, financial
markets, and real-time analytics, pose significant challenges due to their high
velocity, unbounded nature, and complex inter-dimensional dependencies. Sliding
window techniques are critical for processing such streams, but fixed-size
windows struggle to adapt to dynamic changes like concept drift or bursty
patterns. This paper proposes a novel reinforcement learning (RL)-based
approach to dynamically optimize sliding window sizes for multi-dimensional
data streams. By formulating window size selection as an RL problem, we enable
an agent to learn an adaptive policy based on stream characteristics, such as
variance, correlations, and temporal trends. Our method, RL-Window, leverages a
Dueling Deep Q-Network (DQN) with prioritized experience replay to handle
non-stationarity and high-dimensionality. Evaluations on benchmark datasets
(UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms
state-of-the-art methods like ADWIN and CNN-Adaptive in classification
accuracy, drift robustness, and computational efficiency. Additional
qualitative analyses, extended metrics (e.g., energy efficiency, latency), and
a comprehensive dataset characterization further highlight its adaptability and
stability, making it suitable for real-time applications.

</details>


### [79] [Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting](https://arxiv.org/abs/2507.06907)
*Linyun Gao,Qiang Wen,Fumio Machida*

Main category: cs.LG

TL;DR: 提出了一种基于N版本机器学习（NVML）的框架，通过安全感知加权软投票机制提升交通标志识别的抗对抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中交通标志识别易受对抗攻击影响，安全性成为关键挑战。

Method: 采用NVML框架，结合FMEA评估安全风险，动态分配权重；测试了三种投票机制对抗FGSM和PGD攻击的效果。

Result: 实验表明，NVML显著提升了系统在对抗条件下的鲁棒性和安全性。

Conclusion: NVML框架有效增强了交通标志识别系统的安全性，为自动驾驶安全提供了新思路。

Abstract: Autonomous driving is rapidly advancing as a key application of machine
learning, yet ensuring the safety of these systems remains a critical
challenge. Traffic sign recognition, an essential component of autonomous
vehicles, is particularly vulnerable to adversarial attacks that can compromise
driving safety. In this paper, we propose an N-version machine learning (NVML)
framework that integrates a safety-aware weighted soft voting mechanism. Our
approach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential
safety risks and assign dynamic, safety-aware weights to the ensemble outputs.
We evaluate the robustness of three-version NVML systems employing various
voting mechanisms against adversarial samples generated using the Fast Gradient
Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental
results demonstrate that our NVML approach significantly enhances the
robustness and safety of traffic sign recognition systems under adversarial
conditions.

</details>


### [80] [DICE: Data Influence Cascade in Decentralized Learning](https://arxiv.org/abs/2507.06931)
*Tongtian Zhu,Wenhao Li,Can Wang,Fengxiang He*

Main category: cs.LG

TL;DR: 论文提出了一种名为DICE的方法，用于在去中心化网络中估计数据影响力传播，以解决参与者激励不足的问题。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习虽能分散数据消耗和计算负载，但缺乏公平的激励机制，阻碍了参与积极性。

Method: 设计了DICE方法，通过理论推导近似计算任意邻居跳数的影响力传播，结合数据、通信拓扑和损失景观曲率。

Result: DICE为选择合适合作者和识别恶意行为提供了理论基础。

Conclusion: DICE是首个在去中心化环境中估计数据影响力传播的方法，具有实际应用潜力。

Abstract: Decentralized learning offers a promising approach to crowdsource data
consumptions and computational workloads across geographically distributed
compute interconnected through peer-to-peer networks, accommodating the
exponentially increasing demands. However, proper incentives are still in
absence, considerably discouraging participation. Our vision is that a fair
incentive mechanism relies on fair attribution of contributions to
participating nodes, which faces non-trivial challenges arising from the
localized connections making influence ``cascade'' in a decentralized network.
To overcome this, we design the first method to estimate \textbf{D}ata
\textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized
environment. Theoretically, the framework derives tractable approximations of
influence cascade over arbitrary neighbor hops, suggesting the influence
cascade is determined by an interplay of data, communication topology, and the
curvature of loss landscape. DICE also lays the foundations for applications
including selecting suitable collaborators and identifying malicious behaviors.
Project page is available at https://raiden-zhu.github.io/blog/2025/DICE/.

</details>


### [81] [What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models](https://arxiv.org/abs/2507.06952)
*Keyon Vafa,Peter G. Chang,Ashesh Rambachan,Sendhil Mullainathan*

Main category: cs.LG

TL;DR: 该论文提出了一种评估基础模型是否真正理解深层结构的方法，发现模型在适应新任务时可能无法形成与底层世界模型一致的归纳偏差。


<details>
  <summary>Details</summary>
Motivation: 评估基础模型是否能够通过序列预测揭示深层领域理解，类似于开普勒预测行星运动后发现的牛顿力学。

Method: 开发了一种技术，通过生成合成数据集并检查模型如何适应这些数据，测量其归纳偏差是否与世界模型一致。

Result: 基础模型在训练任务上表现优异，但在适应新任务时未能形成与底层世界模型一致的归纳偏差，尤其是在轨道轨迹任务中未能应用牛顿力学。

Conclusion: 基础模型可能仅发展出任务特定的启发式方法，而未能实现泛化能力。

Abstract: Foundation models are premised on the idea that sequence prediction can
uncover deeper domain understanding, much like how Kepler's predictions of
planetary motion later led to the discovery of Newtonian mechanics. However,
evaluating whether these models truly capture deeper structure remains a
challenge. We develop a technique for evaluating foundation models that
examines how they adapt to synthetic datasets generated from some postulated
world model. Our technique measures whether the foundation model's inductive
bias aligns with the world model, and so we refer to it as an inductive bias
probe. Across multiple domains, we find that foundation models can excel at
their training tasks yet fail to develop inductive biases towards the
underlying world model when adapted to new tasks. We particularly find that
foundation models trained on orbital trajectories consistently fail to apply
Newtonian mechanics when adapted to new physics tasks. Further analysis reveals
that these models behave as if they develop task-specific heuristics that fail
to generalize.

</details>


### [82] [Noisy PDE Training Requires Bigger PINNs](https://arxiv.org/abs/2507.06967)
*Sebastien Andre-Sloan,Anirbit Mukherjee,Matthew Colbrook*

Main category: cs.LG

TL;DR: 论文研究了在噪声数据下，物理信息神经网络（PINNs）达到低经验风险的条件，并证明了网络规模的下界。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，数据样本通常带有噪声，因此需要明确PINNs在何种条件下仍能有效降低经验风险。

Method: 通过理论分析，证明了在监督和无监督PINN设置下，网络规模与样本数量及噪声方差的关系。

Result: 研究发现，仅增加噪声监督标签数量无法免费降低经验风险，并验证了PINNs在特定条件下可以达到低于噪声方差的经验风险。

Conclusion: 研究为定量理解噪声环境下训练PINNs的参数需求奠定了基础。

Abstract: Physics-Informed Neural Networks (PINNs) are increasingly used to approximate
solutions of partial differential equations (PDEs), especially in high
dimensions. In real-world applications, data samples are noisy, so it is
important to know when a predictor can still achieve low empirical risk.
However, little is known about the conditions under which a PINN can do so
effectively. We prove a lower bound on the size of neural networks required for
the supervised PINN empirical risk to fall below the variance of noisy
supervision labels. Specifically, if a predictor achieves an empirical risk
$O(\eta)$ below $\sigma^2$ (variance of supervision data), then necessarily
$d_N\log d_N\gtrsim N_s \eta^2$, where $N_s$ is the number of samples and $d_N$
is the number of trainable parameters of the PINN. A similar constraint applies
to the fully unsupervised PINN setting when boundary labels are sampled
noisily. Consequently, increasing the number of noisy supervision labels alone
does not provide a ``free lunch'' in reducing empirical risk. We also show
empirically that PINNs can indeed achieve empirical risks below $\sigma^2$
under such conditions. As a case study, we investigate PINNs applied to the
Hamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for
quantitatively understanding the parameter requirements for training PINNs in
the presence of noise.

</details>


### [83] [Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy](https://arxiv.org/abs/2507.06969)
*Bogdan Kulynych,Juan Felipe Gomez,Georgios Kaissis,Jamie Hayes,Borja Balle,Flavio du Pin Calmon,Jean Louis Raisaro*

Main category: cs.LG

TL;DR: 该论文提出了一种基于假设检验解释的差分隐私（f-DP）方法，统一了重识别、属性推断和数据重建风险的界限，提供了更紧致的隐私保护评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私机制在隐私参数映射到具体风险（如重识别、属性推断和数据重建）时过于悲观且不一致，需要更统一和可调的方法。

Method: 使用f-DP的假设检验解释，推导出统一的风险界限，适用于多种攻击场景，并可调整以评估任意基线风险水平。

Result: 实验表明，该方法比ε-DP、Rényi DP和集中DP更紧致，能将噪声减少20%，并在文本分类任务中提高15%以上的准确率。

Conclusion: 该框架为差分隐私的保护程度提供了统一的解释和校准方法，适用于具体风险水平的评估。

Abstract: Differentially private (DP) mechanisms are difficult to interpret and
calibrate because existing methods for mapping standard privacy parameters to
concrete privacy risks -- re-identification, attribute inference, and data
reconstruction -- are both overly pessimistic and inconsistent. In this work,
we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that
bounds on attack success can take the same unified form across
re-identification, attribute inference, and data reconstruction risks. Our
unified bounds are (1) consistent across a multitude of attack settings, and
(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary
(including worst-case) levels of baseline risk. Empirically, our results are
tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated
DP. As a result, calibrating noise using our bounds can reduce the required
noise by 20% at the same risk level, which yields, e.g., more than 15pp
accuracy increase in a text classification task. Overall, this unifying
perspective provides a principled framework for interpreting and calibrating
the degree of protection in DP against specific levels of re-identification,
attribute inference, or data reconstruction risk.

</details>


### [84] [A Principled Framework for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06979)
*Panagiotis Koromilas,Efthymios Georgiou,Giorgos Bouritsas,Theodoros Giannakopoulos,Mihalis A. Nicolaou,Yannis Panagakis*

Main category: cs.LG

TL;DR: 论文提出两种新的损失函数（MV-InfoNCE和MV-DHEL），解决了多视图对比学习中存在的四个关键问题，并在实验中验证了其优于现有方法且能有效利用多视图优势。


<details>
  <summary>Details</summary>
Motivation: 当前多视图对比学习方法存在四个关键局限性（如目标冲突、视图交互建模不足等），无法充分利用多视图带来的优势。

Method: 提出两种新损失函数：MV-InfoNCE（同时建模所有视图交互）和MV-DHEL（解耦对齐与均匀性）。

Result: 在ImageNet1K等数据集上表现优于现有方法，且能扩展到多模态数据。MV-DHEL尤其能有效缓解维度崩溃问题。

Conclusion: 新方法在多视图对比学习中表现出色，能够充分利用多视图优势，并适用于多模态场景。

Abstract: Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning
(SSL), typically relies on pairs of data views generated through augmentation.
While multiple augmentations per instance (more than two) improve
generalization in supervised learning, current CL methods handle additional
views suboptimally by simply aggregating different pairwise objectives. This
approach suffers from four critical limitations: (L1) it utilizes multiple
optimization terms per data point resulting to conflicting objectives, (L2) it
fails to model all interactions across views and data points, (L3) it inherits
fundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL
losses, and (L4) it prevents fully realizing the benefits of increased view
multiplicity observed in supervised settings. We address these limitations
through two novel loss functions: MV-InfoNCE, which extends InfoNCE to
incorporate all possible view interactions simultaneously in one term per data
point, and MV-DHEL, which decouples alignment from uniformity across views
while scaling interaction complexity with view multiplicity. Both approaches
are theoretically grounded - we prove they asymptotically optimize for
alignment of all views and uniformity, providing principled extensions to
multi-view contrastive learning. Our empirical results on ImageNet1K and three
other datasets demonstrate that our methods consistently outperform existing
multi-view approaches and effectively scale with increasing view multiplicity.
We also apply our objectives to multimodal data and show that, in contrast to
other contrastive objectives, they can scale beyond just two modalities. Most
significantly, ablation studies reveal that MV-DHEL with five or more views
effectively mitigates dimensionality collapse by fully utilizing the embedding
space, thereby delivering multi-view benefits observed in supervised learning.

</details>


### [85] [Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing](https://arxiv.org/abs/2507.06996)
*Eunbyeol Cho,Jiyoun Kim,Minjae Lee,Sungjin Park,Edward Choi*

Main category: cs.LG

TL;DR: RawMed是一个生成多表时间序列电子健康记录（EHR）数据的框架，首次实现了接近原始EHR的合成数据生成，并通过新评估框架验证了其高保真度和实用性。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和法规限制，EHR数据的共享和利用受限，需要生成合成数据。现有方法通常仅生成专家选择的特征，无法捕捉原始EHR的复杂结构和时间动态。

Method: RawMed采用基于文本的表示和压缩技术，以最小预处理捕捉复杂结构和时间动态，并提出了多表时间序列合成EHR的新评估框架。

Result: 在开源EHR数据集上验证，RawMed在保真度和实用性上优于基线模型。

Conclusion: RawMed为合成EHR数据提供了更接近原始数据的解决方案，具有潜在的研究和应用价值。

Abstract: Electronic Health Records (EHR) are time-series relational databases that
record patient interactions and medical events over time, serving as a critical
resource for healthcare research and applications. However, privacy concerns
and regulatory restrictions limit the sharing and utilization of such sensitive
data, necessitating the generation of synthetic EHR datasets. Unlike previous
EHR synthesis methods, which typically generate medical records consisting of
expert-chosen features (e.g. a few vital signs or structured codes only), we
introduce RawMed, the first framework to synthesize multi-table, time-series
EHR data that closely resembles raw EHRs. Using text-based representation and
compression techniques, RawMed captures complex structures and temporal
dynamics with minimal preprocessing. We also propose a new evaluation framework
for multi-table time-series synthetic EHRs, assessing distributional
similarity, inter-table relationships, temporal dynamics, and privacy.
Validated on two open-source EHR datasets, RawMed outperforms baseline models
in fidelity and utility. The code is available at
https://github.com/eunbyeol-cho/RawMed.

</details>


### [86] [Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions](https://arxiv.org/abs/2507.07008)
*Emile Pierret,Bruno Galerne*

Main category: cs.LG

TL;DR: 本文研究了扩散模型在高斯数据分布去模糊任务中的准确性，通过计算Wasserstein距离比较理论解与实际解的差异。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为贝叶斯逆问题的先验，具有灵活性和高方差，但其性能尚未完全明确。本文旨在评估其在特定任务中的准确性。

Method: 在受限的高斯数据分布背景下，计算扩散模型采样器与理想解分布之间的Wasserstein距离。

Result: 研究结果揭示了理论解与实际解之间的差异，并允许比较不同算法的性能。

Conclusion: 本文为扩散模型在逆问题中的应用提供了精确的评估方法，有助于算法比较和改进。

Abstract: Used as priors for Bayesian inverse problems, diffusion models have recently
attracted considerable attention in the literature. Their flexibility and high
variance enable them to generate multiple solutions for a given task, such as
inpainting, super-resolution, and deblurring. However, several unresolved
questions remain about how well they perform. In this article, we investigate
the accuracy of these models when applied to a Gaussian data distribution for
deblurring. Within this constrained context, we are able to precisely analyze
the discrepancy between the theoretical resolution of inverse problems and
their resolution obtained using diffusion models by computing the exact
Wasserstein distance between the distribution of the diffusion model sampler
and the ideal distribution of solutions to the inverse problem. Our findings
allow for the comparison of different algorithms from the literature.

</details>


### [87] [On-Device Training of PV Power Forecasting Models in a Smart Meter for Grid Edge Intelligence](https://arxiv.org/abs/2507.07016)
*Jian Huang,Yongli Zhu,Linna Xu,Zhe Zheng,Wenpeng Cui,Mingyang Sun*

Main category: cs.LG

TL;DR: 本文研究了在资源受限的智能电表上进行边缘侧模型训练的可行性，提出了混合和降低精度的训练方案，并通过光伏功率预测案例验证了其经济性。


<details>
  <summary>Details</summary>
Motivation: 推动电网边缘智能化和设备端训练的概念，以解决资源受限环境下的模型训练问题。

Method: 介绍了设备端训练的技术准备步骤，并设计了混合和降低精度的训练方案，应用于梯度提升树模型和循环神经网络模型。

Result: 实验结果表明，通过现有高级计量基础设施，经济地实现电网边缘智能是可行的。

Conclusion: 资源受限设备上的边缘侧模型训练是可行的，为电网边缘智能提供了经济高效的解决方案。

Abstract: In this paper, an edge-side model training study is conducted on a
resource-limited smart meter. The motivation of grid-edge intelligence and the
concept of on-device training are introduced. Then, the technical preparation
steps for on-device training are described. A case study on the task of
photovoltaic power forecasting is presented, where two representative machine
learning models are investigated: a gradient boosting tree model and a
recurrent neural network model. To adapt to the resource-limited situation in
the smart meter, "mixed"- and "reduced"-precision training schemes are also
devised. Experiment results demonstrate the feasibility of economically
achieving grid-edge intelligence via the existing advanced metering
infrastructures.

</details>


### [88] [PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments](https://arxiv.org/abs/2507.07032)
*Hanqun Cao,Xinyi Zhou,Zijun Gao,Chenyu Wang,Xin Gao,Zhi Zhang,Chunbin Gu,Ge Liu,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: PLAME是一种新型的MSA设计模型，利用预训练蛋白质语言模型的进化嵌入，提升低同源性和孤儿蛋白质的结构预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有折叠模型依赖多序列比对（MSA），但在低同源性和孤儿蛋白质上效果有限，PLAME旨在解决这一问题。

Method: PLAME结合预训练表示和守恒-多样性损失，提出新的MSA筛选方法和序列质量评估指标。

Result: 在AlphaFold2和AlphaFold3基准测试中，PLAME在折叠增强和序列质量评估上达到最优性能。

Conclusion: PLAME不仅提升预测质量，还能作为适配器结合AlphaFold2的准确性和ESMFold的推理速度。

Abstract: Protein structure prediction is essential for drug discovery and
understanding biological functions. While recent advancements like AlphaFold
have achieved remarkable accuracy, most folding models rely heavily on multiple
sequence alignments (MSAs) to boost prediction performance. This dependency
limits their effectiveness on low-homology proteins and orphan proteins, where
MSA information is sparse or unavailable. To address this limitation, we
propose PLAME, a novel MSA design model that leverages evolutionary embeddings
from pretrained protein language models. Unlike existing methods, PLAME
introduces pretrained representations to enhance evolutionary information and
employs a conservation-diversity loss to enhance generation quality.
Additionally, we propose a novel MSA selection method to effectively screen
high-quality MSAs and improve folding performance. We also propose a sequence
quality assessment metric that provides an orthogonal perspective to evaluate
MSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,
PLAME achieves state-of-the-art performance in folding enhancement and sequence
quality assessment, with consistent improvements demonstrated on AlphaFold3.
Ablation studies validate the effectiveness of the MSA selection method, while
extensive case studies on various protein types provide insights into the
relationship between AlphaFold's prediction quality and MSA characteristics.
Furthermore, we demonstrate that PLAME can serve as an adapter achieving
AlphaFold2-level accuracy with the ESMFold's inference speed.

</details>


### [89] [Self-Supervised Learning at the Edge: The Cost of Labeling](https://arxiv.org/abs/2507.07033)
*Roberto Pereira,Fernanda Famá,Asal Rangrazi,Marco Miozzo,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: 对比学习（CL）和自监督学习（SSL）在资源受限的边缘设备上部署时面临数据与计算资源需求高的挑战。本文探讨了SSL在边缘学习中的可行性和效率，分析了不同SSL技术在有限资源下的表现，并提出半监督学习可降低训练CL模型的能耗。实验表明，定制化SSL策略能减少资源消耗达4倍。


<details>
  <summary>Details</summary>
Motivation: 传统CL和SSL方法需要大量数据和计算资源，难以在资源受限的边缘设备上部署。本文旨在探索SSL在边缘学习中的可行性，并优化资源利用。

Method: 分析不同SSL技术在有限计算、数据和能源预算下的适应性，评估其在资源受限环境中学习鲁棒表示的效果，并探讨半监督学习对降低能耗的作用。

Result: 实验证明，定制化SSL策略能在保持性能的同时减少资源消耗达4倍。

Conclusion: SSL技术在边缘学习中具有高效性和可行性，定制化策略可显著降低资源需求，适合边缘设备部署。

Abstract: Contrastive learning (CL) has recently emerged as an alternative to
traditional supervised machine learning solutions by enabling rich
representations from unstructured and unlabeled data. However, CL and, more
broadly, self-supervised learning (SSL) methods often demand a large amount of
data and computational resources, posing challenges for deployment on
resource-constrained edge devices. In this work, we explore the feasibility and
efficiency of SSL techniques for edge-based learning, focusing on trade-offs
between model performance and energy efficiency. In particular, we analyze how
different SSL techniques adapt to limited computational, data, and energy
budgets, evaluating their effectiveness in learning robust representations
under resource-constrained settings. Moreover, we also consider the energy
costs involved in labeling data and assess how semi-supervised learning may
assist in reducing the overall energy consumed to train CL models. Through
extensive experiments, we demonstrate that tailored SSL strategies can achieve
competitive performance while reducing resource consumption by up to 4X,
underscoring their potential for energy-efficient learning at the edge.

</details>


### [90] [An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems](https://arxiv.org/abs/2507.07061)
*Shervin Ghaffari,Zohre Bahranifard,Mohammad Akbari*

Main category: cs.LG

TL;DR: 本文提出了一种集成嵌入方法，通过结合多个嵌入模型和训练元编码器，提升LLM缓存系统中的语义相似性检测能力。实验表明，该方法在语义等效查询中实现了92%的缓存命中率，同时保持85%的准确率拒绝非等效查询。


<details>
  <summary>Details</summary>
Motivation: 现有语义缓存框架依赖单一嵌入模型，无法充分捕捉真实查询分布中的多样语义关系，限制了缓存效率。

Method: 采用集成嵌入方法，结合多个嵌入模型并通过训练元编码器优化语义相似性检测。

Result: 在QQP数据集上，实现了92%的缓存命中率和85%的非等效查询拒绝准确率，显著优于单一模型方法。

Conclusion: 集成嵌入方法能更有效区分语义相似与不相似查询，提升LLM缓存系统的性能和计算效率。

Abstract: Semantic caching enhances the efficiency of large language model (LLM)
systems by identifying semantically similar queries, storing responses once,
and serving them for subsequent equivalent requests. However, existing semantic
caching frameworks rely on single embedding models for query representation,
which limits their ability to capture the diverse semantic relationships
present in real-world query distributions. This paper presents an ensemble
embedding approach that combines multiple embedding models through a trained
meta-encoder to improve semantic similarity detection in LLM caching systems.
We evaluate our method using the Quora Question Pairs (QQP) dataset, measuring
cache hit ratios, cache miss ratios, token savings, and response times. Our
ensemble approach achieves a 92\% cache hit ratio for semantically equivalent
queries while maintaining an 85\% accuracy in correctly rejecting
non-equivalent queries as cache misses. These results demonstrate that ensemble
embedding methods significantly outperform single-model approaches in
distinguishing between semantically similar and dissimilar queries, leading to
more effective caching performance and reduced computational overhead in
LLM-based systems.

</details>


### [91] [Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts](https://arxiv.org/abs/2507.07100)
*Lan Li,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: DCE框架通过频率感知专家组和动态专家选择器解决DIL中的类不平衡和分布偏移问题，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决DIL中因类不平衡和跨域分布偏移导致的模型性能下降问题。

Method: 采用频率感知专家组学习特定频率类别的特征，并通过动态专家选择器平衡新旧域知识。

Result: 在四个基准数据集上验证了DCE的优越性能。

Conclusion: DCE框架有效解决了DIL中的关键挑战，提升了模型性能。

Abstract: Domain-Incremental Learning (DIL) focuses on continual learning in
non-stationary environments, requiring models to adjust to evolving domains
while preserving historical knowledge. DIL faces two critical challenges in the
context of imbalanced data: intra-domain class imbalance and cross-domain class
distribution shifts. These challenges significantly hinder model performance,
as intra-domain imbalance leads to underfitting of few-shot classes, while
cross-domain shifts require maintaining well-learned many-shot classes and
transferring knowledge to improve few-shot class performance in old domains. To
overcome these challenges, we introduce the Dual-Balance Collaborative Experts
(DCE) framework. DCE employs a frequency-aware expert group, where each expert
is guided by specialized loss functions to learn features for specific
frequency groups, effectively addressing intra-domain class imbalance.
Subsequently, a dynamic expert selector is learned by synthesizing
pseudo-features through balanced Gaussian sampling from historical class
statistics. This mechanism navigates the trade-off between preserving many-shot
knowledge of previous domains and leveraging new data to improve few-shot class
performance in earlier tasks. Extensive experimental results on four benchmark
datasets demonstrate DCE's state-of-the-art performance.

</details>


### [92] [Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful](https://arxiv.org/abs/2507.07101)
*Martin Marek,Sanae Lotfi,Aditya Somasundaram,Andrew Gordon Wilson,Micah Goldblum*

Main category: cs.LG

TL;DR: 研究发现小批量训练语言模型更稳定且高效，提出Adam超参数调整规则，并建议避免梯度累积。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为小批量训练不稳定，需梯度累积，但本研究重新审视小批量训练的优势。

Method: 提出Adam超参数调整规则，测试小批量（甚至批量大小为1）训练的稳定性与性能。

Result: 小批量训练更稳定、对超参数更鲁棒、性能不逊于大批量，且支持普通SGD训练。

Conclusion: 建议选择小批量训练并调整超参数，避免梯度累积，除非多设备训练带宽受限。

Abstract: Conventional wisdom dictates that small batch sizes make language model
pretraining and fine-tuning unstable, motivating gradient accumulation, which
trades off the number of optimizer steps for a proportional increase in batch
size. While it is common to decrease the learning rate for smaller batch sizes,
other hyperparameters are often held fixed. In this work, we revisit small
batch sizes all the way down to batch size one, and we propose a rule for
scaling Adam hyperparameters to small batch sizes. We find that small batch
sizes (1) train stably, (2) are consistently more robust to hyperparameter
choices, (3) achieve equal or better per-FLOP performance than larger batch
sizes, and (4) notably enable stable language model training with vanilla SGD,
even without momentum, despite storing no optimizer state. Building on these
results, we provide practical recommendations for selecting a batch size and
setting optimizer hyperparameters. We further recommend against gradient
accumulation unless training on multiple devices with multiple model replicas,
bottlenecked by inter-device bandwidth.

</details>


### [93] [Does Data Scaling Lead to Visual Compositional Generalization?](https://arxiv.org/abs/2507.07102)
*Arnas Uselis,Andrea Dittadi,Seong Joon Oh*

Main category: cs.LG

TL;DR: 研究发现，组合泛化能力由数据多样性而非数据规模驱动，线性分解表示结构是关键。


<details>
  <summary>Details</summary>
Motivation: 探讨当代视觉模型是否具备组合理解能力，以及数据规模和多样性对组合泛化的影响。

Method: 通过控制实验，系统变化数据规模、概念多样性和组合覆盖率，评估预训练模型（DINO、CLIP）。

Result: 组合泛化能力依赖于数据多样性，线性分解表示结构能实现高效泛化。预训练模型表现高于随机但不完美。

Conclusion: 强调构建多样化数据集的重要性，以及高效组合学习所需的表示结构。

Abstract: Compositional understanding is crucial for human intelligence, yet it remains
unclear whether contemporary vision models exhibit it. The dominant machine
learning paradigm is built on the premise that scaling data and model sizes
will improve out-of-distribution performance, including compositional
generalization. We test this premise through controlled experiments that
systematically vary data scale, concept diversity, and combination coverage. We
find that compositional generalization is driven by data diversity, not mere
data scale. Increased combinatorial coverage forces models to discover a
linearly factored representational structure, where concepts decompose into
additive components. We prove this structure is key to efficiency, enabling
perfect generalization from few observed combinations. Evaluating pretrained
models (DINO, CLIP), we find above-random yet imperfect performance, suggesting
partial presence of this structure. Our work motivates stronger emphasis on
constructing diverse datasets for compositional generalization, and considering
the importance of representational structure that enables efficient
compositional learning. Code available at
https://github.com/oshapio/visual-compositional-generalization.

</details>
